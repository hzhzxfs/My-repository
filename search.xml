<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>地理空间数据EDA数据探索性分析</title>
    <url>/2021/04/17/EDA%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>EDA——数据探索性分析，是通过了解数据集的基本情况、变量间的相互关系以及变量与预测值之间的关系，为后期特征工程和建立模型做铺垫。本文以<em>智慧海洋建设竞赛</em>为例进行演示。</p>
<h4 id="1-总体了解数据"><a href="#1-总体了解数据" class="headerlink" title="1. 总体了解数据"></a>1. 总体了解数据</h4><h5 id="1-1-查看样本个数和原始特征维度"><a href="#1-1-查看样本个数和原始特征维度" class="headerlink" title="1.1 查看样本个数和原始特征维度"></a>1.1 查看样本个数和原始特征维度</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_train.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_test.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_train.columns	<span class="comment">#查看列名</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.set_option(<span class="string">&#x27;display.max_info_rows&#x27;</span>,<span class="number">2699639</span>)	<span class="comment">#提高非缺失值检查的行数上线</span></span><br><span class="line"><span class="comment">#pd.options.display.max_info_rows = 2699639</span></span><br><span class="line">data_train.info()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#查看count 非空值数、std 标准差、（25%、50%、75%）分位数等基本情况</span></span><br><span class="line">data_train.describe([<span class="number">0.01</span>,<span class="number">0.025</span>,<span class="number">0.05</span>,<span class="number">0.5</span>,<span class="number">0.75</span>,<span class="number">0.9</span>,<span class="number">0.99</span>])	</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h5 id="1-2-查看缺失值和唯一值等"><a href="#1-2-查看缺失值和唯一值等" class="headerlink" title="1.2 查看缺失值和唯一值等"></a>1.2 查看缺失值和唯一值等</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_train.isnull().<span class="built_in">any</span>()	<span class="comment">#查看缺失值</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#查看含有缺失值的列名</span></span><br><span class="line">data_train.columns[data_train.isnull().<span class="built_in">any</span>()].tolist()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#查看仅有唯一值的特征</span></span><br><span class="line">one_value_fea_train = [col <span class="keyword">for</span> col <span class="keyword">in</span> data_train.columns <span class="keyword">if</span> data_train[col].nunique() &lt;= <span class="number">1</span>]</span><br><span class="line">one_value_fea_test = [col <span class="keyword">for</span> col <span class="keyword">in</span> data_test.columns <span class="keyword">if</span> data_test[col].nunique() &lt;= <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h4 id="2-查看数据特性和特征分布"><a href="#2-查看数据特性和特征分布" class="headerlink" title="2. 查看数据特性和特征分布"></a>2. 查看数据特性和特征分布</h4><h5 id="2-1-渔船轨迹可视化"><a href="#2-1-渔船轨迹可视化" class="headerlink" title="2.1 渔船轨迹可视化"></a>2.1 渔船轨迹可视化</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从每个类别中随机抽取三个渔船的轨迹进行可视化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_three_traj</span>():</span></span><br><span class="line">    fig,axes = plt.subplots(nrows=<span class="number">3</span>,ncols=<span class="number">3</span>,figsize=(<span class="number">20</span>,<span class="number">15</span>))</span><br><span class="line">    plt.subplots_adjust(wspace=<span class="number">0.2</span>,hspace=<span class="number">0.2</span>)</span><br><span class="line">    <span class="comment"># 对于每一个类别，随机选出刺网的三条轨迹进行可视化</span></span><br><span class="line">    lables = [<span class="string">&quot;ciwang&quot;</span>,<span class="string">&quot;weiwang&quot;</span>,<span class="string">&quot;tuowang&quot;</span>]</span><br><span class="line">    <span class="keyword">for</span> i,file_type <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>([<span class="string">&quot;ciwang_data&quot;</span>,<span class="string">&quot;weiwang_data&quot;</span>,<span class="string">&quot;tuowang_data&quot;</span>])):</span><br><span class="line">        data1, data2, data3 = get_random_three_traj(<span class="built_in">type</span>=file_type)</span><br><span class="line">        <span class="keyword">for</span> j, datax <span class="keyword">in</span> <span class="built_in">enumerate</span>([data1, data2, data3]):</span><br><span class="line">            x_data = datax[<span class="string">&quot;x&quot;</span>].loc[-<span class="number">1</span>:].values</span><br><span class="line">            y_data = datax[<span class="string">&quot;y&quot;</span>].loc[-<span class="number">1</span>:].values</span><br><span class="line">            axes[i][j - <span class="number">1</span>].scatter(x_data[<span class="number">0</span>], y_data[<span class="number">0</span>], label=<span class="string">&quot;start&quot;</span>, c=<span class="string">&quot;red&quot;</span>, s=<span class="number">20</span>, marker=<span class="string">&quot;o&quot;</span>)</span><br><span class="line">            axes[i][j - <span class="number">1</span>].plot(x_data, y_data, label=lables[i])</span><br><span class="line">            axes[i][j - <span class="number">1</span>].scatter(x_data[<span class="built_in">len</span>(x_data) - <span class="number">1</span>], y_data[<span class="built_in">len</span>(y_data) - <span class="number">1</span>], label=<span class="string">&quot;end&quot;</span>, c=<span class="string">&quot;green&quot;</span>, s=<span class="number">20</span>,</span><br><span class="line">                                   marker=<span class="string">&quot;D&quot;</span>)</span><br><span class="line">            axes[i][j - <span class="number">1</span>].grid(alpha=<span class="number">2</span>)</span><br><span class="line">            axes[i][j - <span class="number">1</span>].legend(loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">visualize_three_traj()</span><br></pre></td></tr></table></figure>
<p>—————-图1——————</p>
<p>从图中可以发现,不同类别的轨迹有一定区分性.</p>
<blockquote>
<p>刺网为规则多边形.</p>
<p>围网为包围的形状.</p>
<p>拖网为点到点,转弯次数少.</p>
</blockquote>
<p>从轨迹数据可以猜测其特征可能为转弯的角度大小\转弯次数\起始点之间的距离和时间\经度和维度变化范围等.</p>
<p>此外,存在一些异常轨迹需要剔除.</p>
<h4 id="3-坐标序列可视化"><a href="#3-坐标序列可视化" class="headerlink" title="3. 坐标序列可视化"></a>3. 坐标序列可视化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 随机选取某条数据，观察x坐标序列和y坐标序列的变化情况</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_one_traj_x_y</span>():</span></span><br><span class="line">    fig,axes = plt.subplots(nrows=<span class="number">2</span>,ncols=<span class="number">1</span>,figsize=(<span class="number">10</span>,<span class="number">8</span>))</span><br><span class="line">    plt.subplots_adjust(wspace=<span class="number">0.2</span>,hspace=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">    data1 = get_random_one_traj(<span class="built_in">type</span>=<span class="string">&quot;weiwang_data&quot;</span>)</span><br><span class="line">    x = data1[<span class="string">&quot;x&quot;</span>].loc[-<span class="number">1</span>:]</span><br><span class="line">    x = x / <span class="number">10000</span></span><br><span class="line">    </span><br><span class="line">    y = data1[<span class="string">&quot;y&quot;</span>].loc[-<span class="number">1</span>:]</span><br><span class="line">    y = y / <span class="number">10000</span></span><br><span class="line"></span><br><span class="line">    arr1 = np.arange(<span class="built_in">len</span>(x))</span><br><span class="line">    arr2 = np.arange(<span class="built_in">len</span>(y))</span><br><span class="line"></span><br><span class="line">    axes[<span class="number">0</span>].plot(arr1,x,label=<span class="string">&quot;x&quot;</span>)</span><br><span class="line">    axes[<span class="number">1</span>].plot(arr2,y,label=<span class="string">&quot;y&quot;</span>)</span><br><span class="line">    axes[<span class="number">0</span>].grid(alpha=<span class="number">3</span>)</span><br><span class="line">    axes[<span class="number">0</span>].legend(loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line">    axes[<span class="number">1</span>].grid(alpha=<span class="number">3</span>)</span><br><span class="line">    axes[<span class="number">1</span>].legend(loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">visualize_one_traj_x_y()</span><br></pre></td></tr></table></figure>
<p>———-图2———–</p>
<p>由上图可知,存在同一时间段内x\y坐标均未变化,说明可能该时段内渔船正停留在某处.</p>
<h4 id="4-三类渔船速度和方向可视化"><a href="#4-三类渔船速度和方向可视化" class="headerlink" title="4.三类渔船速度和方向可视化"></a>4.三类渔船速度和方向可视化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 每类轨迹，随机选取某个渔船，可视化速度序列和方向序列</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_three_traj_speed_direction</span>():</span></span><br><span class="line">    fig,axes = plt.subplots(nrows=<span class="number">3</span>,ncols=<span class="number">2</span>,figsize=(<span class="number">20</span>,<span class="number">15</span>))</span><br><span class="line">    plt.subplots_adjust(wspace=<span class="number">0.1</span>,hspace=<span class="number">0.1</span>)</span><br><span class="line">    <span class="comment"># 随机选出刺网的三条轨迹进行可视化</span></span><br><span class="line">    file_types = [<span class="string">&quot;ciwang_data&quot;</span>,<span class="string">&quot;weiwang_data&quot;</span>,<span class="string">&quot;tuowang_data&quot;</span>]</span><br><span class="line">    speed_types = [<span class="string">&quot;ciwang_speed&quot;</span>,<span class="string">&quot;weiwang_speed&quot;</span>,<span class="string">&quot;tuowang_speed&quot;</span>]</span><br><span class="line">    doirections = [<span class="string">&quot;ciwang_direction&quot;</span>,<span class="string">&quot;weiwang_direction&quot;</span>,<span class="string">&quot;tuowang_direction&quot;</span>]</span><br><span class="line">    colors = [<span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;brown&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> i,file_name <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(file_types)):</span><br><span class="line">        datax = get_random_one_traj(<span class="built_in">type</span>=file_name)</span><br><span class="line">        x_data = datax[<span class="string">&quot;速度&quot;</span>].loc[-<span class="number">1</span>:].values</span><br><span class="line">        y_data = datax[<span class="string">&quot;方向&quot;</span>].loc[-<span class="number">1</span>:].values</span><br><span class="line">        axes[i][<span class="number">0</span>].plot(<span class="built_in">range</span>(<span class="built_in">len</span>(x_data)), x_data, label=speed_types[i], color=colors[i])</span><br><span class="line">        axes[i][<span class="number">0</span>].grid(alpha=<span class="number">2</span>)</span><br><span class="line">        axes[i][<span class="number">0</span>].legend(loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line">        axes[i][<span class="number">1</span>].plot(<span class="built_in">range</span>(<span class="built_in">len</span>(y_data)), y_data, label=doirections[i], color=colors[i])</span><br><span class="line">        axes[i][<span class="number">1</span>].grid(alpha=<span class="number">2</span>)</span><br><span class="line">        axes[i][<span class="number">1</span>].legend(loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">visualize_three_traj_speed_direction()</span><br></pre></td></tr></table></figure>
<p>————图3—————-<br>由上图可知,不同分类渔船的轨迹速度某些时段均存在连续的低值情况,说明可能存在某些海上停留点;不同类别渔船的方向变化都很大,可能是海上漂泊导致,作为特征对于类别的区分度低,但也存在方向变化不大的时段,强化了对停留点存在的判断.</p>
<h4 id="5-三类渔船速度和方向的数据分布"><a href="#5-三类渔船速度和方向的数据分布" class="headerlink" title="5.三类渔船速度和方向的数据分布"></a>5.三类渔船速度和方向的数据分布</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对某一特征进行数据统计</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_cummulation</span>(<span class="params"><span class="built_in">type</span>,path,kind,columns</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    type:&quot;ciwang&quot;,&quot;weiwang&quot; or &quot;tuowang&quot;</span></span><br><span class="line"><span class="string">    path:数据路径</span></span><br><span class="line"><span class="string">    kind:&quot;速度&quot;or&quot;方向&quot;</span></span><br><span class="line"><span class="string">    columns:与kind对应，&quot;speed&quot;or&quot;direction&quot;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    data_dict = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path + <span class="built_in">type</span>+<span class="string">&quot;.pkl&quot;</span>,<span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> file:</span><br><span class="line">        data_list = pickle.load(file)</span><br><span class="line">    <span class="keyword">for</span> datax <span class="keyword">in</span> tqdm(data_list):</span><br><span class="line">        data = datax[kind].values</span><br><span class="line">        <span class="keyword">for</span> speed <span class="keyword">in</span> data:</span><br><span class="line">            data_dict.setdefault(speed,<span class="number">0</span>)</span><br><span class="line">            data_dict[speed] += <span class="number">1</span></span><br><span class="line">    data_dict = <span class="built_in">dict</span>(<span class="built_in">sorted</span>(data_dict.items(),key=<span class="keyword">lambda</span> x:x[<span class="number">0</span>],reverse=<span class="literal">False</span>))</span><br><span class="line">    data_df = pd.DataFrame.from_dict(data_dict,columns=[columns],orient=<span class="string">&quot;index&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> data_df</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 分别得到速度和方向的分布数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_speed_and_direction_distribution_data</span>(<span class="params"><span class="built_in">type</span></span>):</span></span><br><span class="line">    path = <span class="string">&quot;./data/&quot;</span></span><br><span class="line">    data_speed_df = get_data_cummulation(<span class="built_in">type</span>=<span class="built_in">type</span>, path=path,kind=<span class="string">&quot;速度&quot;</span>,columns=<span class="string">&quot;speed&quot;</span>)</span><br><span class="line">    data_direction_df = get_data_cummulation(<span class="built_in">type</span>=<span class="built_in">type</span>,path=path,kind=<span class="string">&quot;方向&quot;</span>,columns=<span class="string">&quot;direction&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> data_speed_df,data_direction_df</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可视化速度和方向的数据分布</span></span><br><span class="line">df_speeds = []</span><br><span class="line">df_directions = []</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_speed_direction1_distribution</span>():</span></span><br><span class="line">    plt.subplots(nrows=<span class="number">1</span>, ncols=<span class="number">2</span>, figsize=(<span class="number">15</span>, <span class="number">6</span>))</span><br><span class="line">    plt.subplots_adjust(wspace=<span class="number">0.2</span>, hspace=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">    file_types = [<span class="string">&quot;ciwang_data&quot;</span>, <span class="string">&quot;weiwang_data&quot;</span>, <span class="string">&quot;tuowang_data&quot;</span>]</span><br><span class="line">    lables = [<span class="string">&quot;ciwang&quot;</span>, <span class="string">&quot;weiwang&quot;</span>, <span class="string">&quot;tuowang&quot;</span>]</span><br><span class="line">    colors = [<span class="string">&quot;red&quot;</span>, <span class="string">&quot;blue&quot;</span>, <span class="string">&quot;green&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, filenames <span class="keyword">in</span> <span class="built_in">enumerate</span>(file_types):</span><br><span class="line">        df11, df21 = get_speed_and_direction_distribution_data(file_types[i])</span><br><span class="line">        plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">        ax1 = sns.kdeplot(df11[<span class="string">&quot;speed&quot;</span>].values / <span class="number">1000000</span>, color=colors[i],shade=<span class="literal">True</span>)</span><br><span class="line">        plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">        ax3 = sns.kdeplot(df21[<span class="string">&quot;direction&quot;</span>].values / <span class="number">1000000</span>, color=colors[i],shade=<span class="literal">True</span>)</span><br><span class="line">        df_speeds.append(df11)</span><br><span class="line">        df_directions.append(df21)</span><br><span class="line">    ax1.legend(lables)</span><br><span class="line">    ax1.set_xlabel(<span class="string">&quot;Speed&quot;</span>)</span><br><span class="line">    ax3.set_xlabel(<span class="string">&quot;Direction&quot;</span>)</span><br><span class="line">    ax3.legend(lables)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plot_speed_direction1_distribution()</span><br></pre></td></tr></table></figure>
<p>———–图4———————</p>
<p>由上图可知,三种类别渔船的速度分布差异较大,而刺网和围网方向分布差异不明显,拖网方向分布有差异.</p>
]]></content>
      <categories>
        <category>Geospatial Data Analysis</category>
      </categories>
      <tags>
        <tag>DataFrame</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>LightGBM调参_1</title>
    <url>/2021/03/26/lightGBM%E8%B0%83%E5%8F%82/</url>
    <content><![CDATA[<p>#1简单列举一下日常调参过程中常用的几种方法，具体的原理下次补上。</p>
<h4 id="1-经验法"><a href="#1-经验法" class="headerlink" title="1. 经验法:"></a>1. 经验法:</h4><blockquote>
<p>往两个方向调：</p>
<p>1.提高准确率：max_depth, num_leaves, learning_rate</p>
<p>2.降低过拟合：max_bin, min_data_in_leaf；L1, L2正则化；数据抽样, 列采样</p>
</blockquote>
<p>1.使用较小的num_leaves，max_depth和max_bin，降低复杂度。</p>
<p>2.使用min_data_in_leaf和min_sum_hessian_in_leaf，该值越大，模型的学习越保守。</p>
<span id="more"></span>

<p>3.设置bagging_freq和bagging_fraction使用bagging。</p>
<p>4.设置feature_fraction进行特征采样。</p>
<p>5.使用lambda_l1,lambda_l2和min_gain_to_split正则化。</p>
<h4 id="2-贪心调参"><a href="#2-贪心调参" class="headerlink" title="2. 贪心调参:"></a>2. 贪心调参:</h4><p>先调整对模型影响最大的参数，再调整对模型影响次大的参数，缺点是容易调成局部最优，需要多次调试。日常调参顺序如下:</p>
<p>① num_leaves, max_depth</p>
<p>② min_data_in_leaf, min_child_weight</p>
<p>③ bagging_freq, bagging_fraction,  feature_fraction,</p>
<p>④ reg_lambda, reg_alpha</p>
<p>⑤ min_split_gain</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="comment"># 调objective</span></span><br><span class="line">best_obj = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> obj <span class="keyword">in</span> objective:</span><br><span class="line">    model = LGBMRegressor(objective=obj)</span><br><span class="line">    score = cross_val_score(model, X_train, y_train, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;f1&#x27;</span>).mean()</span><br><span class="line">    best_obj[obj] = score</span><br><span class="line"></span><br><span class="line"><span class="comment"># num_leaves</span></span><br><span class="line">best_leaves = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> leaves <span class="keyword">in</span> num_leaves:</span><br><span class="line">    model = LGBMRegressor(objective=<span class="built_in">min</span>(best_obj.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])[<span class="number">0</span>], num_leaves=leaves)</span><br><span class="line">    score = cross_val_score(model, X_train, y_train, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;f1&#x27;</span>).mean()</span><br><span class="line">    best_leaves[leaves] = score</span><br><span class="line"></span><br><span class="line"><span class="comment"># max_depth</span></span><br><span class="line">best_depth = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> depth <span class="keyword">in</span> max_depth:</span><br><span class="line">    model = LGBMRegressor(objective=<span class="built_in">min</span>(best_obj.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])[<span class="number">0</span>],</span><br><span class="line">                          num_leaves=<span class="built_in">min</span>(best_leaves.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])[<span class="number">0</span>],</span><br><span class="line">                          max_depth=depth)</span><br><span class="line">    score = cross_val_score(model, X_train, y_train, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;f1&#x27;</span>).mean()</span><br><span class="line">    best_depth[depth] = score</span><br></pre></td></tr></table></figure>

<p>以此类推，按调参顺序依次调整优化，并且可以对每一个最优参数下模型的得分进行可视化。</p>
<h4 id="3-网格搜索"><a href="#3-网格搜索" class="headerlink" title="3. 网格搜索"></a>3. 网格搜索</h4><p>即穷举搜索，在参数数组里循环遍历，一般大数据集不会用到，因为速度太慢。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_best_cv_params</span>(<span class="params">learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">581</span>, num_leaves=<span class="number">31</span>, max_depth=-<span class="number">1</span>, bagging_fraction=<span class="number">1.0</span>, feature_fraction=<span class="number">1.0</span>, bagging_freq=<span class="number">0</span>, min_data_in_leaf=<span class="number">20</span>, min_child_weight=<span class="number">0.001</span>, min_split_gain=<span class="number">0</span>, reg_lambda=<span class="number">0</span>, reg_alpha=<span class="number">0</span>, param_grid=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    cv_fold = KFold(n_splits=<span class="number">5</span>, shuffle=<span class="literal">True</span>, random_state=<span class="number">2021</span>)</span><br><span class="line"></span><br><span class="line">    model_lgb = lgb.LGBMClassifier(learning_rate=learning_rate,</span><br><span class="line">                                   n_estimators=n_estimators,</span><br><span class="line">                                   num_leaves=num_leaves,</span><br><span class="line">                                   max_depth=max_depth,</span><br><span class="line">                                   bagging_fraction=bagging_fraction,</span><br><span class="line">                                   feature_fraction=feature_fraction,</span><br><span class="line">                                   bagging_freq=bagging_freq,</span><br><span class="line">                                   min_data_in_leaf=min_data_in_leaf,</span><br><span class="line">                                   min_child_weight=min_child_weight,</span><br><span class="line">                                   min_split_gain=min_split_gain,</span><br><span class="line">                                   reg_lambda=reg_lambda,</span><br><span class="line">                                   reg_alpha=reg_alpha,</span><br><span class="line">                                   n_jobs= <span class="number">8</span></span><br><span class="line">                                  )</span><br><span class="line"></span><br><span class="line">    f1 = make_scorer(f1_score, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">    grid_search = GridSearchCV(estimator=model_lgb, </span><br><span class="line">                               cv=cv_fold,</span><br><span class="line">                               param_grid=param_grid,</span><br><span class="line">                               scoring=f1</span><br><span class="line"></span><br><span class="line">                              )</span><br><span class="line">    grid_search.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;模型当前最优参数为:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(grid_search.best_params_))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;模型当前最优得分为:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(grid_search.best_score_))</span><br></pre></td></tr></table></figure>
<p>总体思路是先粗调再细调。在一开始调整时，可设置较大的学习率如0.1，先确定树的个数，再依次调整参数，最后设置较小的学习率如0.05，确定最终参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lgb_params = &#123;<span class="string">&#x27;num_leaves&#x27;</span>: <span class="built_in">range</span>(<span class="number">10</span>, <span class="number">80</span>, <span class="number">5</span>), <span class="string">&#x27;max_depth&#x27;</span>: <span class="built_in">range</span>(<span class="number">3</span>,<span class="number">10</span>,<span class="number">2</span>)&#125;</span><br><span class="line">get_best_cv_params()</span><br><span class="line"><span class="comment">#----------------------------------------</span></span><br><span class="line">lgb_params = &#123;<span class="string">&#x27;num_leaves&#x27;</span>: <span class="built_in">range</span>(<span class="number">25</span>, <span class="number">35</span>, <span class="number">1</span>), <span class="string">&#x27;max_depth&#x27;</span>: <span class="built_in">range</span>(<span class="number">5</span>,<span class="number">9</span>,<span class="number">1</span>)&#125;</span><br><span class="line">get_best_cv_params(n_estimators=<span class="number">85</span>)</span><br><span class="line"><span class="comment">#----------------------------------------</span></span><br><span class="line">lgb_params = &#123;<span class="string">&#x27;bagging_fraction&#x27;</span>: [i/<span class="number">10</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>,<span class="number">10</span>,<span class="number">1</span>)], </span><br><span class="line">              <span class="string">&#x27;feature_fraction&#x27;</span>: [i/<span class="number">10</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>,<span class="number">10</span>,<span class="number">1</span>)],</span><br><span class="line">              <span class="string">&#x27;bagging_freq&#x27;</span>: <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">81</span>,<span class="number">10</span>)&#125;</span><br><span class="line">get_best_cv_params(n_estimators=<span class="number">85</span>, num_leaves=<span class="number">29</span>, max_depth=<span class="number">7</span>, min_data_in_leaf=<span class="number">45</span>）</span><br><span class="line"><span class="comment">#----------------------------------------</span></span><br><span class="line">lgb_params = &#123;<span class="string">&#x27;reg_lambda&#x27;</span>: [<span class="number">0</span>,<span class="number">0.001</span>,<span class="number">0.01</span>,<span class="number">0.03</span>,<span class="number">0.08</span>,<span class="number">0.3</span>,<span class="number">0.5</span>], <span class="string">&#x27;reg_alpha&#x27;</span>: [<span class="number">0</span>,<span class="number">0.001</span>,<span class="number">0.01</span>,<span class="number">0.03</span>,<span class="number">0.08</span>,<span class="number">0.3</span>,<span class="number">0.5</span>]&#125;</span><br><span class="line">get_best_cv_params(n_estimators=<span class="number">85</span>, num_leaves=<span class="number">29</span>, max_depth=<span class="number">7</span>, min_data_in_leaf=<span class="number">45</span>, bagging_fraction=<span class="number">0.9</span>, feature_fraction=<span class="number">0.9</span>, bagging_freq=<span class="number">40</span>)</span><br><span class="line"><span class="comment">#----------------------------------------</span></span><br><span class="line">lgb_params = &#123;<span class="string">&#x27;min_split_gain&#x27;</span>: [i/<span class="number">10</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">11</span>,<span class="number">1</span>)]&#125;</span><br><span class="line">get_best_cv_params(n_estimators=<span class="number">85</span>, num_leaves=<span class="number">29</span>, max_depth=<span class="number">7</span>, min_data_in_leaf=<span class="number">45</span>, bagging_fraction=<span class="number">0.9</span>, feature_fraction=<span class="number">0.9</span>, bagging_freq=<span class="number">40</span>, min_split_gain=<span class="literal">None</span>)</span><br><span class="line"><span class="comment">#----------------------------------------</span></span><br><span class="line">final_params = &#123;</span><br><span class="line">                <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.01</span>,</span><br><span class="line">                <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">29</span>,</span><br><span class="line">                <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">7</span>,</span><br><span class="line">                <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;multiclass&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;num_class&#x27;</span>: <span class="number">4</span>,</span><br><span class="line">                <span class="string">&#x27;min_data_in_leaf&#x27;</span>:<span class="number">45</span>,</span><br><span class="line">                <span class="string">&#x27;min_child_weight&#x27;</span>:<span class="number">0.001</span>,</span><br><span class="line">                <span class="string">&#x27;bagging_fraction&#x27;</span>: <span class="number">0.9</span>,</span><br><span class="line">                <span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.9</span>,</span><br><span class="line">                <span class="string">&#x27;bagging_freq&#x27;</span>: <span class="number">40</span>,</span><br><span class="line">                <span class="string">&#x27;min_split_gain&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">                <span class="string">&#x27;reg_lambda&#x27;</span>:<span class="number">0</span>,</span><br><span class="line">                <span class="string">&#x27;reg_alpha&#x27;</span>:<span class="number">0</span>,</span><br><span class="line">                <span class="string">&#x27;nthread&#x27;</span>: <span class="number">6</span></span><br><span class="line">               &#125;</span><br><span class="line"></span><br><span class="line">cv_result = lgb.cv(train_set=lgb_train,</span><br><span class="line">                   early_stopping_rounds=<span class="number">20</span>,</span><br><span class="line">                   num_boost_round=<span class="number">5000</span>,</span><br><span class="line">                   nfold=<span class="number">5</span>,</span><br><span class="line">                   stratified=<span class="literal">True</span>,</span><br><span class="line">                   shuffle=<span class="literal">True</span>,</span><br><span class="line">                   params=final_params,</span><br><span class="line">                   feval=f1_score_vali,</span><br><span class="line">                   seed=<span class="number">0</span>,</span><br><span class="line">                  )</span><br></pre></td></tr></table></figure>
<h4 id="4-贝叶斯调参"><a href="#4-贝叶斯调参" class="headerlink" title="4. 贝叶斯调参"></a>4. 贝叶斯调参</h4><p>是一种用模型找到目标函数最小值的方法，比网格和随机搜索省时。步骤如下：</p>
<p>① 定义优化函数(rf_cv）</p>
<p>② 建立模型</p>
<p>③ 定义待优化的参数</p>
<p>④ 得到优化结果，并返回要优化的分数指标</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="comment">#定义优化函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rf_cv_lgb</span>(<span class="params">num_leaves, max_depth, bagging_fraction, feature_fraction, bagging_freq, min_data_in_leaf, </span></span></span><br><span class="line"><span class="function"><span class="params">              min_child_weight, min_split_gain, reg_lambda, reg_alpha</span>):</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立模型</span></span><br><span class="line">    model_lgb = lgb.LGBMClassifier(boosting_type=<span class="string">&#x27;gbdt&#x27;</span>, objective=<span class="string">&#x27;multiclass&#x27;</span>, num_class=<span class="number">4</span>,learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">5000</span>,num_leaves=<span class="built_in">int</span>(num_leaves), max_depth=<span class="built_in">int</span>(max_depth), bagging_fraction=<span class="built_in">round</span>(bagging_fraction, <span class="number">2</span>), feature_fraction=<span class="built_in">round</span>(feature_fraction, <span class="number">2</span>),bagging_freq=<span class="built_in">int</span>(bagging_freq), min_data_in_leaf=<span class="built_in">int</span>(min_data_in_leaf),min_child_weight=min_child_weight)</span><br><span class="line">    f1 = make_scorer(f1_score, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">    val = cross_val_score(model_lgb, X_train_split, y_train_split, cv=<span class="number">5</span>, scoring=f1).mean()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> val</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bayes_opt <span class="keyword">import</span> BayesianOptimization</span><br><span class="line"><span class="comment">#定义优化参数</span></span><br><span class="line">bayes_lgb = BayesianOptimization(</span><br><span class="line">    rf_cv_lgb, </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&#x27;num_leaves&#x27;</span>:(<span class="number">10</span>, <span class="number">200</span>),</span><br><span class="line">        <span class="string">&#x27;max_depth&#x27;</span>:(<span class="number">3</span>, <span class="number">20</span>),</span><br><span class="line">        <span class="string">&#x27;bagging_fraction&#x27;</span>:(<span class="number">0.5</span>, <span class="number">1.0</span>),</span><br><span class="line">        <span class="string">&#x27;feature_fraction&#x27;</span>:(<span class="number">0.5</span>, <span class="number">1.0</span>),</span><br><span class="line">        <span class="string">&#x27;bagging_freq&#x27;</span>:(<span class="number">0</span>, <span class="number">100</span>),</span><br><span class="line">        <span class="string">&#x27;min_data_in_leaf&#x27;</span>:(<span class="number">10</span>,<span class="number">100</span>),</span><br><span class="line">        <span class="string">&#x27;min_child_weight&#x27;</span>:(<span class="number">0</span>, <span class="number">10</span>),</span><br><span class="line">        <span class="string">&#x27;min_split_gain&#x27;</span>:(<span class="number">0.0</span>, <span class="number">1.0</span>),</span><br><span class="line">        <span class="string">&#x27;reg_alpha&#x27;</span>:(<span class="number">0.0</span>, <span class="number">10</span>),</span><br><span class="line">        <span class="string">&#x27;reg_lambda&#x27;</span>:(<span class="number">0.0</span>, <span class="number">10</span>),</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#开始优化</span></span><br><span class="line">bayes_lgb.maximize(n_iter=<span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#显示优化结果</span></span><br><span class="line">bayes_lgb.<span class="built_in">max</span></span><br></pre></td></tr></table></figure>

<p>参数优化完成后，可根据优化后的参数建立新的模型，降低学习率并寻找最优模型迭代次数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#设置较小的学习率，并通过cv函数确定当前最优的迭代次数</span></span><br><span class="line">base_params_lgb = &#123;</span><br><span class="line">                    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">                    <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;multiclass&#x27;</span>,</span><br><span class="line">                    <span class="string">&#x27;num_class&#x27;</span>: <span class="number">4</span>,</span><br><span class="line">                    <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.01</span>,</span><br><span class="line">                    <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">138</span>,</span><br><span class="line">                    <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">11</span>,</span><br><span class="line">                    <span class="string">&#x27;min_data_in_leaf&#x27;</span>: <span class="number">43</span>,</span><br><span class="line">                    <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">6.5</span>,</span><br><span class="line">                    <span class="string">&#x27;bagging_fraction&#x27;</span>: <span class="number">0.64</span>,</span><br><span class="line">                    <span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.93</span>,</span><br><span class="line">                    <span class="string">&#x27;bagging_freq&#x27;</span>: <span class="number">49</span>,</span><br><span class="line">                    <span class="string">&#x27;reg_lambda&#x27;</span>: <span class="number">7</span>,</span><br><span class="line">                    <span class="string">&#x27;reg_alpha&#x27;</span>: <span class="number">0.21</span>,</span><br><span class="line">                    <span class="string">&#x27;min_split_gain&#x27;</span>: <span class="number">0.288</span>,</span><br><span class="line">                    <span class="string">&#x27;nthread&#x27;</span>: <span class="number">10</span>,</span><br><span class="line">                    <span class="string">&#x27;verbose&#x27;</span>: -<span class="number">1</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">cv_result_lgb = lgb.cv(</span><br><span class="line">    train_set=train_matrix,</span><br><span class="line">    early_stopping_rounds=<span class="number">1000</span>, </span><br><span class="line">    num_boost_round=<span class="number">20000</span>,</span><br><span class="line">    nfold=<span class="number">5</span>,</span><br><span class="line">    stratified=<span class="literal">True</span>,</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    params=base_params_lgb,</span><br><span class="line">    feval=f1_score_vali,</span><br><span class="line">    seed=<span class="number">0</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;迭代次数&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(cv_result_lgb[<span class="string">&#x27;f1_score-mean&#x27;</span>])))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;最终模型的f1为&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">max</span>(cv_result_lgb[<span class="string">&#x27;f1_score-mean&#x27;</span>])))</span><br></pre></td></tr></table></figure>

<p>模型参数确定之后，建立最终模型并对验证集进行验证。</p>
]]></content>
      <categories>
        <category>Hyperparameter</category>
      </categories>
      <tags>
        <tag>Grid search</tag>
        <tag>Bayesian optimization</tag>
        <tag>Cross validation</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas的一些常用操作_1</title>
    <url>/2021/02/21/pandas_1/</url>
    <content><![CDATA[<blockquote>
<p>今天介绍几个常用的Pandas操作。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;./economics.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="1-DataFrame-to-markdown-latex"><a href="#1-DataFrame-to-markdown-latex" class="headerlink" title="1.DataFrame to markdown/latex"></a>1.DataFrame to markdown/latex</h4><p>dataframe可以转换为许多常用格式，如csv,excel,sql,json,html,latex等等，这里以markdown和latex为例。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(df.to_markdown())</span><br><span class="line"><span class="built_in">print</span>(df.to_latex())</span><br></pre></td></tr></table></figure>
<p>or</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.to_markdown(<span class="string">&#x27;table.md&#x27;</span>)</span><br><span class="line">df.to_latex(<span class="string">&#x27;table.tex&#x27;</span>)</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<p>也可以自定义输出latex格式，如表格宽度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.to_latex(<span class="string">&#x27;tb.tex&#x27;</span>,column_format=<span class="string">&#x27;lp&#123;1.8cm&#125;p&#123;1.8cm&#125;p&#123;1.8cm&#125;p&#123;1.8cm&#125;p&#123;1.8cm&#125;p&#123;1.8cm&#125;&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>除此以外，dataframe还可以保存为图片。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> dataframe_image <span class="keyword">as</span> dfi</span><br><span class="line">dfi.export(obj = df, filename = <span class="string">&#x27;table.jpg&#x27;</span>, fontsize=<span class="number">15</span>)</span><br></pre></td></tr></table></figure>
<h4 id="2-DataFrame常用属性查询"><a href="#2-DataFrame常用属性查询" class="headerlink" title="2.DataFrame常用属性查询"></a>2.DataFrame常用属性查询</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.values	<span class="comment">#值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.index	<span class="comment">#索引号</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.columns	<span class="comment">#列标签</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.dtypes	<span class="comment">#数据类型</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.shape	<span class="comment">#形状(几行几列)</span></span><br></pre></td></tr></table></figure>
<h4 id="3-DataFrame常用基本函数"><a href="#3-DataFrame常用基本函数" class="headerlink" title="3.DataFrame常用基本函数"></a>3.DataFrame常用基本函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.head(<span class="number">5</span>)	<span class="comment">#前5行</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.tail(<span class="number">5</span>)	<span class="comment">#后5行</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.info()	<span class="comment">#信息概况</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.describe()	<span class="comment">#主要统计量(count、mean、std、max、min、quartile)</span></span><br></pre></td></tr></table></figure>
<h4 id="4-DataFrame唯一值函数"><a href="#4-DataFrame唯一值函数" class="headerlink" title="4.DataFrame唯一值函数"></a>4.DataFrame唯一值函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].unique()	<span class="comment">#唯一值组成的数组</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].nunique()    <span class="comment">#唯一值个数</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].value_counts()    <span class="comment">#唯一值及其频数</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.describe()    <span class="comment">#主要统计量(count、mean、std、max、min、quartile)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].duplicated()    <span class="comment">#重复行的布尔值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[df[<span class="string">&#x27;psavert&#x27;</span>].duplicated()]    <span class="comment">#单列去重(删除重复行)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.drop_duplicates(subset=[<span class="string">&#x27;psavert&#x27;</span>, <span class="string">&#x27;pop&#x27;</span>], keep=<span class="string">&#x27;first&#x27;</span>)    <span class="comment">#多列去重(保留first唯一值)</span></span><br></pre></td></tr></table></figure>
<h4 id="5-DataFrame替换函数"><a href="#5-DataFrame替换函数" class="headerlink" title="5.DataFrame替换函数"></a>5.DataFrame替换函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].replace(<span class="number">12.5</span>, <span class="string">&#x27;A&#x27;</span>, inplace = <span class="literal">True</span>)    <span class="comment">#替换某列的单个值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].replace(&#123;<span class="number">12.5</span>:<span class="string">&#x27;A&#x27;</span>, <span class="number">11.7</span>:<span class="string">&#x27;B&#x27;</span>&#125;, inplace = <span class="literal">True</span>)    <span class="comment">#替换某列的多个值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;date&#x27;</span>].replace(&#123;<span class="string">r&#x27;2\d+&#x27;</span>: <span class="string">&#x27;The 21st century&#x27;</span>&#125;, regex=<span class="literal">True</span>, inplace = <span class="literal">True</span>)    <span class="comment">#正则替换</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].mask(df[<span class="string">&#x27;psavert&#x27;</span>]&gt;<span class="number">12.0</span> ,<span class="string">&#x27;A&#x27;</span>, inplace = <span class="literal">True</span>)    <span class="comment">#条件符合，进行替换</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].where(df[<span class="string">&#x27;psavert&#x27;</span>]&lt;<span class="number">12.0</span> ,<span class="string">&#x27;A&#x27;</span>, inplace = <span class="literal">True</span>)    <span class="comment">#条件不符合，进行替换</span></span><br></pre></td></tr></table></figure>
<h4 id="5-DataFrame排序函数"><a href="#5-DataFrame排序函数" class="headerlink" title="5.DataFrame排序函数"></a>5.DataFrame排序函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.sort_values(<span class="string">&#x27;psavert&#x27;</span>,ascending = <span class="literal">False</span>)    <span class="comment">#单列降序排序</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.sort_values([<span class="string">&#x27;psavert&#x27;</span>,<span class="string">&#x27;uempmed&#x27;</span>],ascending=[<span class="literal">True</span>,<span class="literal">False</span>])    <span class="comment">#前者升序情况下，后降序</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>今天先写到这，下一期接着写DataFrame的apply方法。</p>
</blockquote>
]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>DataFrame</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas的一些常用操作_2</title>
    <url>/2021/02/21/pandas_2/</url>
    <content><![CDATA[<blockquote>
<p>今天继续介绍几个常用的Pandas操作。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;./economics.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="1-DataFrame的apply方法"><a href="#1-DataFrame的apply方法" class="headerlink" title="1.DataFrame的apply方法"></a>1.DataFrame的apply方法</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[[<span class="string">&#x27;psavert&#x27;</span>,<span class="string">&#x27;uempmed&#x27;</span>]].apply(<span class="keyword">lambda</span> x:x.<span class="built_in">max</span>()-x.<span class="built_in">min</span>(), axis=<span class="number">1</span>)<span class="comment">#axis=1 将函数应用到列</span></span><br></pre></td></tr></table></figure>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.applymap(<span class="keyword">lambda</span> x:x*<span class="number">10</span>)<span class="comment">#applymap 将函数应用到每个元素</span></span><br></pre></td></tr></table></figure>
<h4 id="2-DataFrame的分组"><a href="#2-DataFrame的分组" class="headerlink" title="2.DataFrame的分组"></a>2.DataFrame的分组</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupby(<span class="string">&#x27;unemploy&#x27;</span>)[<span class="string">&#x27;psavert&#x27;</span>].median()<span class="comment">#样例:df.groupby(分组依据)[数据来源].使用操作</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">condition = df.unemploy &gt; df.unemploy.mean()<span class="comment">#使用condition定义分组依据</span></span><br><span class="line">df.groupby(condition)[<span class="string">&#x27;psavert&#x27;</span>].mean()<span class="comment">#分为True和False两组</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupby([condition, df[<span class="string">&#x27;unemploy&#x27;</span>]])[<span class="string">&#x27;psavert&#x27;</span>].mean()<span class="comment">#True组和False两组分别细分</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupby([df[<span class="string">&#x27;unemploy&#x27;</span>], df[<span class="string">&#x27;uempmed&#x27;</span>]])[<span class="string">&#x27;psavert&#x27;</span>].mean()<span class="comment">#两级分组</span></span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gb = df.groupby([<span class="string">&#x27;unemploy&#x27;</span>])<span class="comment">#Groupby对象</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gb.size()<span class="comment">#每组的元素个数</span></span><br><span class="line"><span class="comment">#和DataFrame一样，Groupby对象也有max\idxmin\all\\nunique\quantile\prod等函数，这里不一一列举。</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gb.agg([<span class="string">&#x27;skew&#x27;</span>, <span class="string">&#x27;sum&#x27;</span>, <span class="string">&#x27;idxmax&#x27;</span>])<span class="comment">#agg聚合函数，查看每个分组的三个统计量</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gb.agg(<span class="keyword">lambda</span> x: x.mean()-x.<span class="built_in">min</span>())<span class="comment">#在agg中自定义函数</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gb.<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x.shape[<span class="number">0</span>] &gt; <span class="number">100</span>)<span class="comment">#组过滤</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gb.apply(<span class="keyword">lambda</span> x: x**<span class="number">2</span>)<span class="comment">#组的apply方法</span></span><br></pre></td></tr></table></figure>
<h4 id="3-DataFrame的连接"><a href="#3-DataFrame的连接" class="headerlink" title="3.DataFrame的连接"></a>3.DataFrame的连接</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df1 = df[<span class="number">0</span>:<span class="number">10</span>]</span><br><span class="line">df2 = df[<span class="number">10</span>:<span class="number">20</span>]</span><br><span class="line">df1.merge(df2, on=<span class="string">&#x27;date&#x27;</span>, how=<span class="string">&#x27;outer&#x27;</span>)<span class="comment">#merge表示关系型连接，包括左连接、右连接、内连接和外(全)连接</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.concat([df1, df2], axis=<span class="number">0</span>)<span class="comment">#concat是方向性连接，axis=0表示纵向连接</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_min</span>(<span class="params">x1, x2</span>):</span></span><br><span class="line">    <span class="built_in">min</span> = x1.where(x1&lt;x2, x1)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">min</span></span><br><span class="line">df1.combine(df2, choose_min)<span class="comment">#使用combine函数自定义连接规则</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>下一期是Pandas的常见数据处理，包括缺失数据、文本数据、分类数据和时序数据。</p>
</blockquote>
]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>DataFrame</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas的一些常用操作_3</title>
    <url>/2021/02/21/pandas_3/</url>
    <content><![CDATA[<blockquote>
<p>今天介绍Pandas对一些常见数据的处理方法。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;./economics.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="1-缺失数据处理"><a href="#1-缺失数据处理" class="headerlink" title="1.缺失数据处理"></a>1.缺失数据处理</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.isna()<span class="comment">#是否有缺失值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.isna().mean()<span class="comment">#缺失的比例 </span></span><br></pre></td></tr></table></figure>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[df.psavert.isna()]<span class="comment">#查看某列是否有缺失值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[df[[<span class="string">&#x27;psavert&#x27;</span>,<span class="string">&#x27;pop&#x27;</span>,<span class="string">&#x27;uempmed&#x27;</span>]].isna().<span class="built_in">any</span>(<span class="number">1</span>)]<span class="comment">#查看所有列至少有一个缺失值的行“any()至少有一个为空,all()都为空”</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[df[[<span class="string">&#x27;psavert&#x27;</span>,<span class="string">&#x27;pop&#x27;</span>,<span class="string">&#x27;uempmed&#x27;</span>]].notna().<span class="built_in">all</span>(<span class="number">1</span>)]<span class="comment">#查看所有没有缺失值的行</span></span><br><span class="line">df.loc[df[[<span class="string">&#x27;psavert&#x27;</span>,<span class="string">&#x27;pop&#x27;</span>,<span class="string">&#x27;uempmed&#x27;</span>]].notna().<span class="built_in">all</span>(<span class="number">1</span>)]<span class="comment">#查看所有没有缺失值的区域</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.dropna(axis = <span class="number">0</span>, how = <span class="string">&#x27;any&#x27;</span>, subset = [<span class="string">&#x27;psavert&#x27;</span>, <span class="string">&#x27;pop&#x27;</span>])<span class="comment">#axis=0(删除)行,how=&#x27;any&#x27;至少有一个缺失的行</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.dropna(axis = <span class="number">1</span>, thresh = df.shape[<span class="number">0</span>]-<span class="number">5</span>)<span class="comment">#删除超过5个缺失值的列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.fillna(method = <span class="string">&#x27;ffill&#x27;</span>, limit = <span class="number">1</span>)<span class="comment">#method=&#x27;ffill&#x27;用前面的元素填充/method=&#x27;bfill&#x27;用后面的元素填充,limit=1连续缺失值的最大填充次数为1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.fillna(df.mean())<span class="comment">#用每列的均值填充</span></span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.interpolate(limit_direction=<span class="string">&#x27;both&#x27;</span>, limit=<span class="number">1</span>)<span class="comment">#用线性插值填充(both为双向限制插值)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.interpolate(<span class="string">&#x27;nearest&#x27;</span>).values<span class="comment">#用最近邻插值填充</span></span><br></pre></td></tr></table></figure>
<p>此外，也可以<a href="https://blog.csdn.net/wj1298250240/article/details/103600075">使用KNN来填充缺失值</a>。</p>
<h4 id="2-文本数据处理"><a href="#2-文本数据处理" class="headerlink" title="2.文本数据处理"></a>2.文本数据处理</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts = pd.Series(df[<span class="string">&#x27;Area&#x27;</span>].values, index=df[<span class="string">&#x27;pct_2014&#x27;</span>])<span class="comment">#DataFrame转换为Series</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>[<span class="number">0</span>]<span class="comment">#查看第一个字符</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.<span class="built_in">len</span>()<span class="comment">#查看字符长度</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.split(<span class="string">&#x27;[aon]&#x27;</span>, n=<span class="number">3</span>, expand=<span class="literal">True</span>)<span class="comment">#从左到右拆分字符串，最大拆分次数3次，生成多列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.join(<span class="string">&#x27;-&#x27;</span>)<span class="comment">#每个字符用“-”连接</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.cat(ts2, sep=<span class="string">&#x27;-&#x27;</span>)<span class="comment">#合并两个字符Series，连接符为“-”</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.contains(<span class="string">&#x27;[a-z]u&#x27;</span>)<span class="comment">#查看包含正则模式的序列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.startswith(<span class="string">&#x27;A&#x27;</span>)<span class="comment">#查看以A为开始的序列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.endswith(<span class="string">&#x27;e&#x27;</span>)<span class="comment">#查看以e为结尾的序列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.match(<span class="string">&#x27;A|s&#x27;</span>)<span class="comment">#查看以A为开头,s为结尾的序列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.count(<span class="string">&#x27;[A|s]&#x27;</span>)<span class="comment">#查看以A为开头,s为结尾的序列数量</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.find(<span class="string">&#x27;de&#x27;</span>)<span class="comment">#从左往右寻找&#x27;de&#x27;,匹配返回位置索引,未找到返回&#x27;-1&#x27;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.rfind(<span class="string">&#x27;de&#x27;</span>)<span class="comment">#从右往左寻找</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.replace(<span class="string">&#x27;\s?&#x27;</span>, <span class="string">&#x27;LA&#x27;</span>, regex=<span class="literal">True</span>)<span class="comment">#使用正则进行字符串替换</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pat = <span class="string">&#x27;(\w+o)(\w+s)(\w+u)(\w+n)&#x27;</span></span><br><span class="line">ts.<span class="built_in">str</span>.extract(pat)<span class="comment">#拆分&#x27;o&#x27;,&#x27;s&#x27;,&#x27;u&#x27;,&#x27;n&#x27;为4列</span></span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.to_numeric(ts, errors=<span class="string">&#x27;ignore&#x27;</span>)<span class="comment">#将可以转为数值的字符转为数值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.pad(<span class="number">6</span>,<span class="string">&#x27;left&#x27;</span>,<span class="string">&#x27;*&#x27;</span>)<span class="comment">#选定字符串长度为6的填充为&quot;*&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.lstrip()<span class="comment">#去掉字符串左侧空格</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.zfill(<span class="number">8</span>)<span class="comment">#用0补足8位</span></span><br></pre></td></tr></table></figure>
<h4 id="3-分类数据"><a href="#3-分类数据" class="headerlink" title="3.分类数据"></a>3.分类数据</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = df.psavert.astype(<span class="string">&#x27;category&#x27;</span>)<span class="comment">#Dataframe转为category对象</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s.cat.categories<span class="comment">#查看分类对象属性</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s.cat.add_categories(<span class="string">&#x27;C1&#x27;</span>)<span class="comment">#增加一个类别</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s.cat.remove_categories(<span class="number">11.7</span>)<span class="comment">#删除一个类别</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = s.cat.rename_categories(&#123;<span class="string">&#x27;S1&#x27;</span>:<span class="string">&#x27;xxx&#x27;</span>&#125;)<span class="comment">#重命名类别及其值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s.cat.reorder_categories([<span class="string">&#x27;S1&#x27;</span>, <span class="string">&#x27;S2&#x27;</span>, <span class="string">&#x27;S3&#x27;</span>, <span class="string">&#x27;S4&#x27;</span>], ordered=<span class="literal">True</span>)<span class="comment">#设置排序</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.sort_values(<span class="string">&#x27;psavert&#x27;</span>)<span class="comment">#值排序</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.set_index(<span class="string">&#x27;psavert&#x27;</span>).sort_index()<span class="comment">#作为索引排序</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">res = df.psavert &lt;= <span class="string">&#x27;S4&#x27;</span><span class="comment">#比较顺序</span></span><br><span class="line">res</span><br></pre></td></tr></table></figure>
<h4 id="4-时序数据"><a href="#4-时序数据" class="headerlink" title="4.时序数据"></a>4.时序数据</h4><p>————-图——————–</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = pd.to_datetime(df.date)<span class="comment">#生成时间序列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s2 = pd.to_datetime([<span class="string">&#x27;2020\\1\\1&#x27;</span>,<span class="string">&#x27;2020\\1\\3&#x27;</span>],<span class="built_in">format</span>=<span class="string">&#x27;%Y\\%m\\%d&#x27;</span>)<span class="comment">#强制格式转换,生成时间序列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.date_range(<span class="string">&#x27;1967-07-01&#x27;</span>,<span class="string">&#x27;2015-04-01&#x27;</span>, freq=<span class="string">&#x27;D&#x27;</span>)<span class="comment">#查看两个时间之间的时间,间隔1天</span></span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#dt对象</span></span><br><span class="line">s.dt.daysinmonth<span class="comment">#每个月几天</span></span><br><span class="line">s.dt.dayofweek<span class="comment">#每周几天</span></span><br><span class="line">s.dt.dayofweek.isin([<span class="number">5</span>,<span class="number">6</span>])<span class="comment">#是否包含双休日</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.Timestamp(<span class="string">&#x27;20210223 22:00:00&#x27;</span>)-pd.Timestamp(<span class="string">&#x27;20210222 18:35:00&#x27;</span>)<span class="comment">#计算两个时间之差</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>DataFrame</tag>
      </tags>
  </entry>
  <entry>
    <title>图神经网络——基于图神经网络的节点表征学习</title>
    <url>/2021/06/23/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%8A%82%E7%82%B9%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>Graph的特征表示非常复杂：</p>
<blockquote>
<p>1.复杂的拓扑结构，较难从图像中的感受野提取有效信息；<br>2.无特定的节点顺序；<br>3.通常graph会是动态变化的， 且使用多模态特征。</p>
</blockquote>
<p>高质量的节点表征能够用于衡量节点的相似性，同时高质量的节点表征也是准确分类节点的前提。</p>
<p>本文以Cora论文引用网络数据集为例，对MLP、GCN、GAT三种神经网络的分类性能进行对比。首先载入数据集并定义可视化函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#载入数据集</span></span><br><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> Planetoid</span><br><span class="line"><span class="keyword">from</span> torch_geometric.transforms <span class="keyword">import</span> NormalizeFeatures</span><br><span class="line"></span><br><span class="line">dataset = Planetoid(root=<span class="string">&#x27;dataset&#x27;</span>, name=<span class="string">&#x27;Cora&#x27;</span>, transform=NormalizeFeatures())</span><br></pre></td></tr></table></figure>
<span id="more"></span>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#定义可视化函数，并观察整体数据分布</span></span><br><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize</span>(<span class="params">h, color</span>):</span></span><br><span class="line">    z = TSNE(n_components=<span class="number">2</span>).fit_transform(out.detach().cpu().numpy())</span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br><span class="line"></span><br><span class="line">    plt.scatter(z[:, <span class="number">0</span>], z[:, <span class="number">1</span>], s=<span class="number">70</span>, c=color, cmap=<span class="string">&quot;Set2&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">out = data.x</span><br><span class="line">visualize(out,data.y)</span><br></pre></td></tr></table></figure>
<p><img src="https://z3.ax1x.com/2021/06/28/RtavIs.png" alt="整张图"></p>
<h4 id="1-MLP-Multi-layer-Perceptron-在图节点分类中的应用"><a href="#1-MLP-Multi-layer-Perceptron-在图节点分类中的应用" class="headerlink" title="1. MLP(Multi-layer Perceptron)在图节点分类中的应用"></a>1. MLP(Multi-layer Perceptron)在图节点分类中的应用</h4><p>多层感知机（MLP，Multilayer Perceptron）也叫人工神经网络（ANN，Artificial Neural Network）.</p>
<h5 id="1-1-MLP代码"><a href="#1-1-MLP代码" class="headerlink" title="1.1 MLP代码"></a>1.1 MLP代码</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#构造MLP</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Linear</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MLP, self).__init__()</span><br><span class="line">        torch.manual_seed(<span class="number">2021</span>)</span><br><span class="line">        self.lin1 = Linear(dataset.num_features, hidden_channels)</span><br><span class="line">        self.lin2 = Linear(hidden_channels, dataset.num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.lin1(x)</span><br><span class="line">        x = x.relu()</span><br><span class="line">        x = F.dropout(x, p=<span class="number">0.5</span>, training=self.training)</span><br><span class="line">        x = self.lin2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">model = MLP(hidden_channels=<span class="number">16</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果：<br>MLP(<br>       (lin1): Linear(in_features=1433, out_features=16, bias=True)<br>       (lin2): Linear(in_features=16, out_features=7, bias=True)<br>)</p>
</blockquote>
<p>该MLP由两个线性层、一个ReLU非线性层和一个dropout组成。第一个线程层将1433维的节点表征嵌入(embedding)到低维空间中(hidden_channels=16)，第二个线性层将节点表征嵌入到类别空间中(num_classes=7)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#训练MLP</span></span><br><span class="line">model = MLP(hidden_channels=<span class="number">16</span>)</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()  <span class="comment"># Define loss criterion.</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.01</span>, weight_decay=<span class="number">5e-4</span>)  <span class="comment"># Define optimizer.</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>():</span></span><br><span class="line">    model.train()</span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># Clear gradients.</span></span><br><span class="line">    out = model(data.x)  <span class="comment"># Perform a single forward pass.</span></span><br><span class="line">    loss = criterion(out[data.train_mask], data.y[data.train_mask])  <span class="comment"># Compute the loss solely based on the training nodes.</span></span><br><span class="line">    loss.backward()  <span class="comment"># Derive gradients.</span></span><br><span class="line">    optimizer.step()  <span class="comment"># Update parameters based on gradients.</span></span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">201</span>):</span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        loss = train()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch: <span class="subst">&#123;epoch:03d&#125;</span>, Loss: <span class="subst">&#123;loss:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果:<br>Epoch: 050, Loss: 1.1777<br>Epoch: 100, Loss: 0.5491<br>Epoch: 150, Loss: 0.4577<br>Epoch: 200, Loss: 0.2876</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#测试训练后的MLP</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    out = model(data.x)</span><br><span class="line">    pred = out.argmax(dim=<span class="number">1</span>)  <span class="comment"># Use the class with highest probability.</span></span><br><span class="line">    test_correct = pred[data.test_mask] == data.y[data.test_mask]  <span class="comment"># Check against ground-truth labels.</span></span><br><span class="line">    test_acc = <span class="built_in">int</span>(test_correct.<span class="built_in">sum</span>()) / <span class="built_in">int</span>(data.test_mask.<span class="built_in">sum</span>())  <span class="comment"># Derive ratio of correct predictions.</span></span><br><span class="line">    <span class="keyword">return</span> test_acc</span><br><span class="line"></span><br><span class="line">test_acc = test()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Test Accuracy: <span class="subst">&#123;test_acc:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果：<br>Test Accuracy: 0.5850</p>
</blockquote>
<p>MLP的结果较差，是因为用于训练此神经网络的有标签节点数量过少，它对未见过的节点泛化能力很差。</p>
<h4 id="2-GCN-Graph-Convolutional-Network-在图节点分类中的应用"><a href="#2-GCN-Graph-Convolutional-Network-在图节点分类中的应用" class="headerlink" title="2 GCN(Graph Convolutional Network)在图节点分类中的应用"></a>2 GCN(Graph Convolutional Network)在图节点分类中的应用</h4><p>GCN，图卷积神经网络，本质上和CNN的作用一样，就是一个特征提取器，只不过它的对象是图数据。关键在于如何定义局部感受域:</p>
<ul>
<li>Spatial approach: 指定节点的边的方向;</li>
<li>Spectral approach: 通过图的拉普拉斯矩阵的特征值和特征向量对图结构进行处理.</li>
</ul>
<h5 id="2-1-GCN公式"><a href="#2-1-GCN公式" class="headerlink" title="2.1 GCN公式"></a>2.1 GCN公式</h5><p>$$<br>\mathbf{X}^{\prime} = \mathbf{\hat{D}}^{-1/2} \mathbf{\hat{A}}\mathbf{\hat{D}}^{-1/2} \mathbf{X} \mathbf{\Theta}<br>$$<br>其中$\mathbf{\hat{A}} = \mathbf{A} + \mathbf{I}$表示插入自环的邻接矩阵,$\mathbf{I}$是单位矩阵，$\hat{D}<em>{ii} = \sum</em>{j=0} \hat{A}<em>{ij}$表示$\mathbf{\hat{A}}$的对角线度矩阵。$\mathbf{\hat{D}}^{-1/2} \mathbf{\hat{A}}<br>\mathbf{\hat{D}}^{-1/2}$是对称归一化矩阵，它的节点式公式为：<br>$$<br>\mathbf{x}^{\prime}<em>i = \mathbf{\Theta} \sum</em>{j \in \mathcal{N}(v) \cup{ i }} \frac{e</em>{j,i}}{\sqrt{\hat{d}_j \hat{d}<em>i}} \mathbf{x}<em>j<br>$$<br>其中，$\hat{d}<em>i = 1 + \sum</em>{j \in \mathcal{N}(i)} e</em>{j,i}$，$e</em>{j,i}$表示从源节点$j$到目标节点$i$的边的对称归一化系数（默认值为1.0）。</p>
<h5 id="2-2-GCN代码"><a href="#2-2-GCN代码" class="headerlink" title="2.2 GCN代码"></a>2.2 GCN代码</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#构造GCN</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> GCNConv</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GCN</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(GCN, self).__init__()</span><br><span class="line">        torch.manual_seed(<span class="number">2021</span>)</span><br><span class="line">        self.conv1 = GCNConv(dataset.num_features, hidden_channels)</span><br><span class="line">        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, edge_index</span>):</span></span><br><span class="line">        <span class="comment"># x:输入节点特征,可以是节点特征矩阵或一维节点索引张量</span></span><br><span class="line">        <span class="comment"># edge_type:每条边的一维关系类型/索引</span></span><br><span class="line">        x = self.conv1(x, edge_index)</span><br><span class="line">        x = x.relu()</span><br><span class="line">        x = F.dropout(x, p=<span class="number">0.5</span>, training=self.training)</span><br><span class="line">        x = self.conv2(x, edge_index)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">model = GCN(hidden_channels=<span class="number">16</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果：<br>GCN(<br>      (conv1): GCNConv(1433, 16)<br>      (conv2): GCNConv(16, 7)<br>)</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#可视化未训练的GCN</span></span><br><span class="line">model = GCN(hidden_channels=<span class="number">16</span>)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">out = model(data.x, data.edge_index)</span><br><span class="line">visualize(out, color=data.y)</span><br></pre></td></tr></table></figure>
<p><img src="https://z3.ax1x.com/2021/06/28/RtaLqg.png" alt="未训练GCN"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#训练GCN</span></span><br><span class="line">model = GCN(hidden_channels=<span class="number">16</span>)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.01</span>, weight_decay=<span class="number">5e-4</span>)</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>():</span></span><br><span class="line">      model.train()</span><br><span class="line">      optimizer.zero_grad()  <span class="comment"># Clear gradients.</span></span><br><span class="line">      out = model(data.x, data.edge_index)  <span class="comment"># Perform a single forward pass.</span></span><br><span class="line">      loss = criterion(out[data.train_mask], data.y[data.train_mask])  <span class="comment"># Compute the loss solely based on the training nodes.</span></span><br><span class="line">      loss.backward()  <span class="comment"># Derive gradients.</span></span><br><span class="line">      optimizer.step()  <span class="comment"># Update parameters based on gradients.</span></span><br><span class="line">      <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">201</span>):</span><br><span class="line">    loss = train()</span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch: <span class="subst">&#123;epoch:03d&#125;</span>, Loss: <span class="subst">&#123;loss:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果：<br>Epoch: 050, Loss: 1.1346<br>Epoch: 100, Loss: 0.5471<br>Epoch: 150, Loss: 0.4021<br>Epoch: 200, Loss: 0.3391</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#测试</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">      model.<span class="built_in">eval</span>()</span><br><span class="line">      out = model(data.x, data.edge_index)</span><br><span class="line">      pred = out.argmax(dim=<span class="number">1</span>)  <span class="comment"># Use the class with highest probability.</span></span><br><span class="line">      test_correct = pred[data.test_mask] == data.y[data.test_mask]  <span class="comment"># Check against ground-truth labels.</span></span><br><span class="line">      test_acc = <span class="built_in">int</span>(test_correct.<span class="built_in">sum</span>()) / <span class="built_in">int</span>(data.test_mask.<span class="built_in">sum</span>())  <span class="comment"># Derive ratio of correct predictions.</span></span><br><span class="line">      <span class="keyword">return</span> test_acc</span><br><span class="line"></span><br><span class="line">test_acc = test()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Test Accuracy: <span class="subst">&#123;test_acc:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果:Test Accuracy: 0.8090</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#可视化训练后的GCN</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">out = model(data.x, data.edge_index)</span><br><span class="line">visualize(out, color=data.y)</span><br></pre></td></tr></table></figure>
<p><img src="https://z3.ax1x.com/2021/06/28/Rtajaj.png" alt="训练后GCN"></p>
<h4 id="3-GAT-Graph-Attention-Network-在图节点分类中的应用"><a href="#3-GAT-Graph-Attention-Network-在图节点分类中的应用" class="headerlink" title="3.GAT(Graph Attention Network)在图节点分类中的应用"></a>3.GAT(Graph Attention Network)在图节点分类中的应用</h4><p>GAT的提出解决了GCN存在的问题:</p>
<ul>
<li>GCN 假设图是无向的,因为利用了对称的拉普拉斯矩阵 (只有邻接矩阵 A 是对称的，拉普拉斯矩阵才可以正交分解)，不能直接用于有向图。</li>
<li>GCN 不能处理动态图,GCN 在训练时依赖于具体的图结构，测试的时候也要在相同的图上进行。因此只能处理 transductive 任务，不能处理 inductive 任务。</li>
<li>GCN 不能为每个邻居分配不同的权重,GCN 在卷积时对所有邻居节点均一视同仁，不能根据节点重要性分配不同的权重。<h5 id="3-1-GAT公式"><a href="#3-1-GAT公式" class="headerlink" title="3.1 GAT公式"></a>3.1 GAT公式</h5>图注意力算子:<br>$$<br>\mathbf{x}^{\prime}<em>i = \alpha</em>{i,i}\mathbf{\Theta}\mathbf{x}<em>{i} +<br>\sum</em>{j \in \mathcal{N}(i)} \alpha_{i,j}\mathbf{\Theta}\mathbf{x}<em>{j}<br>$$<br>注意力系数$\alpha</em>{i,j}$为:<br>$$<br>\alpha_{i,j} =<br>\frac{<br>\exp\left(\mathrm{LeakyReLU}\left(\mathbf{a}^{\top}<br>[\mathbf{\Theta}\mathbf{x}_i , \Vert , \mathbf{\Theta}\mathbf{x}<em>j]<br>\right)\right)}<br>{\sum</em>{k \in \mathcal{N}(i) \cup { i }}<br>\exp\left(\mathrm{LeakyReLU}\left(\mathbf{a}^{\top}<br>[\mathbf{\Theta}\mathbf{x}_i , \Vert , \mathbf{\Theta}\mathbf{x}_k]<br>\right)\right)}<br>$$<h5 id="3-2-GAT代码"><a href="#3-2-GAT代码" class="headerlink" title="3.2 GAT代码"></a>3.2 GAT代码</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#构造GAT</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Linear</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> GATConv</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GAT</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(GAT, self).__init__()</span><br><span class="line">        torch.manual_seed(<span class="number">2021</span>)</span><br><span class="line">        self.conv1 = GATConv(dataset.num_features, hidden_channels)</span><br><span class="line">        self.conv2 = GATConv(hidden_channels, dataset.num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, edge_index</span>):</span></span><br><span class="line">        x = self.conv1(x, edge_index)</span><br><span class="line">        x = x.relu()</span><br><span class="line">        x = F.dropout(x, p=<span class="number">0.5</span>, training=self.training)</span><br><span class="line">        x = self.conv2(x, edge_index)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">model = GAT(hidden_channels=<span class="number">16</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果:<br>GAT(</p>
<pre><code>(conv1): GATConv(1433, 16, heads=1)
(conv2): GATConv(16, 7, heads=1)
</code></pre>
<p>)</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#可视化未训练的GAT</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">out = model(data.x, data.edge_index)</span><br><span class="line">visualize(out, color=data.y)</span><br></pre></td></tr></table></figure>
<img src="https://z3.ax1x.com/2021/06/28/RtaqsS.png" alt="未训练GAT"></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#训练GAT</span></span><br><span class="line">model = GAT(hidden_channels=<span class="number">16</span>)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.01</span>, weight_decay=<span class="number">5e-4</span>)</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>():</span></span><br><span class="line">      model.train()</span><br><span class="line">      optimizer.zero_grad()  <span class="comment"># Clear gradients.</span></span><br><span class="line">      out = model(data.x, data.edge_index)  <span class="comment"># Perform a single forward pass.</span></span><br><span class="line">      loss = criterion(out[data.train_mask], data.y[data.train_mask])  <span class="comment"># 只根据训练节点计算损失</span></span><br><span class="line">      loss.backward()  <span class="comment"># Derive gradients.</span></span><br><span class="line">      optimizer.step()  <span class="comment"># 根据梯度更新参数</span></span><br><span class="line">      <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">201</span>):</span><br><span class="line">    loss = train()</span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch: <span class="subst">&#123;epoch:03d&#125;</span>, Loss: <span class="subst">&#123;loss:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果:<br>Epoch: 050, Loss: 0.8583<br>Epoch: 100, Loss: 0.3209<br>Epoch: 150, Loss: 0.2267<br>Epoch: 200, Loss: 0.1939</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#测试GAT</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">      model.<span class="built_in">eval</span>()</span><br><span class="line">      out = model(data.x, data.edge_index)</span><br><span class="line">      pred = out.argmax(dim=<span class="number">1</span>)  <span class="comment"># 选取概率最高的一类</span></span><br><span class="line">      test_correct = pred[data.test_mask] == data.y[data.test_mask]		<span class="comment"># 预测与真实对比</span></span><br><span class="line">      test_acc = <span class="built_in">int</span>(test_correct.<span class="built_in">sum</span>()) / <span class="built_in">int</span>(data.test_mask.<span class="built_in">sum</span>())	<span class="comment"># 准确率</span></span><br><span class="line">      <span class="keyword">return</span> test_acc</span><br><span class="line"></span><br><span class="line">test_acc = test()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Test Accuracy: <span class="subst">&#123;test_acc:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果:Test Accuracy: 0.7310</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#可视化训练后的GAT</span></span><br><span class="line">model = GAT(hidden_channels=<span class="number">16</span>)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">out = model(data.x, data.edge_index)</span><br><span class="line">visualize(out, color=data.y)</span><br></pre></td></tr></table></figure>
<p><img src="https://z3.ax1x.com/2021/06/28/RtaXZQ.png" alt="训练后GAT"></p>
<h4 id="4-总结"><a href="#4-总结" class="headerlink" title="4.总结"></a>4.总结</h4><p>GCN和GAT的结果都优于MLP,原因是他们同时考虑了节点自身信息与周围邻接节点的信息.</p>
<p>GCN和GAT的共同点:</p>
<ul>
<li>都遵循消息传递范式；</li>
<li>在邻接节点信息变换阶段，它们都对邻接节点做归一化和线性变换；</li>
<li>在邻接节点信息聚合阶段，它们都将变换后的邻接节点信息做求和聚合；</li>
<li>在中心节点信息变换阶段，它们都只是简单返回邻接节点信息聚合阶段的聚合结果。</li>
</ul>
<p>GCN和GAT的不同点在于归一化方法不同():</p>
<ul>
<li>GCN根据中心节点与邻接节点的度计算归一化系数;GAT根据中心节点与邻接节点的相似度计算归一化系数。</li>
<li>GCN的归一化方式依赖于图的拓扑结构：不同的节点会有不同的度，同时不同节点的邻接节点的度也不同，于是在一些应用中GCN图神经网络会表现出较差的泛化能力;GAT的归一化方式依赖于中心节点与邻接节点的相似度，相似度是训练得到的，因此不受图的拓扑结构的影响，在不同的任务中都会有较好的泛化表现。</li>
</ul>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://github.com/datawhalechina/team-learning-nlp/tree/master/GNN">datawhale-GNN开源学习资料</a><br>2.<a href="https://zhuanlan.zhihu.com/p/306261981">知乎-图节点表征学习</a><br>3.<a href="https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.RGCNConv">GCNConv官方文档</a><br>4.<a href="https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GATConv">GATConv官方文档</a><br>5.<a href="https://baijiahao.baidu.com/s?id=1671028964544884749&wfr=spider&for=pc">GAT图注意力网络</a></p>
]]></content>
      <categories>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>PyG</tag>
        <tag>Graph</tag>
        <tag>MLP</tag>
        <tag>GCN</tag>
        <tag>GAT</tag>
      </tags>
  </entry>
  <entry>
    <title>Tsfresh——自动化特征工程工具</title>
    <url>/2021/03/25/tsfresh/</url>
    <content><![CDATA[<blockquote>
<p>改进模型的潜在途径之一是：生成更多的潜在特征，输入更多的样本。</p>
</blockquote>
<p>Tsfresh是处理时间序列数据的特征工程工具，能够自动计算大量时间序列特征，如平均值、最大值、峰度等。之后，可以使用这些特征集构建机器学习模型。</p>
<p>本文以<em>天池-心跳信号分类预测</em>为例，演示tsfresh工具的用法。</p>
<h2 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h2><h5 id="1-合并train和test数据"><a href="#1-合并train和test数据" class="headerlink" title="1. 合并train和test数据"></a>1. 合并train和test数据</h5><p>合并数据集，对整体数据做统一的特征工程。(注意需要为test数据添加label列，值为-1，方便后续操作)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_test[<span class="string">&#x27;label&#x27;</span>] = -<span class="number">1</span></span><br><span class="line">all_data = pd.concat((data_train, data_test)).reset_index(drop = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h5 id="2-对原特征一列拆成多列，并为每条数据添加时间特征time"><a href="#2-对原特征一列拆成多列，并为每条数据添加时间特征time" class="headerlink" title="2. 对原特征一列拆成多列，并为每条数据添加时间特征time"></a>2. 对原特征一列拆成多列，并为每条数据添加时间特征time</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_heatbeat_df = all_data[<span class="string">&#x27;heartbeat_signals&#x27;</span>].<span class="built_in">str</span>.split(<span class="string">&#x27;,&#x27;</span>, expand = <span class="literal">True</span>).stack()</span><br></pre></td></tr></table></figure>
<h5 id="3-Index处理"><a href="#3-Index处理" class="headerlink" title="3. Index处理"></a>3. Index处理</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_heatbeat_df = all_heatbeat_df.reset_inex()</span><br><span class="line">all_heatbeat_df = all_heatbeat_df.set_inex(<span class="string">&#x27;level_0&#x27;</span>)</span><br><span class="line">all_heatbeat_df.index.name = <span class="literal">None</span></span><br><span class="line">all_heatbeat_df.rename(columns=&#123;<span class="string">&#x27;level_1&#x27;</span>:<span class="string">&#x27;time&#x27;</span>, <span class="number">0</span>:<span class="string">&#x27;heartbeat_signals&#x27;</span>, inpalce = <span class="literal">True</span>&#125;)</span><br><span class="line">all_heatbeat_df[<span class="string">&#x27;heartbeat_signals&#x27;</span>].astype(<span class="built_in">float</span>)</span><br></pre></td></tr></table></figure>

<h5 id="4-label列单独存储，不进入tsfresh"><a href="#4-label列单独存储，不进入tsfresh" class="headerlink" title="4. label列单独存储，不进入tsfresh"></a>4. label列单独存储，不进入tsfresh</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_data_label = all_data[<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">all_data = all_data.drop[<span class="string">&#x27;label&#x27;</span>, axis=<span class="number">1</span>].drop[<span class="string">&#x27;heartbeat_signals&#x27;</span>, axis=<span class="number">1</span>]</span><br><span class="line">all_data = all_data.join(all_heatbeat_df)</span><br></pre></td></tr></table></figure>

<h5 id="5-tsfresh特征抽取"><a href="#5-tsfresh特征抽取" class="headerlink" title="5. tsfresh特征抽取"></a>5. tsfresh特征抽取</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tsfresh <span class="keyword">import</span> extract_features</span><br><span class="line">all_features = extract_features(all_data, column_id=<span class="string">&#x27;id&#x27;</span>, column_sort=<span class="string">&#x27;time&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h5 id="6-特征选择"><a href="#6-特征选择" class="headerlink" title="6.特征选择"></a>6.特征选择</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#删除nan值</span></span><br><span class="line"><span class="keyword">from</span> tsfresh.utilities.dataframe_functions <span class="keyword">import</span> impute</span><br><span class="line">impute(all_features)</span><br></pre></td></tr></table></figure>

<h5 id="7-相关性特征提取"><a href="#7-相关性特征提取" class="headerlink" title="7.相关性特征提取"></a>7.相关性特征提取</h5><p>衍生众多特征之后，许多特征之间可能有很多相关性，需进一步筛选。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tsfresh <span class="keyword">import</span> select_features</span><br><span class="line">all_features_filtered = select_features(all_features, all_data_label)</span><br></pre></td></tr></table></figure>
<h5 id="8-特征重命名，重新添加label"><a href="#8-特征重命名，重新添加label" class="headerlink" title="8.特征重命名，重新添加label"></a>8.特征重命名，重新添加label</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num = all_features_filtered.columns.size</span><br><span class="line">all_features_filtered.columns = [<span class="string">&#x27;f_&#x27;</span> + <span class="built_in">str</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span> (<span class="number">0</span>, num)]</span><br><span class="line">all_features_filtered[<span class="string">&#x27;label&#x27;</span>] = all_data_label</span><br></pre></td></tr></table></figure>

<h4 id="tsfresh包深入探究"><a href="#tsfresh包深入探究" class="headerlink" title="tsfresh包深入探究"></a>tsfresh包深入探究</h4><h5 id="1-筛选特征的方法"><a href="#1-筛选特征的方法" class="headerlink" title="1. 筛选特征的方法"></a>1. 筛选特征的方法</h5><p>上文采用了手工打标签的方式划分训练集和测试集，略显麻烦，可以通过tsfresh的内置方法来提取训练数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">kind_to_fc_parameters = tsfresh.feature_extration.settings.from_columns(directed_features)</span><br></pre></td></tr></table></figure>
<p>生成的params为训练集的特征字典，可以当作参数传入后续计算中。</p>
<h5 id="2-自定义特征衍生规则"><a href="#2-自定义特征衍生规则" class="headerlink" title="2. 自定义特征衍生规则"></a>2. 自定义特征衍生规则</h5><p>tsfresh自带的衍生规则以字典的形式存放，可以直接调用，也可以自定义。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#单个特征计算</span></span><br><span class="line">tsfresh.feature_extraction.feature_calculators.abs_energy(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">#设定衍生规则</span></span><br><span class="line"><span class="keyword">from</span> tsfresh.featureextraction <span class="keyword">import</span> extractfeatures</span><br><span class="line">params = &#123;<span class="string">&#x27;fft_coefficient:[&#123;&#x27;</span>coe<span class="string">f&#x27;:0, &#x27;</span>att<span class="string">r&#x27;:&#x27;</span><span class="built_in">abs</span><span class="string">&#x27;&#125;], &#x27;</span>kurtosis<span class="string">&#x27;: None, &#x27;</span>skewness<span class="string">&#x27;: None&#125;</span></span><br></pre></td></tr></table></figure>

<p>后续可以在extractfeatures中设置defaultparameters = params，具体用法见<a href="https://tsfresh.readthedocs.io/en/latest/text/feature_extraction_settings.html">操作文档</a>。</p>
<h5 id="3-减小内存使用"><a href="#3-减小内存使用" class="headerlink" title="3. 减小内存使用"></a>3. 减小内存使用</h5><p>tsfresh默认参数太吃内存，且耗时长，笔者i7的16G笔记本最多只能跑到60%的进度就会卡住，天池notebook和谷歌colab都是没跑出结果就断线了，硬要跑的话只能租个高配服务器，或者调整衍生特征的数量，通过<strong>chunksize</strong>设置，具体用法见<a href="https://tsfresh.readthedocs.io/en/latest/text/feature_extraction_settings.html">操作文档</a>。</p>
]]></content>
      <categories>
        <category>Feature engineering</category>
      </categories>
      <tags>
        <tag>tsfresh</tag>
      </tags>
  </entry>
  <entry>
    <title>图神经网络——基本图论与PyG库</title>
    <url>/2021/07/05/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9B%BE%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h3 id="图神经网络——基于图神经网络的图表征学习方法"><a href="#图神经网络——基于图神经网络的图表征学习方法" class="headerlink" title="图神经网络——基于图神经网络的图表征学习方法"></a>图神经网络——基于图神经网络的图表征学习方法</h3><p>图表征学习要求在输入节点属性、边和边的属性（如果有的话）得到一个向量作为图的表征，基于图表征进一步的我们可以做图的预测，而图同构网络（Graph Isomorphism Network, GIN）的图表征网络是当前最经典的图表征学习网络。</p>
<h4 id="1-GNN的邻域聚合-消息传递"><a href="#1-GNN的邻域聚合-消息传递" class="headerlink" title="1.GNN的邻域聚合(消息传递)"></a>1.GNN的邻域聚合(消息传递)</h4><p>GNN的目标是以图结构数据和节点特征作为输入，以学习到节点（或图）的embedding，用于分类任务。<br>基于邻域聚合的GNN可以拆分为以下三个模块：</p>
<ul>
<li>Aggregate：聚合一阶邻域特征。</li>
<li>Combine：将邻居聚合的特征 与 当前节点特征合并， 以更新当前节点特征。</li>
<li>Readout（可选）：如果是对graph分类，需要将graph中所有节点特征转变成graph特征。</li>
</ul>
<p>但是Aggregate的三种方式sum、mean、max的表征能力不够强大。</p>
<span id="more"></span>

<p><img src="https://i.loli.net/2021/07/12/XQxKfkPsL14g2O5.jpg" alt="FFFF.jpg"><br>如上图，节点v和v’为中心节点，通过聚合邻居特征生成embedding来分析不同aggregate设置下是否能区分不同的结构。设红绿蓝色节点特征值分别为r,g,b，不考虑combine。</p>
<blockquote>
<p>图a中：<br>mean：左$\frac{1}{2}(b+b)=b$，右$\frac{1}{3}(b+b+b)=b$，无法区分；<br>max：左$b$，右$b$，无法区分；<br>sum：左$2b$，右$3b$，可以区分。</p>
</blockquote>
<blockquote>
<p>图b中：<br>mean：左$\frac{1}{2}(r+g)$，右$\frac{1}{3}(g+2r)=b$，可以区分；<br>max：左$max(r,g)$，右$max(r,r,g)$，无法区分；<br>sum：左$r+g$，右$2r+g$，可以区分。</p>
</blockquote>
<blockquote>
<p>图c中：<br>mean：左$\frac{1}{2}(r+g)$，右$\frac{1}{4}(2g+2r)=b$，无法区分；<br>max：左$max(r,g)$，右$max(r,r,g,g)$，无法区分；<br>sum：左$r+g$，右$2r+2g$，可以区分。</p>
</blockquote>
<p>这说明，sum基本可以学习精确的结构信息、mean偏向学习分布信息，max偏向学习有代表性的元素信息，无法区分某些结构的图，故性能会比sum差一点。</p>
<h4 id="2-Weisfeiler-Lehman-Test-WL-Test"><a href="#2-Weisfeiler-Lehman-Test-WL-Test" class="headerlink" title="2.Weisfeiler-Lehman Test (WL Test)"></a>2.Weisfeiler-Lehman Test (WL Test)</h4><p>图的同构性测试算法（Weisfeiler-Lehman），简称WL Test，是一种用于测试两个图是否同构的算法。<br>WL Test 的一维形式，类似GNN中的邻接节点聚合。WL Test先迭代地聚合节点及其邻接节点的标签，然后将聚合的标签散列（hash）成新标签，该过程形式化为下方的公示，<br>$$<br>L^{h}<em>{u} \leftarrow \operatorname{hash}\left(L^{h-1}</em>{u} + \sum_{v \in \mathcal{N}(U)} L^{h-1}<em>{v}\right)<br>$$<br>上式中，$L^{h}</em>{u}$表示节点$u$的第$h$次迭代的标签，第$0$次迭代的标签为节点原始标签。</p>
<p>在迭代过程中，发现两个图之间的节点的标签不同时，就可以确定这两个图是非同构的。需要注意的是节点标签可能的取值只能是有限个数。</p>
<p>WL测试不能保证对所有图都有效，特别是对于具有高度对称性的图，如链式图、完全图、环图和星图，它会判断错误。</p>
<p>给定两个图$G$和$G^{\prime}$，每个节点拥有标签（实际中，一些图没有节点标签，我们可以以节点的度作为标签）。</p>
<p><img src="https://i.loli.net/2021/07/12/yr3axiC4PFOdqkJ.png" alt="C1.png"></p>
<p><strong>Weisfeiler-Leman Test 算法通过重复执行以下给节点打标签的过程来实现图是否同构的判断</strong>：</p>
<ol>
<li>聚合自身与邻接节点的标签得到一串字符串，自身标签与邻接节点的标签中间用<code>,</code>分隔，邻接节点的标签按升序排序。<strong>排序的原因在于要保证单射性，即保证输出的结果不因邻接节点的顺序改变而改变。</strong></li>
</ol>
<p><img src="https://i.loli.net/2021/07/12/wREO2ejIZMWicHv.png" alt="C2.png"></p>
<ol start="2">
<li><strong>标签散列，即标签压缩，将较长的字符串映射到一个简短的标签。</strong></li>
</ol>
<p><img src="https://i.loli.net/2021/07/12/pgdboxL6BKAr9Rz.png" alt="C3.png"></p>
<ol start="3">
<li><strong>给节点重新打上标签。</strong><br><img src="https://i.loli.net/2021/07/12/TOFRUV2INngGCxo.png" alt="C4.png"></li>
</ol>
<p>每重复一次以上的过程，就完成一次节点自身标签与邻接节点标签的聚合。</p>
<p><strong>当出现两个图相同节点标签的出现次数不一致时，即可判断两个图不相似</strong>。如果上述的步骤重复一定的次数后，没有发现有相同节点标签的出现次数不一致的情况，那么我们无法判断两个图是否同构。</p>
<p>当两个节点的$h$层的标签一样时，表示分别以这两个节点为根节点的WL子树是一致的。<strong>WL子树与普通子树不同</strong>，WL子树包含重复的节点。下图展示了一棵以1节点为根节点高为2的WL子树。</p>
<p><img src="https://i.loli.net/2021/07/12/KVkxiSlCvDf63F8.png" alt="C5.png"></p>
<h4 id="3-图相似性评估"><a href="#3-图相似性评估" class="headerlink" title="3.图相似性评估"></a>3.图相似性评估</h4><p>WL Test只能判断两个图的相似性，无法衡量图之间的相似性。要衡量两个图的相似性，需要用WL Subtree Kernel方法。该方法的思想是用WL Test算法得到节点的多层的标签，分别统计图中各类标签出现的次数，存于一个向量，这个向量可以作为图的表征<strong>。</strong>两个图的表征向量的内积，即可作为这两个图的相似性估计，内积越大表示相似性越高。</p>
<p><img src="https://i.loli.net/2021/07/12/IbyZ7h8JEXYRk3t.jpg" alt="gggg.jpg"></p>
<h4 id="4-图同构网络模型的构建"><a href="#4-图同构网络模型的构建" class="headerlink" title="4.图同构网络模型的构建"></a>4.图同构网络模型的构建</h4><p>通过GIN学习的节点表征向量可以用于类似于节点分类、边预测这样的任务。而对于图分类任务。READOUT函数：给定独立的节点的表征向量集，生成整个图的表征向量。</p>
<p>GIN的READOUT模块使用concat+sum，对每次迭代得到的所有节点特征求和得到图的特征，然后拼接起来，公式如下：<br>$$<br>h_{G} = \text{CONCAT}(\text{READOUT}\left({h_{v}^{(k)}|v\in G}\right)|k=0,1,\cdots, K)<br>$$</p>
<h4 id="5-基于图同构网络（GIN）的图表征网络的实现"><a href="#5-基于图同构网络（GIN）的图表征网络的实现" class="headerlink" title="5.基于图同构网络（GIN）的图表征网络的实现"></a>5.基于图同构网络（GIN）的图表征网络的实现</h4><p>基于图同构网络的图表征学习主要包含以下两个过程：</p>
<ol>
<li>计算得到节点表征；</li>
<li>对图上各个节点的表征做图池化（Graph Pooling），或称为图读出（Graph Readout），得到图的表征（Graph Representation）。</li>
</ol>
<p>基于图同构网络的图表征模块（GINGraphRepr Module），首先采用<code>GINNodeEmbedding</code>模块对图上每一个节点做节点嵌入（Node Embedding），得到节点表征；然后对节点表征做图池化得到图的表征；最后用一层线性变换对图表征转换为对图的预测。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> global_add_pool, global_mean_pool, global_max_pool, GlobalAttention, Set2Set</span><br><span class="line"><span class="keyword">from</span> gin_node <span class="keyword">import</span> GINNodeEmbedding</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GINGraphRepr</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_tasks=<span class="number">1</span>, num_layers=<span class="number">5</span>, emb_dim=<span class="number">300</span>, residual=<span class="literal">False</span>, drop_ratio=<span class="number">0</span>, JK=<span class="string">&quot;last&quot;</span>, graph_pooling=<span class="string">&quot;sum&quot;</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        num_tasks(int, optional)：图表征的维度；</span></span><br><span class="line"><span class="string">    	num_layers(int, optional)：卷积层数；</span></span><br><span class="line"><span class="string">        emb_dim(int, optional)：node embedding的维度；</span></span><br><span class="line"><span class="string">        residual(bool, optional)：是否添加剩余的连接；</span></span><br><span class="line"><span class="string">        drop_ratio (float, optional)：dropout rate；</span></span><br><span class="line"><span class="string">        JK (str, optional)：可选的值为&quot;last&quot;和&quot;sum&quot;。选&quot;last&quot;，只取最后一层的结点的嵌入，选&quot;sum&quot;对各层的结点的嵌入求和。</span></span><br><span class="line"><span class="string">        raph_pooling (str, optional)：node embedding的池化方法，可选的值为&quot;sum&quot;，&quot;mean&quot;，&quot;max&quot;，&quot;attention&quot;和&quot;set2set&quot;。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(GINGraphPooling, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.drop_ratio = drop_ratio</span><br><span class="line">        self.JK = JK</span><br><span class="line">        self.emb_dim = emb_dim</span><br><span class="line">        self.num_tasks = num_tasks</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.num_layers &lt; <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Number of GNN layers must be greater than 1.&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.gnn_node = GINNodeEmbedding(num_layers, emb_dim, JK=JK, drop_ratio=drop_ratio, residual=residual)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Pooling function to generate whole-graph embeddings</span></span><br><span class="line">        <span class="keyword">if</span> graph_pooling == <span class="string">&quot;sum&quot;</span>:</span><br><span class="line">            self.pool = global_add_pool</span><br><span class="line">        <span class="keyword">elif</span> graph_pooling == <span class="string">&quot;mean&quot;</span>:</span><br><span class="line">            self.pool = global_mean_pool</span><br><span class="line">        <span class="keyword">elif</span> graph_pooling == <span class="string">&quot;max&quot;</span>:</span><br><span class="line">            self.pool = global_max_pool</span><br><span class="line">        <span class="keyword">elif</span> graph_pooling == <span class="string">&quot;attention&quot;</span>:</span><br><span class="line">            self.pool = GlobalAttention(gate_nn=nn.Sequential(</span><br><span class="line">                nn.Linear(emb_dim, emb_dim), nn.BatchNorm1d(emb_dim), nn.ReLU(), nn.Linear(emb_dim, <span class="number">1</span>)))</span><br><span class="line">        <span class="keyword">elif</span> graph_pooling == <span class="string">&quot;set2set&quot;</span>:</span><br><span class="line">            self.pool = Set2Set(emb_dim, processing_steps=<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid graph pooling type.&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> graph_pooling == <span class="string">&quot;set2set&quot;</span>:</span><br><span class="line">            self.graph_pred_linear = nn.Linear(<span class="number">2</span>*self.emb_dim, self.num_tasks)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.graph_pred_linear = nn.Linear(self.emb_dim, self.num_tasks)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, batched_data</span>):</span></span><br><span class="line">        h_node = self.gnn_node(batched_data)</span><br><span class="line"></span><br><span class="line">        h_graph = self.pool(h_node, batched_data.batch)</span><br><span class="line">        output = self.graph_pred_linear(h_graph)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            <span class="keyword">return</span> output</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># At inference time, relu is applied to output to ensure positivity</span></span><br><span class="line">            <span class="comment"># 因为预测目标的取值范围就在 (0, 50] 内</span></span><br><span class="line">            <span class="keyword">return</span> torch.clamp(output, <span class="built_in">min</span>=<span class="number">0</span>, <span class="built_in">max</span>=<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>attention：基于Attention对节点表征加权求和，使用模块 <a href="https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.glob.GlobalAttention">torch_geometric.nn.glob.GlobalAttention</a>。<br>set2set：另一种基于Attention对节点表征加权求和的方法，使用模块 <a href="https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.glob.Set2Set">torch_geometric.nn.glob.Set2Set</a>。</p>
</blockquote>
<h4 id="6-基于图同构网络的节点嵌入模块（GINNodeEmbedding-Module）"><a href="#6-基于图同构网络的节点嵌入模块（GINNodeEmbedding-Module）" class="headerlink" title="6.基于图同构网络的节点嵌入模块（GINNodeEmbedding Module）"></a>6.基于图同构网络的节点嵌入模块（GINNodeEmbedding Module）</h4><p>此节点嵌入模块基于多层<code>GINConv</code>实现结点嵌入的计算。首先用<code>AtomEncoder</code>对其做嵌入得到第0层节点表征然后我们逐层计算节点表征，从第1层开始到第num_layers层，每一层节点表征的计算都以上一层的节点表征<code>h_list[layer]</code>、边<code>edge_index</code>和边的属性<code>edge_attr</code>为输入。<br>需要注意的是，<code>GINConv</code>的层数越多，此节点嵌入模块的感受野（receptive field）越大，结点<code>i</code>的表征最远能捕获到结点<code>i</code>的距离为<code>num_layers</code>的邻接节点的信息。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> mol_encoder <span class="keyword">import</span> AtomEncoder</span><br><span class="line"><span class="keyword">from</span> gin_conv <span class="keyword">import</span> GINConv</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># GNN to generate node embedding</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GINNodeEmbedding</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Output:</span></span><br><span class="line"><span class="string">        node representations</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_layers, emb_dim, drop_ratio=<span class="number">0.5</span>, JK=<span class="string">&quot;last&quot;</span>, residual=<span class="literal">False</span></span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(GINNodeEmbedding, self).__init__()</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.drop_ratio = drop_ratio</span><br><span class="line">        self.JK = JK</span><br><span class="line">        self.residual = residual</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.num_layers &lt; <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Number of GNN layers must be greater than 1.&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.atom_encoder = AtomEncoder(emb_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># List of GNNs</span></span><br><span class="line">        self.convs = torch.nn.ModuleList()</span><br><span class="line">        self.batch_norms = torch.nn.ModuleList()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.convs.append(GINConv(emb_dim))</span><br><span class="line">            self.batch_norms.append(torch.nn.BatchNorm1d(emb_dim))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, batched_data</span>):</span></span><br><span class="line">        x, edge_index, edge_attr = batched_data.x, batched_data.edge_index, batched_data.edge_attr</span><br><span class="line"></span><br><span class="line">        <span class="comment"># computing input node embedding</span></span><br><span class="line">        h_list = [self.atom_encoder(x)]  <span class="comment"># 先将类别型原子属性转化为原子表征</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers):</span><br><span class="line">            h = self.convs[layer](h_list[layer], edge_index, edge_attr)</span><br><span class="line">            h = self.batch_norms[layer](h)</span><br><span class="line">            <span class="keyword">if</span> layer == self.num_layers - <span class="number">1</span>:</span><br><span class="line">                <span class="comment"># remove relu for the last layer</span></span><br><span class="line">                h = F.dropout(h, self.drop_ratio, training=self.training)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                h = F.dropout(F.relu(h), self.drop_ratio, training=self.training)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.residual:</span><br><span class="line">                h += h_list[layer]</span><br><span class="line"></span><br><span class="line">            h_list.append(h)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Different implementations of Jk-concat</span></span><br><span class="line">        <span class="keyword">if</span> self.JK == <span class="string">&quot;last&quot;</span>:</span><br><span class="line">            node_representation = h_list[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">elif</span> self.JK == <span class="string">&quot;sum&quot;</span>:</span><br><span class="line">            node_representation = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers + <span class="number">1</span>):</span><br><span class="line">                node_representation += h_list[layer]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> node_representation</span><br></pre></td></tr></table></figure>
<h4 id="7-图同构卷积层（GINConv）"><a href="#7-图同构卷积层（GINConv）" class="headerlink" title="7.图同构卷积层（GINConv）"></a>7.图同构卷积层（GINConv）</h4><p>图同构卷积层<code>GINConv</code>的数学定义如下：<br>$$<br>\mathbf{x}^{\prime}<em>i = h</em>{\mathbf{\Theta}} \left( (1 + \epsilon) \cdot\mathbf{x}<em>i + \sum</em>{j \in \mathcal{N}(i)} \mathbf{x}_j \right)<br>$$<br>构建代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> MessagePassing</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> ogb.graphproppred.mol_encoder <span class="keyword">import</span> BondEncoder</span><br><span class="line"></span><br><span class="line"><span class="comment"># GIN convolution along the graph structure</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GINConv</span>(<span class="params">MessagePassing</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, emb_dim</span>):</span></span><br><span class="line">        <span class="comment">#emb_dim (int): node embedding dimensionality</span></span><br><span class="line">        <span class="built_in">super</span>(GINConv, self).__init__(aggr = <span class="string">&quot;add&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.mlp = nn.Sequential(nn.Linear(emb_dim, emb_dim), nn.BatchNorm1d(emb_dim), nn.ReLU(), nn.Linear(emb_dim, emb_dim))</span><br><span class="line">        self.eps = nn.Parameter(torch.Tensor([<span class="number">0</span>]))</span><br><span class="line">        self.bond_encoder = BondEncoder(emb_dim = emb_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, edge_index, edge_attr</span>):</span></span><br><span class="line">        edge_embedding = self.bond_encoder(edge_attr) <span class="comment"># 先将类别型边属性转换为边表征</span></span><br><span class="line">        out = self.mlp((<span class="number">1</span> + self.eps) *x + self.propagate(edge_index, x=x, edge_attr=edge_embedding))</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">message</span>(<span class="params">self, x_j, edge_attr</span>):</span></span><br><span class="line">        <span class="keyword">return</span> F.relu(x_j + edge_attr)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">self, aggr_out</span>):</span></span><br><span class="line">        <span class="keyword">return</span> aggr_out</span><br></pre></td></tr></table></figure>

<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://github.com/datawhalechina/team-learning-nlp/tree/master/GNN">datawhale-GNN开源学习资料</a><br>2.<a href="https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#global-pooling-layers">Global Pooling Layers</a></p>
]]></content>
      <categories>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>WL Test</tag>
        <tag>GIN</tag>
      </tags>
  </entry>
  <entry>
    <title>图神经网络——基本图论与PyG库</title>
    <url>/2021/06/15/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E5%9B%BE%E8%AE%BA%E4%B8%8EPyG%E5%BA%93/</url>
    <content><![CDATA[<p>在以往的深度学习中，主要的数据形式包括矩阵、张量、序列(sequence)和时间序列(time series)，然而还有许多数据是图的结构，如社交网络、知识图谱等。图结构数据具有以下特点：</p>
<blockquote>
<p>任意的大小和复杂的拓扑结构；<br>没有固定的节点排序或参考点；<br>通常是动态的，并具有多模态的特征；<br>图的信息包括节点信息、边信息和拓扑结构信息。</p>
</blockquote>
<h4 id="1-图结构数据"><a href="#1-图结构数据" class="headerlink" title="1. 图结构数据"></a>1. 图结构数据</h4><h5 id="1-1-图-Graphs"><a href="#1-1-图-Graphs" class="headerlink" title="1.1 图(Graphs)"></a>1.1 图(Graphs)</h5><p><strong>·</strong> 一个图记作$\mathcal{G}={\mathcal{V}, \mathcal{E}}$，其中 $\mathcal{V}=\left{v_{1}, \ldots, v_{N}\right}$是数量为$N=|\mathcal{V}|$ 的节点的集合， $\mathcal{E}=\left{e_{1}, \ldots, e_{M}\right}$ 是数量为 $M$ 的边的集合。<br><strong>·</strong> 节点表示实体(entities)，边表示实体间的关系(relations)；节点和边的信息可以是类别型或数值型的。<br><strong>·</strong> 只有一种类型的节点和一种类型的边的图称为同质图(Homogeneous Graph)。<br><strong>·</strong> 存在多种类型的节点和多种类型的边的图称为异质图(Heterogeneous Graph)</p>
<span id="more"></span>

<h5 id="1-2-图的邻接矩阵-Adjacency-Matrix"><a href="#1-2-图的邻接矩阵-Adjacency-Matrix" class="headerlink" title="1.2 图的邻接矩阵(Adjacency Matrix)"></a>1.2 图的邻接矩阵(Adjacency Matrix)</h5><p><strong>·</strong> 图$\mathcal{G}={\mathcal{V}, \mathcal{E}}$，其对应的邻接矩阵记为$\mathbf{A} \in{0,1}^{N \times N}$。$\mathbf{A}<em>{i, j}=1$表示存在从节点$v_i$到$v_j$的边，反之表示不存在从节点$v_i$到$v_j$的边。<br><strong>·</strong> 在无向图中，节点$v_i$到$v_j$的边存在，则节点$v_j$到$v_i$的边也存在，所以无向图的邻接矩阵的对称的。<br><img src="https://z3.ax1x.com/2021/06/28/RtabM8.png" alt="图1"><br>其邻接矩阵为：<br><img src="https://z3.ax1x.com/2021/06/28/RtaoGt.png" alt="图2"><br><strong>·</strong> 在无权图中，各条边的权重是等价的，都为1。<br><strong>·</strong> 在有权图中，其对应的邻接矩阵通常被记为$\mathbf{W} \in{0,1}^{N \times N}$，其中$\mathbf{W}</em>{i, j}=w_{ij}$表示从节点$v_i$到$v_j$的边的权重。若边不存在时，边的权重为$0$。</p>
<h5 id="1-3-节点的度-degree"><a href="#1-3-节点的度-degree" class="headerlink" title="1.3 节点的度(degree)"></a>1.3 节点的度(degree)</h5><p><strong>·</strong> 在有向有权图中，节点$v_i$的出度(out degree)等于从$v_i$出发的边的权重之和，节点$v_i$的入度(in degree)等于从连向$v_i$的边的权重之和；节点$v_i$的度记为$d(v_i)$，入度记为$d_{in}(v_i)$，出度记为$d_{out}(v_i)$。<br><strong>·</strong> 在无向图中，出度=入度。<br><strong>·</strong> 在无权图中，入读等于连向$v_i$的边的数量，出度等于从$v_i$出发的边的数量。</p>
<h5 id="1-4-邻接节点-neighbors"><a href="#1-4-邻接节点-neighbors" class="headerlink" title="1.4 邻接节点(neighbors)"></a>1.4 邻接节点(neighbors)</h5><p><strong>·</strong> 与节点$v_i$直接相连的节点称为邻接节点，记为**$\mathcal{N(v_i)}$**。<br><strong>·</strong> 与节点$v_i$距离$k$步的称为$k$跳远的邻接节点(neighbors with $k$-hop) [一个节点的$2$跳远的邻接节点包含自身]</p>
<h5 id="1-5-行走-walk"><a href="#1-5-行走-walk" class="headerlink" title="1.5 行走(walk)"></a>1.5 行走(walk)</h5><p><strong>·</strong> $walk(v_1, v_2) = (v_1, e_6,e_5,e_4,e_1,v_2)$，这是一次“行走”，从节点$v_1$出发，依次经过边$e_6,e_5,e_4,e_1$，最终到达节点$v_2$的“行走”。<br><strong>·</strong> “行走”中，节点允许重复。<br><img src="https://z3.ax1x.com/2021/06/28/Rta7xf.png" alt="图3"></p>
<h5 id="1-6-路径-path"><a href="#1-6-路径-path" class="headerlink" title="1.6 路径(path)"></a>1.6 路径(path)</h5><p>“路径”是节点不可重复的“行走”。</p>
<h5 id="1-7-子图-subgraph"><a href="#1-7-子图-subgraph" class="headerlink" title="1.7 子图(subgraph)"></a>1.7 子图(subgraph)</h5><p>指节点集和边集分别是整图的节点集的子集和边集的子集的图。</p>
<h5 id="1-8-连通分量-connected-component"><a href="#1-8-连通分量-connected-component" class="headerlink" title="1.8 连通分量(connected component)"></a>1.8 连通分量(connected component)</h5><p>图$\mathcal{G}={\mathcal{V}, \mathcal{E}}$的子图为$\mathcal{G}^{\prime}$，记属于图$\mathcal{G}$但不属于$\mathcal{G}^{\prime}$图的节点集合记为$\mathcal{V}/\mathcal{V}^{\prime}$，属于$\mathcal{V}^{\prime}$的任意节点对之间存在至少一条路径，但不存在一条边连接属于$\mathcal{V}^{\prime}$的节点与属于$\mathcal{V}/\mathcal{V}^{\prime}$的节点，则图$\mathcal{G}^{\prime}$是图$\mathcal{G}$的连通分量。<br><img src="https://z3.ax1x.com/2021/06/28/RtaTRP.png" alt="图4"><br>左右两个子图都是整图的连通分量。</p>
<h5 id="1-8-连通图-connected-graph"><a href="#1-8-连通图-connected-graph" class="headerlink" title="1.8 连通图(connected graph)"></a>1.8 连通图(connected graph)</h5><p>只包含一个连通分量(即其自身)的图是一个连通图。</p>
<h5 id="1-9-最短路径-shortest-path"><a href="#1-9-最短路径-shortest-path" class="headerlink" title="1.9 最短路径(shortest path)"></a>1.9 最短路径(shortest path)</h5><p>$v_{s}, v_{t} \in \mathcal{V}$ 是图$\mathcal{G}={\mathcal{V}, \mathcal{E}}$上的一对节点，节点对$v_{s}, v_{t} \in \mathcal{V}$之间所有路径集合$\mathcal{P}<em>{\mathrm{st}}$。节点对$v</em>{s}, v_{t}$之间的最短路径$p_{\mathrm{s} t}^{\mathrm{sp}}$为$\mathcal{P}<em>{\mathrm{st}}$中长度最短的一条路径，即$p</em>{\mathrm{s} t}^{\mathrm{sp}}=\arg \min <em>{p \in \mathcal{P}</em>{\mathrm{st}}}|p|$，$p$表示$\mathcal{P}_{\mathrm{st}}$中的一条路径，$|p|$是路径$p$的长度(边数量×权重)。</p>
<h5 id="1-10-直径-diameter"><a href="#1-10-直径-diameter" class="headerlink" title="1.10 直径(diameter)"></a>1.10 直径(diameter)</h5><p>一个连通图$\mathcal{G}={\mathcal{V}, \mathcal{E}}$中，其直径为其所有节点对之间的最短路径的最大值，即$\operatorname{diameter}(\mathcal{G})=\max <em>{v</em>{s}, v_{t} \in \mathcal{V}} \min <em>{p \in \mathcal{P}</em>{s t}}|p|$</p>
<h5 id="1-11-拉普拉斯矩阵-Laplacian-Matrix"><a href="#1-11-拉普拉斯矩阵-Laplacian-Matrix" class="headerlink" title="1.11 拉普拉斯矩阵(Laplacian Matrix)"></a>1.11 拉普拉斯矩阵(Laplacian Matrix)</h5><p>图$\mathcal{G}={\mathcal{V}, \mathcal{E}}$，其邻接矩阵为$A$，其拉普拉斯矩阵定义为$\mathbf{L=D-A}$，其中$\mathbf{D=diag(d(v_1), \cdots, d(v_N))}$。</p>
<h5 id="1-12-对称归一化的拉普拉斯矩阵-Symmetric-normalized-Laplacian"><a href="#1-12-对称归一化的拉普拉斯矩阵-Symmetric-normalized-Laplacian" class="headerlink" title="1.12 对称归一化的拉普拉斯矩阵(Symmetric normalized Laplacian)"></a>1.12 对称归一化的拉普拉斯矩阵(Symmetric normalized Laplacian)</h5><p>图$\mathcal{G}={\mathcal{V}, \mathcal{E}}$，其邻接矩阵为$A$，其规范化的拉普拉斯矩阵定义为$\mathbf{L=D^{-\frac{1}{2}}(D-A)D^{-\frac{1}{2}}=I-D^{-\frac{1}{2}}AD^{-\frac{1}{2}}}$。</p>
<h4 id="2-图结构数据上的机器学习"><a href="#2-图结构数据上的机器学习" class="headerlink" title="2. 图结构数据上的机器学习"></a>2. 图结构数据上的机器学习</h4><blockquote>
<p><strong>节点预测</strong>：预测节点的类别或某类属性的取值。<br><strong>边预测</strong>：预测两个节点间是否存在链接。<br><strong>图预测</strong>：对不同的图进行分类或预测图的属性。<br><strong>节点聚类</strong>：检测节点是否形成一个类。</p>
</blockquote>
<h4 id="3-环境配置"><a href="#3-环境配置" class="headerlink" title="3. 环境配置"></a>3. 环境配置</h4><p><a href="https://pytorch-geometric.readthedocs.io/en/latest/">PyTorch Geometric</a> (PyG)是面向几何深度学习的PyTorch的扩展库，基于PyG库，我们可以轻松地根据数据生成一个图对象。</p>
<h5 id="3-1-安装的pytorch和cudatoolkit"><a href="#3-1-安装的pytorch和cudatoolkit" class="headerlink" title="3.1 安装的pytorch和cudatoolkit"></a>3.1 安装的pytorch和cudatoolkit</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda install pytorch torchvision torchaudio cudatoolkit&#x3D;11.1 -c pytorch -c nvidia</span><br></pre></td></tr></table></figure>
<p>安装成功后可以验证一下。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)</span><br><span class="line"><span class="built_in">print</span>(torch.version.cuda)</span><br></pre></td></tr></table></figure>
<h5 id="3-2-安装PyG"><a href="#3-2-安装PyG" class="headerlink" title="3.2 安装PyG"></a>3.2 安装PyG</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-<span class="number">1.8</span><span class="number">.0</span>+cu111.html</span><br><span class="line">pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-<span class="number">1.8</span><span class="number">.0</span>+cu111.html</span><br><span class="line">pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-<span class="number">1.8</span><span class="number">.0</span>+cu111.html</span><br><span class="line">pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-<span class="number">1.8</span><span class="number">.0</span>+cu111.html</span><br><span class="line">pip install torch-geometric</span><br></pre></td></tr></table></figure>

<h4 id="4-Data类——PyG中图的表示"><a href="#4-Data类——PyG中图的表示" class="headerlink" title="4. Data类——PyG中图的表示"></a>4. Data类——PyG中图的表示</h4><h5 id="4-1-构造函数"><a href="#4-1-构造函数" class="headerlink" title="4.1 构造函数"></a>4.1 构造函数</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Data</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, x=<span class="literal">None</span>, edge_index=<span class="literal">None</span>, edge_attr=<span class="literal">None</span>, y=<span class="literal">None</span>, **kwargs</span>):</span></span><br><span class="line">    self.x = x</span><br><span class="line">    self.edge_index = edge_index</span><br><span class="line">    self.edge_attr = edge_attr</span><br><span class="line">    self.y = y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> key, item <span class="keyword">in</span> kwargs.items():</span><br><span class="line">        <span class="keyword">if</span> key == <span class="string">&#x27;num_nodes&#x27;</span>:</span><br><span class="line">            self.__num_nodes__ = item</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self[key] = item</span><br></pre></td></tr></table></figure>
<p>Args:<br><strong>·</strong> x (Tensor, optional): 节点属性矩阵，大小为<code>[num_nodes, num_node_features]</code><br><strong>·</strong> edge_index (LongTensor, optional): 边索引矩阵，大小为<code>[2, num_edges]</code>，第0行为尾节点，第1行为头节点，头指向尾。<br><strong>·</strong> edge_attr (Tensor, optional): 边属性矩阵，大小为<code>[num_edges, num_edge_features]</code><br><strong>·</strong> y (Tensor, optional): 节点或图的标签，任意大小(其实也可以是边的标签)</p>
<h5 id="4-2-参数"><a href="#4-2-参数" class="headerlink" title="4.2 参数"></a>4.2 参数</h5><p>一个图至少包含<code>x, edge_index, edge_attr, y, num_nodes</code>5个属性，也可以指定额外参数使<code>Data</code>对象包含其他的属性：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">graph = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y, num_nodes=num_nodes, other_attr=other_attr)</span><br></pre></td></tr></table></figure>

<h5 id="4-3-Data对象与其他数据互转"><a href="#4-3-Data对象与其他数据互转" class="headerlink" title="4.3 Data对象与其他数据互转"></a>4.3 Data对象与其他数据互转</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#dict to Data</span></span><br><span class="line">graph_dict = &#123;</span><br><span class="line">    <span class="string">&#x27;x&#x27;</span>: x,</span><br><span class="line">    <span class="string">&#x27;edge_index&#x27;</span>: edge_index,</span><br><span class="line">    <span class="string">&#x27;edge_attr&#x27;</span>: edge_attr,</span><br><span class="line">    <span class="string">&#x27;y&#x27;</span>: y,</span><br><span class="line">    <span class="string">&#x27;num_nodes&#x27;</span>: num_nodes,</span><br><span class="line">    <span class="string">&#x27;other_attr&#x27;</span>: other_attr</span><br><span class="line">&#125;</span><br><span class="line">graph_data = Data.from_dict(graph_dict)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Data to dict</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Data_to_dict</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">return</span> &#123;key: item <span class="keyword">for</span> key, item <span class="keyword">in</span> self&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Data to namedtuple</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Data_to_namedtuple</span>(<span class="params">self</span>):</span></span><br><span class="line">    keys = self.keys</span><br><span class="line">    DataTuple = collections.namedtuple(<span class="string">&#x27;DataTuple&#x27;</span>, keys)</span><br><span class="line">    <span class="keyword">return</span> DataTuple(*[self[key] <span class="keyword">for</span> key <span class="keyword">in</span> keys])</span><br></pre></td></tr></table></figure>

<h5 id="4-4-Data对象属性"><a href="#4-4-Data对象属性" class="headerlink" title="4.4 Data对象属性"></a>4.4 Data对象属性</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#获取Data对象属性</span></span><br><span class="line">x = graph_data[<span class="string">&#x27;x&#x27;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#获取Data对象属性关键字</span></span><br><span class="line">graph_data.keys()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#对边排序并移除重复的边</span></span><br><span class="line">graph_data.coalesce()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> KarateClub</span><br><span class="line">dataset = KarateClub()</span><br><span class="line">data = dataset[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(data.num_nodes) <span class="comment">#节点数量</span></span><br><span class="line"><span class="built_in">print</span>(data.num_edges) <span class="comment">#边数量</span></span><br><span class="line"><span class="built_in">print</span>(data.num_node_features) <span class="comment">#节点属性维度</span></span><br><span class="line"><span class="built_in">print</span>(data.num_edge_features) <span class="comment">#边属性维度</span></span><br><span class="line"><span class="built_in">print</span>(data.num_edges/data.num_nodes) <span class="comment">#平均节点度</span></span><br><span class="line"><span class="built_in">print</span>(data.is_coalesced()) <span class="comment">#是否边是有序的同时不含有重复的边</span></span><br><span class="line"><span class="built_in">print</span>(data.train_mask.<span class="built_in">sum</span>()) <span class="comment">#用作训练集的节点</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">int</span>(data.train_mask.<span class="built_in">sum</span>())/data.num_nodes) <span class="comment">#用作训练集的节点数占比</span></span><br><span class="line"><span class="built_in">print</span>(data.contains_isolated_nodes()) <span class="comment">#此图是否包含孤立的节点</span></span><br><span class="line"><span class="built_in">print</span>(data.contains_self_loops()) <span class="comment">#此图是否包含自环的边</span></span><br><span class="line"><span class="built_in">print</span>(data.is_undirected()) <span class="comment">#此图是否是无向图</span></span><br></pre></td></tr></table></figure>

<h4 id="5-Dataset类——PyG中图数据集的表示"><a href="#5-Dataset类——PyG中图数据集的表示" class="headerlink" title="5.Dataset类——PyG中图数据集的表示"></a>5.Dataset类——PyG中图数据集的表示</h4><h5 id="5-1-数据集的下载"><a href="#5-1-数据集的下载" class="headerlink" title="5.1 数据集的下载"></a>5.1 数据集的下载</h5><p>首先下载PyG内置的Planetoid数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> Planetoid</span><br><span class="line">dataset = Planetoid(root=<span class="string">&#x27;/dataset/Cora&#x27;</span>, name=<span class="string">&#x27;Cora&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(dataset))    <span class="comment">#数据集中图的数量</span></span><br><span class="line"><span class="built_in">print</span>(dataset.num_classes)   <span class="comment">#分类任务的数量</span></span><br><span class="line"><span class="built_in">print</span>(dataset.num_node_features)   <span class="comment">##节点属性维度</span></span><br></pre></td></tr></table></figure>
<h5 id="5-1-数据集的使用"><a href="#5-1-数据集的使用" class="headerlink" title="5.1 数据集的使用"></a>5.1 数据集的使用</h5><p>定义一个名为<code>Net</code>的图神经网络模型，将节点分类图数据集加入训练。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Net().to(device)</span><br><span class="line">data = dataset[<span class="number">0</span>].to(device)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.01</span>, weight_decay=<span class="number">5e-4</span>)</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    out = model(data)</span><br><span class="line">    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>

<p>1.<a href="https://github.com/datawhalechina/team-learning-nlp/tree/master/GNN">datawhale-GNN开源学习资料</a></p>
]]></content>
      <categories>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>PyG</tag>
        <tag>Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>图神经网络——消息传递网络</title>
    <url>/2021/06/19/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E6%B6%88%E6%81%AF%E4%BC%A0%E9%80%92%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<p>消息传递(Message Passing) 指的是目标节点$S1$的邻居$\mathcal{N(S1)}$——B1、B2、B3，这些邻居节点根据一定的规则将信息(特征)，汇总到目标节点上。信息汇总中最简单的规则就是逐个元素相加。<br>在pytorch-geometric的官方文档中，消息传递图神经网络被描述为:<br>$$<br>\mathbf{x}_i^{(k)} = \gamma^{(k)} \left( \mathbf{x}<em>i^{(k-1)}, \square</em>{j \in \mathcal{N}(i)} , \phi^{(k)}\left(\mathbf{x}<em>i^{(k-1)}, \mathbf{x}<em>j^{(k-1)},\mathbf{e}</em>{j,i}\right) \right),<br>$$<br>其中，$\mathbf{e}</em>{j,i} \in \mathbb{R}^D$ 表示从节点$j$到节点$i$的边的属性，$\mathbf{x}^{(k-1)}_i\in\mathbb{R}^F$表示$(k-1)$层中节点$i$的节点表征，$\square$表示聚合策略，$\gamma$和$\phi$表示一些神经网络方法，比如MLPs多层感知器、LSTM等。<br>从公式中可以看出，目标节点$x_i$在k层的特征可以通过$x_i$在上一层(k-1层)的特征与其相邻节点$x_j$在上一层(k-1层)的特征以及相邻节点到目标节点的边的特征，这三个特征在k层通过$\square$的聚合策略(aggregate)，通过一个$\gamma$在k层的分析方法来导出目标节点$x_i$的特征。</p>
<span id="more"></span>

<h4 id="2-MessagePassing基类"><a href="#2-MessagePassing基类" class="headerlink" title="2. MessagePassing基类"></a>2. MessagePassing基类</h4><p>Pytorch Geometric(PyG)提供了<a href="https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.message_passing.MessagePassing">MessagePassing</a>基类，通过继承基类，并定义message()方法、update()方法、aggregate()方法，可以构造消息传递图神经网络。</p>
<h5 id="2-1-MessagePassing-init-aggr-”add”-flow-”source-to-target”-node-dim-2"><a href="#2-1-MessagePassing-init-aggr-”add”-flow-”source-to-target”-node-dim-2" class="headerlink" title="2.1 MessagePassing.__init__(aggr=”add”, flow=”source_to_target”, node_dim=-2)"></a>2.1 MessagePassing.__init__(aggr=”add”, flow=”source_to_target”, node_dim=-2)</h5><blockquote>
<p>aggr：定义聚合方案(“add”、”mean”或 “max”)，默认值”add”；<br>flow：定义消息传递方向(“source_to_target”或 “target_to_source”)，默认值”source_to_target”；<br>node_dim：定义沿哪个维度传播，指的是节点表征张量(Tensor)的哪一个维度是节点维度，默认值-2(第0维)。</p>
</blockquote>
<h5 id="2-2-MessagePassing-propagate-edge-index-size-None-kwargs"><a href="#2-2-MessagePassing-propagate-edge-index-size-None-kwargs" class="headerlink" title="2.2 MessagePassing.propagate(edge_index, size=None, **kwargs)"></a>2.2 MessagePassing.propagate(edge_index, size=None, **kwargs)</h5><blockquote>
<p>这是一个集成方法，调用其会依次调用message、aggregate、update方法。<br>edge_index：边的端点的索引，当flow=”source_to_target”时，节点edge_index[0]的信息将被传递到节点edge_index[1]；当flow=”target_to_source”时，节点edge_index[1]的信息将被传递到节点edge_index[0]。<br>size：邻接节点的数量与中心节点的数量，默认值None(对称矩阵)；<br>**kwaegs：图的其他属性或额外的数据。</p>
</blockquote>
<h5 id="2-3-MessagePassing-message-…"><a href="#2-3-MessagePassing-message-…" class="headerlink" title="2.3 MessagePassing.message(…)"></a>2.3 MessagePassing.message(…)</h5><blockquote>
<p>以函数的方式构造消息；<br>flow=”source_to_target”，此方式下，message方法负责产生source node需要传出的信息。</p>
</blockquote>
<h5 id="2-4-MessagePassing-update-aggr-out-…"><a href="#2-4-MessagePassing-update-aggr-out-…" class="headerlink" title="2.4 MessagePassing.update(aggr_out, …)"></a>2.4 MessagePassing.update(aggr_out, …)</h5><blockquote>
<p>为每个节点$i \in \mathcal{V}$更新节点表征。</p>
</blockquote>
<h5 id="2-5-MessagePassing-aggregate-…"><a href="#2-5-MessagePassing-aggregate-…" class="headerlink" title="2.5 MessagePassing.aggregate(…)"></a>2.5 MessagePassing.aggregate(…)</h5><p>将从源节点传递过来的消息聚合在目标节点上，一般可选的聚合方式有sum, mean和max。</p>
<h4 id="3-MessagePassing子类"><a href="#3-MessagePassing子类" class="headerlink" title="3. MessagePassing子类"></a>3. MessagePassing子类</h4><h5 id="3-1-GCNConv类"><a href="#3-1-GCNConv类" class="headerlink" title="3.1 GCNConv类"></a>3.1 GCNConv类</h5><p>以继承MessagePassing基类的GCNConv类为例，可以实现一个简单的GNN。<br>GCNConv的公式如下:<br>$$<br>\mathbf{x}<em>i^{(k)} = \sum</em>{j \in \mathcal{N}(i) \cup { i }} \frac{1}{\sqrt{\deg(i)} \cdot \sqrt{\deg(j)}} \cdot \left( \mathbf{\Theta} \cdot \mathbf{x}_j^{(k-1)} \right)<br>$$<br>其中，邻接节点的表征$\mathbf{x}_j^{(k-1)}$首先通过与权重矩阵$\mathbf{\Theta}$相乘进行变换，然后按端点的度(degree)$\deg(i), \deg(j)$进行归一化处理，最后进行求和。这个公式可以分为以下几个步骤：</p>
<blockquote>
<p>向邻接矩阵添加自环边。<br>对节点表征做线性转换。<br>计算归一化系数。<br>归一化邻接节点的节点表征。<br>将相邻节点表征相加（”求和 “聚合）。</p>
</blockquote>
<p>GCNConv继承了MessagePassing，并以”求和”作为领域节点信息聚合方式。该层的所有逻辑都在forward()方法中：<br>1.通过torch_geometric.utils.add_self_loops()函数向边索引添加自循环边，目的是改进原始不考虑中心节点自身的信息量的问题；<br>2.通过torch.nn.Linear实例对节点表征进行线性变换；<br>3.归一化系数是由每个节点的节点度得出的，它被转换为每条边的节点度。结果被保存在形状为[num_edges,]的变量norm中。</p>
<h5 id="3-2-实现"><a href="#3-2-实现" class="headerlink" title="3.2 实现"></a>3.2 实现</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> Planetoid</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> MessagePassing</span><br><span class="line"><span class="keyword">from</span> torch_geometric.utils <span class="keyword">import</span> add_self_loops, degree</span><br><span class="line"><span class="keyword">from</span> torch_sparse <span class="keyword">import</span> SparseTensor</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GCNConv</span>(<span class="params">MessagePassing</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span></span><br><span class="line">        <span class="comment">#in_channels可理解为输入通道数，out_channels可理解为卷积核数量</span></span><br><span class="line">        <span class="built_in">super</span>(GCNConv, self).__init__(aggr=<span class="string">&#x27;add&#x27;</span>, flow=<span class="string">&#x27;source_to_target&#x27;</span>) <span class="comment">#继承，策略为合并</span></span><br><span class="line">        <span class="comment"># &quot;Add&quot; aggregation (Step 5).</span></span><br><span class="line">        <span class="comment"># flow=&#x27;source_to_target&#x27; 表示消息从源节点传播到目标节点</span></span><br><span class="line">        self.lin = torch.nn.Linear(in_channels, out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, edge_index</span>):</span></span><br><span class="line">        <span class="comment"># x has shape [N, in_channels]</span></span><br><span class="line">        <span class="comment"># edge_index has shape [2, E]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 1: 添加自循环到节点特征矩阵(Add self-loops to the adjacency matrix.)</span></span><br><span class="line">        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2: 节点特征矩阵的线性变换(Linearly transform node feature matrix.)</span></span><br><span class="line">        x = self.lin(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 3: Compute normalization.</span></span><br><span class="line">        row, col = edge_index <span class="comment">#用行、列描述特征矩阵</span></span><br><span class="line">        deg = degree(col, x.size(<span class="number">0</span>), dtype=x.dtype)</span><br><span class="line">        deg_inv_sqrt = deg.<span class="built_in">pow</span>(-<span class="number">0.5</span>)</span><br><span class="line">        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 4-5: Start propagating messages.</span></span><br><span class="line">        adjmat = SparseTensor(row=edge_index[<span class="number">0</span>], col=edge_index[<span class="number">1</span>], value=torch.ones(edge_index.shape[<span class="number">1</span>]))</span><br><span class="line">        <span class="comment"># 此处传的不再是edge_index，而是SparseTensor类型的Adjancency Matrix</span></span><br><span class="line">        <span class="keyword">return</span> self.propagate的(adjmat, x=x, norm=norm, deg=deg.view((-<span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line">		<span class="comment"># 此处省略MessagePassing.propagate的代码.</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">message</span>(<span class="params">self, x_j, norm</span>):</span></span><br><span class="line">        <span class="comment"># x_j has shape [E, out_channels]</span></span><br><span class="line">        <span class="comment"># deg_i has shape [E, 1]</span></span><br><span class="line">        <span class="comment"># Step 4: Normalize node features.</span></span><br><span class="line">        <span class="keyword">return</span> norm.view(-<span class="number">1</span>, <span class="number">1</span>) * x_j</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">aggregate</span>(<span class="params">self, inputs, index, ptr, dim_size</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;self.aggr:&#x27;</span>, self.aggr)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;`aggregate` is called&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>().aggregate(inputs, index, ptr=ptr, dim_size=dim_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">message_and_aggregate</span>(<span class="params">self, adj_t, x, norm</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;`message_and_aggregate` is called&#x27;</span>)</span><br><span class="line">        <span class="comment"># 没有实现真实的消息传递与消息聚合的操作</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">self, inputs, deg</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(deg)</span><br><span class="line">        <span class="keyword">return</span> inputs</span><br><span class="line"></span><br><span class="line">dataset = Planetoid(root=<span class="string">&#x27;dataset&#x27;</span>, name=<span class="string">&#x27;Cora&#x27;</span>)</span><br><span class="line">data = dataset[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">net = GCNConv(data.num_features, <span class="number">64</span>)</span><br><span class="line">h_nodes = net(data.x, data.edge_index)</span><br><span class="line"><span class="comment"># print(h_nodes.shape)</span></span><br></pre></td></tr></table></figure>

<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://github.com/datawhalechina/team-learning-nlp/tree/master/GNN">datawhale-GNN开源学习资料</a></p>
]]></content>
      <categories>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>PyG</tag>
        <tag>GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>图神经网络——节点分类与边预测</title>
    <url>/2021/06/27/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E8%8A%82%E7%82%B9%E5%88%86%E7%B1%BB%E4%B8%8E%E8%BE%B9%E9%A2%84%E6%B5%8B/</url>
    <content><![CDATA[<h4 id="1-InMemoryDataset基类"><a href="#1-InMemoryDataset基类" class="headerlink" title="1.InMemoryDataset基类"></a>1.InMemoryDataset基类</h4><p>在PyG中，可以通过继承InMemoryDataset类来自定义一个数据可全部存储到内存的数据集类。(继承Dataset是分次加载到内存，继承InMemoryDataset是一次性加载所有数据到内存)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InMemoryDataset</span>(<span class="params">root: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span>, transform: <span class="type">Optional</span>[<span class="type">Callable</span>] = <span class="literal">None</span>, pre_transform: <span class="type">Optional</span>[<span class="type">Callable</span>] = <span class="literal">None</span>, pre_filter: <span class="type">Optional</span>[<span class="type">Callable</span>] = <span class="literal">None</span></span>)</span></span><br></pre></td></tr></table></figure>
<p>参数说明：</p>
<blockquote>
<p>transform：数据转换函数，用于转换Data对象，每一次数据获取过程中都会被执行。<br>pre_transform：数据转换函数，用于转换Data对象，在Data对象被保存到文件前调用。<br>pre_filter：检查数据是否要保留的函数，接收一个Data对象，返回此Data对象是否应该被包含在最终的数据集中，在Data对象被保存到文件前调用。</p>
</blockquote>
<span id="more"></span>
<h4 id="2-Sequential容器"><a href="#2-Sequential容器" class="headerlink" title="2.Sequential容器"></a>2.Sequential容器</h4><p>nn.Sequential是nn.module的容器，用于按顺序包装一组网络层。参数说明：</p>
<blockquote>
<p>args(str)：模型的全局输入参数；<br>modules ([(str, Callable) or Callable]) ：模块列表。</p>
</blockquote>
<h4 id="3-节点分类"><a href="#3-节点分类" class="headerlink" title="3. 节点分类"></a>3. 节点分类</h4><p>定义一个GAT图神经网络，通过hidden_channels_list参数来设置每一层GATConv的outchannel，所以hidden_channels_list长度即为GATConv的层数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#载入数据集</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> Planetoid</span><br><span class="line"><span class="keyword">from</span> torch_geometric.transforms <span class="keyword">import</span> NormalizeFeatures</span><br><span class="line">dataset = Planetoid(root=<span class="string">&#x27;dataset&#x27;</span>, ame=<span class="string">&#x27;Cora&#x27;</span>,transform=NormalizeFeatures())</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Linear,ReLU</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> GATConv, Sequential</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用Sequential容器定义一个GAT网络</span></span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> GATConv</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GAT</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_features, hidden_channels_list, num_classes</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(GAT, self).__init__()</span><br><span class="line">        torch.manual_seed(<span class="number">2021</span>)</span><br><span class="line">        hns = [num_features] + hidden_channels_list</span><br><span class="line">        conv_list = []</span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(hidden_channels_list)):</span><br><span class="line">            conv_list.append((GATConv(hns[idx], hns[idx+<span class="number">1</span>]), <span class="string">&#x27;x, edge_index -&gt; x&#x27;</span>))</span><br><span class="line">            conv_list.append(ReLU(inplace=<span class="literal">True</span>),)</span><br><span class="line"></span><br><span class="line">        self.convseq = Sequential(<span class="string">&#x27;x, edge_index&#x27;</span>, conv_list)</span><br><span class="line">        self.linear = Linear(hidden_channels_list[-<span class="number">1</span>], num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, edge_index</span>):</span></span><br><span class="line">        x = self.convseq(x, edge_index)</span><br><span class="line">        x = F.dropout(x, p=<span class="number">0.5</span>, training=self.training)</span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#训练和测试</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model = GAT(num_features=dataset.num_features, hidden_channels_list=[<span class="number">200</span>, <span class="number">100</span>], num_classes=dataset.num_classes).to(device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.01</span>, weight_decay=<span class="number">5e-4</span>)</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment">#train()、test()省略，与上章一致</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">201</span>):</span><br><span class="line">    loss = train()</span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch: <span class="subst">&#123;epoch:03d&#125;</span>, Loss: <span class="subst">&#123;loss:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">test_acc = test()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Test Accuracy: <span class="subst">&#123;test_acc:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>构建2层GAT，Accuracy为0.7640；构建2层GCN，Accuracy为0.6490。<br>构造3层GAT(将hidden_channels_list的值改为[200,100,50])，Accuracy为0.7680；构建3层GCN，Accuracy为0.5190。</p>
</blockquote>
<h4 id="4-边预测"><a href="#4-边预测" class="headerlink" title="4 边预测"></a>4 边预测</h4><p>边预测任务的目标是预测两个节点间是否有边。做边预测任务首先需要获取正负样本数量平衡的数据集(edge_index存储的是正样本，需要采样一些不存在边的节点对作为负样本边)，PyG中可以通过train_test_split_edges(data, val_ratio=0.05, test_ratio=0.1)采样负样本边。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#构造边预测神经网络</span></span><br><span class="line"><span class="keyword">import</span> os.path <span class="keyword">as</span> osp</span><br><span class="line"><span class="keyword">from</span> torch_geometric.utils <span class="keyword">import</span> negative_sampling</span><br><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> Planetoid</span><br><span class="line"><span class="keyword">import</span> torch_geometric.transforms <span class="keyword">as</span> T</span><br><span class="line"><span class="keyword">from</span> torch_geometric.utils <span class="keyword">import</span> train_test_split_edges</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> GCNConv</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = GCNConv(in_channels, <span class="number">128</span>)</span><br><span class="line">        self.conv2 = GCNConv(<span class="number">128</span>, out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, x, edge_index</span>):</span></span><br><span class="line">        x = self.conv1(x, edge_index)</span><br><span class="line">        x = x.relu()</span><br><span class="line">        <span class="keyword">return</span> self.conv2(x, edge_index)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, z, pos_edge_index, neg_edge_index</span>):</span></span><br><span class="line">        edge_index =  torch.cat([pos_edge_index, neg_edge_index], dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> (z[edge_index[<span class="number">0</span>]] * z[edge_index[<span class="number">1</span>]]).<span class="built_in">sum</span>(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode_all</span>(<span class="params">self, z</span>):</span></span><br><span class="line">        <span class="comment">#对所有的节点对预测存在边的几率</span></span><br><span class="line">        prob_adj = z @ z.t()    <span class="comment"># @ 表示矩阵乘法</span></span><br><span class="line">        <span class="keyword">return</span> (prob_adj &gt; <span class="number">0</span>).nonzero(as_tuple=<span class="literal">False</span>).t()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义单个epoch的训练过程</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_link_labels</span>(<span class="params">pos_edge_index, neg_edge_index</span>):</span></span><br><span class="line">    num_links = pos_edge_index.size(<span class="number">1</span>) + neg_edge_index.size(<span class="number">1</span>)</span><br><span class="line">    link_labels = torch.zeros(num_links, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">    link_labels[:pos_edge_index.size(<span class="number">1</span>)] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> link_labels</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">data, model, optimizer</span>):</span></span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    neg_edge_index = negative_sampling(</span><br><span class="line">        edge_index = data.train_pos_edge_index,</span><br><span class="line">        num_nodes = data.num_nodes,</span><br><span class="line">        num_neg_samples = data.train_pos_edge_index.size(<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    train_neg_edge_set = <span class="built_in">set</span>(<span class="built_in">map</span>(<span class="built_in">tuple</span>, neg_edge_index.T.tolist()))</span><br><span class="line">    val_pos_edge_set = <span class="built_in">set</span>(<span class="built_in">map</span>(<span class="built_in">tuple</span>, data.val_pos_edge_index.T.tolist()))</span><br><span class="line">    test_pos_edge_set = <span class="built_in">set</span>(<span class="built_in">map</span>(<span class="built_in">tuple</span>, data.test_pos_edge_index.T.tolist()))</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">len</span>(train_neg_edge_set &amp; val_pos_edge_set) &gt; <span class="number">0</span>) <span class="keyword">or</span> (<span class="built_in">len</span>(train_neg_edge_set &amp; test_pos_edge_set) &gt; <span class="number">0</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;wrong!&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    z = model.encode(data.x, data.train_pos_edge_index)</span><br><span class="line">    link_logits = model.decode(z, data.train_pos_edge_index, neg_edge_index)</span><br><span class="line">    link_labels = get_link_labels(data.train_pos_edge_index, neg_edge_index).to(data.x.device)</span><br><span class="line">    loss = F.binary_cross_entropy_with_logits(link_logits, link_labels)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#定义单个epoch验证与测试过程</span></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">data, model</span>):</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    z = model.encode(data.x, data.train_pos_edge_index)</span><br><span class="line"></span><br><span class="line">    results = []</span><br><span class="line">    <span class="keyword">for</span> prefix <span class="keyword">in</span> [<span class="string">&#x27;val&#x27;</span>, <span class="string">&#x27;test&#x27;</span>]:</span><br><span class="line">        pos_edge_index = data[<span class="string">f&#x27;<span class="subst">&#123;prefix&#125;</span>_pos_edge_index&#x27;</span>]</span><br><span class="line">        neg_edge_index = data[<span class="string">f&#x27;<span class="subst">&#123;prefix&#125;</span>_neg_edge_index&#x27;</span>]</span><br><span class="line">        link_logits = model.decode(z, pos_edge_index, neg_edge_index)</span><br><span class="line">        link_probs = link_logits.sigmoid()</span><br><span class="line">        link_labels = get_link_labels(pos_edge_index, neg_edge_index)</span><br><span class="line">        results.append(roc_auc_score(link_labels.cpu(), link_probs.cpu()))</span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#完整的训练、验证与测试</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">	dataset = Planetoid(<span class="string">&#x27;/Dataset/Planetoid/Cora&#x27;</span>, <span class="string">&#x27;Cora&#x27;</span>, transform=T.NormalizeFeatures())</span><br><span class="line">    data = dataset[<span class="number">0</span>]</span><br><span class="line">    ground_truth_edge_index = data.edge_index.to(device)</span><br><span class="line">    data.train_mask = data.val_mask = data.test_mask = data.y = <span class="literal">None</span></span><br><span class="line">    data = train_test_split_edges(data)</span><br><span class="line">    data = data.to(device)</span><br><span class="line"></span><br><span class="line">    model = Net(dataset.num_features, <span class="number">64</span>).to(device)</span><br><span class="line">    optimizer = torch.optim.Adam(params=model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">    best_val_auc = test_auc = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">101</span>):</span><br><span class="line">        loss = train(data, model, optimizer)</span><br><span class="line">        val_auc, tmp_test_auc = test(data, model)</span><br><span class="line">        <span class="keyword">if</span> val_auc &gt; best_val_auc:</span><br><span class="line">            best_val_auc = val_auc</span><br><span class="line">            test_auc = tmp_test_auc</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch: <span class="subst">&#123;epoch:03d&#125;</span>, Loss: <span class="subst">&#123;loss:<span class="number">.4</span>f&#125;</span>, Val: <span class="subst">&#123;val_auc:<span class="number">.4</span>f&#125;</span>, &#x27;</span></span><br><span class="line">              <span class="string">f&#x27;Test: <span class="subst">&#123;test_auc:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    z = model.encode(data.x, data.train_pos_edge_index)</span><br><span class="line">    final_edge_index = model.decode_all(z)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果：Epoch: 100, Loss: 0.4414, Val: 0.9330, Test: 0.8943</p>
</blockquote>
<p>将Sequential容器用于边预测，需要在Net类定义中将__init_函数和main()做部分修改：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#替换上面的对应代码</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, hidden_channels_list, out_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        torch.manual_seed(<span class="number">2021</span>)</span><br><span class="line">        hns = [in_channels] + hidden_channels_list</span><br><span class="line">        conv_list = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(hidden_channels_list)-<span class="number">1</span>):</span><br><span class="line">            conv_list.append((GCNConv(hns[idx], hns[idx+<span class="number">1</span>]), <span class="string">&#x27;x, edge_index -&gt; x&#x27;</span>))</span><br><span class="line">            conv_list.append(ReLU(inplace=<span class="literal">True</span>), )</span><br><span class="line">        conv_list.append((GCNConv(hns[-<span class="number">2</span>], hns[-<span class="number">1</span>]), <span class="string">&#x27;x, edge_index -&gt; x&#x27;</span>))</span><br><span class="line"></span><br><span class="line">        self.convseq = Sequential(<span class="string">&#x27;x, edge_index&#x27;</span>, conv_list)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">        model = Net(dataset.num_features,[<span class="number">200</span>,<span class="number">100</span>],dataset.num_classes).to(device)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>在边预测任务用Sequential容器的结果：<br>Epoch: 100, Loss: 0.4226, Val: 0.9123, Test: 0.8958</p>
</blockquote>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://github.com/datawhalechina/team-learning-nlp/tree/master/GNN">datawhale-GNN开源学习资料</a><br>2.<a href="https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.sequential.Sequential">GNN官方文档</a><br>3.<a href="https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.sequential.Sequential">Sequential官网文档</a></p>
]]></content>
      <categories>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>PyG</tag>
        <tag>GCN</tag>
        <tag>GAT</tag>
        <tag>Sequential</tag>
      </tags>
  </entry>
  <entry>
    <title>图神经网络——超大图上的节点表征学习</title>
    <url>/2021/07/01/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E8%B6%85%E5%A4%A7%E5%9B%BE%E4%B8%8A%E7%9A%84%E8%8A%82%E7%82%B9%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>图卷积网络(GCN)已经成功地应用于许多基于图形的应用，然而大规模的GCN的训练仍然具有挑战性。目前基于SGD的算法要么面临着随GCN层数呈指数增长的高计算成本，要么面临着保存整个图形和每个节点的embedding到内存的巨大空间(显存)需求。于是论文<a href="https://arxiv.org/abs/1905.07953">Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Network</a>提出了Cluster-GCN方法来解决超大图的训练问题。</p>
<h4 id="1-Cluster-GCN方法简单概括"><a href="#1-Cluster-GCN方法简单概括" class="headerlink" title="1.Cluster-GCN方法简单概括"></a>1.Cluster-GCN方法简单概括</h4><ul>
<li>利用图节点聚类算法将一个图的节点划分为$c$个簇，每一次选择几个簇的节点和这些节点对应的边构成一个子图，然后对子图做训练。</li>
<li>由于是利用图节点聚类算法将节点划分为多个簇，所以簇内边的数量要比簇间边的数量多得多，所以可以提高表征利用率，并提高图神经网络的训练效率。</li>
<li>每一次随机选择多个簇来组成一个batch，这样不会丢失簇间的边，同时也不会有batch内类别分布偏差过大的问题。</li>
<li>基于小图进行训练，不会消耗很多内存空间，于是我们可以训练更深的神经网络，进而可以达到更高的精度。</li>
</ul>
<span id="more"></span>
<h4 id="2-GCN回顾"><a href="#2-GCN回顾" class="headerlink" title="2.GCN回顾"></a>2.GCN回顾</h4><p>论文<a href="https://arxiv.org/abs/1609.02907">2017. Semi-supervised classification with graph convolutional networks</a>中提出了GCN。作者定义了一种类似卷积的运算，称为谱图卷积。这使得CNN可以直接在图形上操作。GCN中的每一层通过考虑相邻节点的embedding，来更新Graph中的每个节点的特征向量表示。GCN的逐层正向传播可以总结为：<br>$$<br>X^{\left( l+1 \right)}=f\left( X^l,A \right) =\sigma \left( \tilde{D}^{-\frac{1}{2}}\tilde{A}D^{-\frac{1}{2}}X^{\left( l \right)}W^{\left( l \right)} \right)<br>$$</p>
<ul>
<li>$X$是所有节点的特征向量构成的特征矩阵。</li>
<li>$X^l$和$X^{l+1}$分别是$l$层的输入和输出矩阵，$X^l$也是第$l$层对于所有节点的embedding。</li>
<li>A是图的邻接矩阵。</li>
<li>${A}=A+I_N$是带有自环的无向图的邻接矩阵。</li>
<li>$I_N$是单位矩阵。</li>
<li>$\tilde{D}<em>{ii}=\sum{\tilde{A}</em>{ij}}$是带有自环的无向图的度矩阵，是一个对角矩阵。</li>
<li>$W^{(l)}$是一个可训练权重矩阵或参数矩阵。</li>
<li>$\sigma(\cdot)$是激活函数，通常被设为<code>ReLU</code>。</li>
</ul>
<p>为了简化计算，假设所有层的表征维度都是F，上式写为：<br>$$<br>Z^{(l+1)}=A^{\prime} X^{(l)} W^{(l)}, X^{(l+1)}=\sigma\left(Z^{(l+1)}\right)<br>$$</p>
<ul>
<li>$A^{\prime}$是归一化和规范化后的邻接矩阵</li>
</ul>
<p>将GCN用于半监督节点分类时，目标是通过最小化损失函数来学习上式中的权重矩阵：<br>$$<br>\mathcal{L}=\frac{1}{\left|\mathcal{Y}<em>{L}\right|} \sum</em>{i \in \mathcal{Y}<em>{L}} \operatorname{loss}\left(y</em>{i}, z_{i}^{L}\right)<br>$$</p>
<ul>
<li>$\mathcal{Y}_{\mathcal{L}}$是节点类别。</li>
<li>$z_{i}^{(L)}$是$Z^{(L)}$的第$i$行，表示节点$i$的最终层预测。</li>
<li>$y_{i}$为节点$i$的真实类别。<h4 id="3-Cluster-GCN的速度"><a href="#3-Cluster-GCN的速度" class="headerlink" title="3.Cluster-GCN的速度"></a>3.Cluster-GCN的速度</h4>原始的GCN的训练使用的是全批量梯度下降(Full Gradient Descent)，它的计算和内存成本很高:</li>
<li>在内存方面，通过反向传播来计算full gradient需要存储所有的embedding矩阵，这需要O(NFL)的内存空间（F是特征数量，N是结点数量，L是网络层数）。</li>
<li>在收敛速度方面，由于在每个epoch模型才更新一次，所以需要很多个epoch才会使模型达到收敛。</li>
</ul>
<p>mini-batch SGD可以提高GCN的训练速度，其不需要计算整个梯度，只需要计算每次更新的mini-batch的梯度。使用大小为$b=|\mathcal{B}|$的$\mathcal{B} \subseteq[N]$来表示一个batch的节点索引。SGD的每一步都将计算梯度估计值$\frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \nabla \operatorname{loss}\left(y_{i}, z_{i}^{(L)}\right)$来进行参数更新。</p>
<p>尽管在每个epoch收敛得更快，但SGD在GCN训练上引入了另一个计算开销（邻域扩展），这使得SGD的每个epoch时间内比全梯度下降（full gradient descent）要慢得多——计算一个节点$i$的梯度，需要节点$i$的embedding，而节点$i$的embedding的依赖于前面的层里的它的邻居的embeddings，为了提取前面的层中它的邻居节点embeddings，还需要进一步聚合每个邻居节点的邻居节点的embeddings。假设一个图神经网络有$L+1$层，节点的平均的度为$d$。为了得到节点$i$的梯度，需要聚合图上$O\left(d^{L}\right)$的节点的表征，与权重矩阵$W^{(l)}$相乘，所以计算任意节点表征的时间开销是$O\left(F^{2}\right)$。所以SGD一个节点的梯度的计算需要$O\left(d^{L} F^{2}\right)$的时间。</p>
<h4 id="4-Cluster-GCN详解"><a href="#4-Cluster-GCN详解" class="headerlink" title="4.Cluster-GCN详解"></a>4.Cluster-GCN详解</h4><p>在mini-batch SGD更新中，设计一个batch和相应的计算sub graph来最大限度地提高embedding utilization。Cluster-GCN通过将embedding utilization连接到一个聚类目标上来实现。</p>
<p>对于一个Graph$G$，我们将其节点划分为$c$个簇：$\mathcal{V}=\left[\mathcal{V}<em>{1}, \cdots \mathcal{V}</em>{c}\right]$，其中$\mathcal{V}<em>{t}$由第$t$个簇中的节点组成，得到$c$个子图:<br>$$<br>\bar{G}=\left[G</em>{1}, \cdots, G_{c}\right]=\left[\left{\mathcal{V}<em>{1}, \mathcal{E}</em>{1}\right}, \cdots,\left{\mathcal{V}<em>{c}, \mathcal{E}</em>{c}\right}\right]<br>\notag<br>$$<br>其中$\mathcal{E}<em>{t}$只由$\mathcal{V}</em>{t}$中的节点之间的边组成。经过节点重组，邻接矩阵被划分为大小为$c^{2}$的块矩阵，如下所示<br>$$<br>A=\bar{A}+\Delta=\left[\begin{array}{ccc}<br>A_{11} &amp; \cdots &amp; A_{1 c} \<br>\vdots &amp; \ddots &amp; \vdots \<br>A_{c 1} &amp; \cdots &amp; A_{c c}<br>\end{array}\right]<br>$$<br>其中<br>$$<br>\bar{A}=\left[\begin{array}{ccc}<br>A_{11} &amp; \cdots &amp; 0 \<br>\vdots &amp; \ddots &amp; \vdots \<br>0 &amp; \cdots &amp; A_{c c}<br>\end{array}\right], \Delta=\left[\begin{array}{ccc}<br>0 &amp; \cdots &amp; A_{1 c} \<br>\vdots &amp; \ddots &amp; \vdots \<br>A_{c 1} &amp; \cdots &amp; 0<br>\end{array}\right]<br>$$</p>
<ul>
<li>对角线上的块$A_{t t}$是大小为$\left|\mathcal{V}<em>{t}\right| \times\left|\mathcal{V}</em>{t}\right|$的邻接矩阵，它由$G_{t}$内部的边构成。</li>
<li>$\bar{A}$是图$\bar{G}$的邻接矩阵。</li>
<li>$A_{s t}$由两个簇$\mathcal{V}<em>{s}$和$\mathcal{V}</em>{t}$之间的边构成。</li>
<li>$\Delta$是由$A$的所有非对角线块组成的矩阵。</li>
</ul>
<p>用块对角线邻接矩阵$\bar{A}$去近似邻接矩阵$A$之后，完整的损失函数可以根据batch分解成多个部分之和：</p>
<p><a href="https://imgtu.com/i/W2fREV"><img src="https://z3.ax1x.com/2021/07/25/W2fREV.png" alt="W2fREV.png"></a></p>
<p><a href="https://imgtu.com/i/W2fX4O"><img src="https://z3.ax1x.com/2021/07/25/W2fX4O.png" alt="W2fX4O.png"></a></p>
<p>Cluster-GCN使用了Graph聚类算法来划分Graph。Graph聚类的方法，如metis和graclus等，旨在在Graph中的顶点上构建分区，使得簇内连接远大于簇间连接，从而更好的捕捉聚类和区分结构。</p>
<p><img src="https://i.loli.net/2021/07/12/h2CLbu9VMEzyNTa.jpg" alt="LAYER.jpg"></p>
<h4 id="5-Cluster-GCN实践"><a href="#5-Cluster-GCN实践" class="headerlink" title="5.Cluster-GCN实践"></a>5.Cluster-GCN实践</h4><p>PyG库中提供了Cluster-GCN的接口，可以像训练普通神经网络一样在超大图上训练图神经网络。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#导入库和数据集</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> ModuleList</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> SAGEConv</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> Reddit, Reddit2</span><br><span class="line"><span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> ClusterData, ClusterLoader, NeighborSampler</span><br><span class="line"></span><br><span class="line">dataset = Reddit(<span class="string">&#x27;../dataset/Reddit&#x27;</span>)</span><br><span class="line">data = dataset[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(dataset.num_classes)</span><br><span class="line"><span class="built_in">print</span>(data.num_nodes)</span><br><span class="line"><span class="built_in">print</span>(data.num_edges)</span><br><span class="line"><span class="built_in">print</span>(data.num_features)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果：<br>41, 232965, 114615892, 602</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#图节点聚类并生成数据加载器</span></span><br><span class="line">cluster_data = ClusterData(data, num_parts=<span class="number">1500</span>, recursive=<span class="literal">False</span>, save_dir=dataset.processed_dir)</span><br><span class="line"></span><br><span class="line">train_loader = ClusterLoader(cluster_data, batch_size=<span class="number">20</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">subgraph_loader = NeighborSampler(data.edge_index, sizes=[-<span class="number">1</span>], batch_size=<span class="number">1024</span>, shuffle=<span class="literal">False</span>,num_workers=<span class="number">12</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#构建网络</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.convs = ModuleList(</span><br><span class="line">            [SAGEConv(in_channels, <span class="number">128</span>),</span><br><span class="line">             SAGEConv(<span class="number">128</span>, out_channels)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, edge_index</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i, conv <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.convs):</span><br><span class="line">            x = conv(x, edge_index)</span><br><span class="line">            <span class="keyword">if</span> i != <span class="built_in">len</span>(self.convs) - <span class="number">1</span>:</span><br><span class="line">                x = F.relu(x)</span><br><span class="line">                x = F.dropout(x, p=<span class="number">0.5</span>, training=self.training)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inference</span>(<span class="params">self, x_all</span>):</span></span><br><span class="line">        pbar = tqdm(total=x_all.size(<span class="number">0</span>) * <span class="built_in">len</span>(self.convs))</span><br><span class="line">        pbar.set_description(<span class="string">&#x27;Evaluating&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i, conv <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.convs):</span><br><span class="line">            xs = []</span><br><span class="line">            <span class="keyword">for</span> batch_size, n_id, adj <span class="keyword">in</span> subgraph_loader:</span><br><span class="line">                edge_index, _, size = adj.to(device)</span><br><span class="line">                x = x_all[n_id].to(device)</span><br><span class="line">                x_target = x[:size[<span class="number">1</span>]]</span><br><span class="line">                x = conv((x, x_target), edge_index)</span><br><span class="line">                <span class="keyword">if</span> i != <span class="built_in">len</span>(self.convs) - <span class="number">1</span>:</span><br><span class="line">                    x = F.relu(x)</span><br><span class="line">                xs.append(x.cpu())</span><br><span class="line"></span><br><span class="line">                pbar.update(batch_size)</span><br><span class="line"></span><br><span class="line">            x_all = torch.cat(xs, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        pbar.close()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x_all</span><br></pre></td></tr></table></figure>
<p>此神经网络的forward函数的定义与普通的图神经网络并无区别，而inference方法应用于推理阶段，为了获取更高的预测精度，所以使用subgraph_loader。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#训练、验证、测试</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">model = Net(dataset.num_features, dataset.num_classes).to(device)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.005</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>():</span></span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    total_loss = total_nodes = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">        batch = batch.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        out = model(batch.x, batch.edge_index)</span><br><span class="line">        loss = F.nll_loss(out[batch.train_mask], batch.y[batch.train_mask])</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        nodes = batch.train_mask.<span class="built_in">sum</span>().item()</span><br><span class="line">        total_loss += loss.item() * nodes</span><br><span class="line">        total_nodes += nodes</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_loss / total_nodes</span><br><span class="line"></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span>  <span class="comment"># Inference should be performed on the full graph.</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    out = model.inference(data.x)</span><br><span class="line">    y_pred = out.argmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    accs = []</span><br><span class="line">    <span class="keyword">for</span> mask <span class="keyword">in</span> [data.train_mask, data.val_mask, data.test_mask]:</span><br><span class="line">        correct = y_pred[mask].eq(data.y[mask]).<span class="built_in">sum</span>().item()</span><br><span class="line">        accs.append(correct / mask.<span class="built_in">sum</span>().item())</span><br><span class="line">    <span class="keyword">return</span> accs</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">31</span>):</span><br><span class="line">    loss = train()</span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        train_acc, val_acc, test_acc = test()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch: <span class="subst">&#123;epoch:02d&#125;</span>, Loss: <span class="subst">&#123;loss:<span class="number">.4</span>f&#125;</span>, Train: <span class="subst">&#123;train_acc:<span class="number">.4</span>f&#125;</span>, &#x27;</span></span><br><span class="line">              <span class="string">f&#x27;Val: <span class="subst">&#123;val_acc:<span class="number">.4</span>f&#125;</span>, test: <span class="subst">&#123;test_acc:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch: <span class="subst">&#123;epoch:02d&#125;</span>, Loss: <span class="subst">&#123;loss:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果：<br>Epoch: 10, Loss: 0.2893, Train: 0.9620, Val: 0.9516, test: 0.9496<br>Epoch: 20, Loss: 0.2478, Train: 0.9664, Val: 0.9546, test: 0.9560<br>Epoch: 30, Loss: 0.2397, Train: 0.9689, Val: 0.9515, test: 0.9502</p>
</blockquote>
<p>在训练过程中，使用train_loader获取batch，每次根据多个簇组成的batch进行训练；在验证阶段，使用subgraph_loader，在计算一个节点的表征时会计算该节点的距离从$0$到$L$的邻接节点，可以更好地测试神经网络的性能。</p>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://github.com/datawhalechina/team-learning-nlp/tree/master/GNN">datawhale-GNN开源学习资料</a><br>2.<a href="https://arxiv.org/abs/1905.07953">Cluster-GCN论文</a> </p>
]]></content>
      <categories>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>图神经网络——基本图论与PyG库</title>
    <url>/2021/07/09/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E8%B6%85%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E9%9B%86%E7%B1%BB%E7%9A%84%E5%88%9B%E5%BB%BA%E5%92%8C%E5%9B%BE%E9%A2%84%E6%B5%8B%E4%BB%BB%E5%8A%A1%E5%AE%9E%E8%B7%B5/</url>
    <content><![CDATA[<h3 id="图神经网络——超大规模数据集类的创建和图预测任务实践"><a href="#图神经网络——超大规模数据集类的创建和图预测任务实践" class="headerlink" title="图神经网络——超大规模数据集类的创建和图预测任务实践"></a>图神经网络——超大规模数据集类的创建和图预测任务实践</h3><p>当数据集规模超级大时，很难有足够大的内存完全存下所有数据。因此需要一个按需加载样本到内存的数据集类。</p>
<h4 id="1-Dataset基类"><a href="#1-Dataset基类" class="headerlink" title="1 Dataset基类"></a>1 <code>Dataset</code>基类</h4><h5 id="1-1-Dataset基类介绍"><a href="#1-1-Dataset基类介绍" class="headerlink" title="1.1 Dataset基类介绍"></a>1.1 <code>Dataset</code>基类介绍</h5><p>在PyG中，通过继承<code>torch_geometric.data.Dataset</code>基类来自定义一个按需加载样本到内存的数据集类。<br>继承此基类相比较继承<code>torch_geometric.data.InMemoryDataset</code>基类要多实现以下方法：</p>
<ul>
<li><code>len()</code>：返回数据集中的样本的数量。</li>
<li><code>get()</code>：实现加载单个图的操作。注意：在内部，getitem()返回通过调用get()来获取Data对象，并根据transform参数对它们进行选择性转换。</li>
</ul>
<span id="more"></span>
<h5 id="1-2-继承torch-geometric-data-Dataset基类的代码实现"><a href="#1-2-继承torch-geometric-data-Dataset基类的代码实现" class="headerlink" title="1.2 继承torch_geometric.data.Dataset基类的代码实现"></a>1.2 继承torch_geometric.data.Dataset基类的代码实现</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os.path <span class="keyword">as</span> osp</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> Dataset, download_url</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyOwnDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, root, transform=<span class="literal">None</span>, pre_transform=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyOwnDataset, self).__init__(root, transform, pre_transform)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">raw_file_names</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&#x27;some_file_1&#x27;</span>, <span class="string">&#x27;some_file_2&#x27;</span>, ...]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">processed_file_names</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&#x27;data_1.pt&#x27;</span>, <span class="string">&#x27;data_2.pt&#x27;</span>, ...]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">download</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># Download to `self.raw_dir`.</span></span><br><span class="line">        path = download_url(url, self.raw_dir)</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process</span>(<span class="params">self</span>):</span></span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> raw_path <span class="keyword">in</span> self.raw_paths:</span><br><span class="line">            <span class="comment"># Read data from `raw_path`.</span></span><br><span class="line">            data = Data(...)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.pre_filter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="keyword">not</span> self.pre_filter(data):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.pre_transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                data = self.pre_transform(data)</span><br><span class="line"></span><br><span class="line">            torch.save(data, osp.join(self.processed_dir, <span class="string">&#x27;data_&#123;&#125;.pt&#x27;</span>.<span class="built_in">format</span>(i)))</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">len</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.processed_file_names)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        data = torch.load(osp.join(self.processed_dir, <span class="string">&#x27;data_&#123;&#125;.pt&#x27;</span>.<span class="built_in">format</span>(idx)))</span><br><span class="line">        <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure>
<h5 id="1-3-特殊情况"><a href="#1-3-特殊情况" class="headerlink" title="1.3 特殊情况"></a>1.3 特殊情况</h5><ul>
<li>download/process步骤可以跳过<br>对于无需下载数据集原文件的情况，不重写（override）<code>download</code>方法即可跳过下载。<br>对于无需对数据集做预处理的情况，不重写<code>process</code>方法即可跳过预处理。</li>
<li>有些Dataset类无需定义<br>可以不用定义一个<code>Dataset</code>类，而直接生成一个<code>Dataloader</code>对象，直接用于训练：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> Data, DataLoader</span><br><span class="line"></span><br><span class="line">data_list = [Data(...), ..., Data(...)]</span><br><span class="line">loader = DataLoader(data_list, batch_size=<span class="number">32</span>)</span><br></pre></td></tr></table></figure>
或者将一个列表的<code>Data</code>对象组成一个<code>batch</code>：<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> Data, Batch</span><br><span class="line"></span><br><span class="line">data_list = [Data(...), ..., Data(...)]</span><br><span class="line">loader = Batch.from_data_list(data_list, batch_size=<span class="number">32</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="2-图样本封装成批（BATCHING）与DataLoader类"><a href="#2-图样本封装成批（BATCHING）与DataLoader类" class="headerlink" title="2 图样本封装成批（BATCHING）与DataLoader类"></a>2 图样本封装成批（BATCHING）与<code>DataLoader</code>类</h4><h5 id="2-1-合并小图组成大图"><a href="#2-1-合并小图组成大图" class="headerlink" title="2.1 合并小图组成大图"></a>2.1 合并小图组成大图</h5><p>PyTorch Geometric中采用的是将多个图封装成批的方式，将小图作为连通组件（connected component）的形式合并，构建一个大图。于是小图的邻接矩阵存储在大图邻接矩阵的对角线上。<br>该方法的优势在于：</p>
<ul>
<li>依靠消息传递方案的GNN运算不需要被修改。</li>
<li>没有额外的计算或内存的开销。（因为它们是以稀疏的方式保存的，只保留非零项）</li>
</ul>
<p>通过<a href="https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.DataLoader"><code>torch_geometric.data.DataLoader</code></a>类，多个小图被封装成一个大图。</p>
<h5 id="2-2-小图的属性增值与拼接"><a href="#2-2-小图的属性增值与拼接" class="headerlink" title="2.2 小图的属性增值与拼接"></a>2.2 小图的属性增值与拼接</h5><p>将小图存储到大图中时需要对小图的属性做一些修改，比如对节点序号增值，PyTorch Geometric的<code>DataLoader</code>类会自动对<code>edge_index</code>张量增值，增加的值为当前被处理图的前面的图的累积节点数量。</p>
<h5 id="2-3-图的匹配（Pairs-of-Graphs）"><a href="#2-3-图的匹配（Pairs-of-Graphs）" class="headerlink" title="2.3 图的匹配（Pairs of Graphs）"></a>2.3 图的匹配（Pairs of Graphs）</h5><p>不同类型的节点数量不一致，<code>edge_index</code>边的源节点与目标节点进行增值操作不同。</p>
<h5 id="2-4-二部图（Bipartite-Graphs）"><a href="#2-4-二部图（Bipartite-Graphs）" class="headerlink" title="2.4 二部图（Bipartite Graphs）"></a>2.4 二部图（Bipartite Graphs）</h5><p>部图是图论中的一种特殊模型。设$G=(V,E)$是一个无向图，如果顶点$V$可分割为两个互不相交的子集$(A,B)$，并且图中的每条边$(i,j)$所关联的两个顶点i和j分别属于这两个不同的顶点集$(i in A,j in B)$，则称图$G$为一个二部图。它的邻接矩阵定义两种类型的节点之间的连接关系。一般来说，不同类型的节点数量不需要一致，于是二部图的邻接矩阵$A \in {0,1}^{N \times M}$可能为平方矩阵，即可能有$N \neq M$。</p>
<h5 id="2-5-在新的维度上做拼接"><a href="#2-5-在新的维度上做拼接" class="headerlink" title="2.5 在新的维度上做拼接"></a>2.5 在新的维度上做拼接</h5><p>图级别属性或预测目标,<code>Data</code>对象的属性需要在一个新的维度上做拼接，此时形状为<code>[num_features]</code>的属性列表应该被返回为<code>[num_examples, num_features]</code>，而不是<code>[num_examples * num_features]</code>。</p>
<h4 id="3-创建超大规模数据集类实践"><a href="#3-创建超大规模数据集类实践" class="headerlink" title="3 创建超大规模数据集类实践"></a>3 创建超大规模数据集类实践</h4><p><a href="https://ogb.stanford.edu/kddcup2021/pcqm4m/"><strong>PCQM4M-LSC</strong></a>是一个分子图的量子特性回归数据集，它包含了3,803,453个图。<br>定义的数据集类如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> os.path <span class="keyword">as</span> osp</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> ogb.utils <span class="keyword">import</span> smiles2graph</span><br><span class="line"><span class="keyword">from</span> ogb.utils.torch_util <span class="keyword">import</span> replace_numpy_with_torchtensor</span><br><span class="line"><span class="keyword">from</span> ogb.utils.url <span class="keyword">import</span> download_url, extract_zip</span><br><span class="line"><span class="keyword">from</span> rdkit <span class="keyword">import</span> RDLogger</span><br><span class="line"><span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> Data, Dataset</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"></span><br><span class="line">RDLogger.DisableLog(<span class="string">&#x27;rdApp.*&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyPCQM4MDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, root</span>):</span></span><br><span class="line">        self.url = <span class="string">&#x27;https://dgl-data.s3-accelerate.amazonaws.com/dataset/OGB-LSC/pcqm4m_kddcup2021.zip&#x27;</span></span><br><span class="line">        <span class="built_in">super</span>(MyPCQM4MDataset, self).__init__(root)</span><br><span class="line"></span><br><span class="line">        filepath = osp.join(root, <span class="string">&#x27;raw/data.csv.gz&#x27;</span>)</span><br><span class="line">        data_df = pd.read_csv(filepath)</span><br><span class="line">        self.smiles_list = data_df[<span class="string">&#x27;smiles&#x27;</span>]</span><br><span class="line">        self.homolumogap_list = data_df[<span class="string">&#x27;homolumogap&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">raw_file_names</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;data.csv.gz&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">download</span>(<span class="params">self</span>):</span></span><br><span class="line">        path = download_url(self.url, self.root)</span><br><span class="line">        extract_zip(path, self.root)</span><br><span class="line">        os.unlink(path)</span><br><span class="line">        shutil.move(osp.join(self.root, <span class="string">&#x27;pcqm4m_kddcup2021/raw/data.csv.gz&#x27;</span>), osp.join(self.root, <span class="string">&#x27;raw/data.csv.gz&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">len</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.smiles_list)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        smiles, homolumogap = self.smiles_list[idx], self.homolumogap_list[idx]</span><br><span class="line">        graph = smiles2graph(smiles)</span><br><span class="line">        <span class="keyword">assert</span>(<span class="built_in">len</span>(graph[<span class="string">&#x27;edge_feat&#x27;</span>]) == graph[<span class="string">&#x27;edge_index&#x27;</span>].shape[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">assert</span>(<span class="built_in">len</span>(graph[<span class="string">&#x27;node_feat&#x27;</span>]) == graph[<span class="string">&#x27;num_nodes&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        x = torch.from_numpy(graph[<span class="string">&#x27;node_feat&#x27;</span>]).to(torch.int64)</span><br><span class="line">        edge_index = torch.from_numpy(graph[<span class="string">&#x27;edge_index&#x27;</span>]).to(torch.int64)</span><br><span class="line">        edge_attr = torch.from_numpy(graph[<span class="string">&#x27;edge_feat&#x27;</span>]).to(torch.int64)</span><br><span class="line">        y = torch.Tensor([homolumogap])</span><br><span class="line">        num_nodes = <span class="built_in">int</span>(graph[<span class="string">&#x27;num_nodes&#x27;</span>])</span><br><span class="line">        data = Data(x, edge_index, edge_attr, y, num_nodes=num_nodes)</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取数据集划分</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_idx_split</span>(<span class="params">self</span>):</span></span><br><span class="line">        split_dict = replace_numpy_with_torchtensor(torch.load(osp.join(self.root, <span class="string">&#x27;pcqm4m_kddcup2021/split_dict.pt&#x27;</span>)))</span><br><span class="line">        <span class="keyword">return</span> split_dict</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    dataset = MyPCQM4MDataset(<span class="string">&#x27;dataset2&#x27;</span>)</span><br><span class="line">    <span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> DataLoader</span><br><span class="line">    <span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line">    dataloader = DataLoader(dataset, batch_size=<span class="number">256</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(dataloader):</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>在生成一个该数据集类的对象时:</p>
<ul>
<li>首先会检查指定的文件夹下是否存在<code>data.csv.gz</code>文件，如果不在，则会执行<code>download</code>方法，这一过程是在运行<code>super</code>类的<code>__init__</code>方法中发生的。</li>
<li>然后程序继续执行<code>__init__</code>方法的剩余部分，读取<code>data.csv.gz</code>文件，获取存储图信息的<code>smiles</code>格式的字符串，以及回归预测的目标<code>homolumogap</code>。由<code>smiles</code>格式的字符串转成图的过程在<code>get()</code>方法中实现，这样在生成一个<code>ataLoader</code>变量时，通过指定<code>num_workers</code>可以实现并行执行生成多个图。</li>
</ul>
<h4 id="4-图预测任务实践"><a href="#4-图预测任务实践" class="headerlink" title="4 图预测任务实践"></a>4 图预测任务实践</h4><h5 id="4-1-通过试验寻找最佳超参数"><a href="#4-1-通过试验寻找最佳超参数" class="headerlink" title="4.1 通过试验寻找最佳超参数"></a>4.1 通过试验寻找最佳超参数</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/sh</span></span><br><span class="line"></span><br><span class="line">python main.py  --task_name GINGraphPooling\    # 为当前试验取名</span><br><span class="line">                --device 0\                     </span><br><span class="line">                --num_layers 5\                 # 使用GINConv层数</span><br><span class="line">                --graph_pooling sum\            # 图读出方法</span><br><span class="line">                --emb_dim 256\                  # 节点嵌入维度</span><br><span class="line">                --drop_ratio 0.\</span><br><span class="line">                --save_test\                    # 是否对测试集做预测并保留预测结果</span><br><span class="line">                --batch_size 512\</span><br><span class="line">                --epochs 100\</span><br><span class="line">                --weight_decay 0.00001\</span><br><span class="line">                --early_stop 10\                # 当有early_stop个epoches验证集结果没有提升，则停止训练</span><br><span class="line">                --num_workers 4\</span><br><span class="line">                --dataset_root dataset          # 存放数据集的根目录</span><br></pre></td></tr></table></figure>
<p>这段代码运行后，程序会在<code>saves</code>目录下创建一个<code>task_name</code>参数指定名称的文件夹用于记录试验过程。试验运行过程中，所有的<code>print</code>输出都会写入到试验文件夹下的<code>output</code>文件，<code>tensorboard.SummaryWriter</code>记录的信息也存储在试验文件夹下的文件中。<br>修改上方的命令再执行，即可试验不同的超参数，所有试验的过程与结果信息都存储于<code>saves</code>文件夹下。启动<code>TensorBoard</code>会话，选择<code>saves</code>文件夹，即可查看所有试验的过程与结果信息。</p>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://github.com/datawhalechina/team-learning-nlp/tree/master/GNN">datawhale-GNN开源学习资料</a><br>2.<a href="https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.InMemoryDataset"><code>Dataset</code>类官方文档</a></p>
]]></content>
      <categories>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>Dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>地理数据分析常用工具</title>
    <url>/2021/04/15/%E5%9C%B0%E7%90%86%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7/</url>
    <content><![CDATA[<p>在地理空间数据分析中，常用一些模块进行地理数据分析、特征提取及可视化，包括shapely、geopandas、folium、kepler.gl、geohash等工具。</p>
<h4 id="1-shapely"><a href="#1-shapely" class="headerlink" title="1. shapely"></a>1. shapely</h4><p>shapely是基于笛卡尔坐标的几何对象操作和分析Python库，底层基于GEOS和JTS拓扑运算库。</p>
<h5 id="1-1-Point对象"><a href="#1-1-Point对象" class="headerlink" title="1.1 Point对象"></a>1.1 Point对象</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> shapely.geometry <span class="keyword">import</span> Point</span><br><span class="line"></span><br><span class="line">point1 = Point(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">point2 = Point(<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">point3 = Point(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#点的可视化</span></span><br><span class="line">geo.GeometryCollection([point1,point2,point3])</span><br><span class="line"></span><br><span class="line"><span class="comment">#Point转为numpy数组</span></span><br><span class="line"><span class="built_in">print</span>(np.array(point))</span><br></pre></td></tr></table></figure>
<span id="more"></span>

<h5 id="1-2-LineString对象"><a href="#1-2-LineString对象" class="headerlink" title="1.2 LineString对象"></a>1.2 LineString对象</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建LineString对象</span></span><br><span class="line">line1 = geo.LineString([(<span class="number">0</span>,<span class="number">0</span>),(<span class="number">2</span>,-<span class="number">2.2</span>),(<span class="number">3</span>,<span class="number">3.3</span>),(<span class="number">4</span>,-<span class="number">4.4</span>),(<span class="number">5</span>,-<span class="number">5.5</span>),(<span class="number">6</span>,<span class="number">6.6</span>)])</span><br><span class="line">line1 </span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#点和线的可视化</span></span><br><span class="line">geo.GeometryCollection([line1,Point(<span class="number">1</span>,<span class="number">1</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算点线或线线的最短距离</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;点线距离:&#x27;</span>+<span class="built_in">str</span>(Point(<span class="number">1</span>,<span class="number">1</span>).distance(line1)))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#几何中心</span></span><br><span class="line">center = line1.centroid </span><br><span class="line">geo.GeometryCollection([line1,center])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#几何对象的最小外接矩形</span></span><br><span class="line">b_rect = line1.envelope </span><br><span class="line">geo.GeometryCollection([line1,b_rect]) </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> <span class="comment">#简化线(Douglas-Pucker算法)</span></span><br><span class="line">line1_simplify = line1.simplify(<span class="number">0.4</span>, preserve_topology=<span class="literal">False</span>) </span><br><span class="line">line1</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#生成线的缓冲区</span></span><br><span class="line">buffer_with_circle = line1.buffer(<span class="number">0.3</span>)  </span><br><span class="line">geo.GeometryCollection([line1,buffer_with_circle])</span><br></pre></td></tr></table></figure>
<h5 id="1-3-LinearRings对象"><a href="#1-3-LinearRings对象" class="headerlink" title="1.3 LinearRings对象"></a>1.3 LinearRings对象</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> shapely.geometry.polygon <span class="keyword">import</span> LinearRing</span><br><span class="line"></span><br><span class="line">ring = geo.polygon.LinearRing([(<span class="number">0</span>, <span class="number">0</span>), (<span class="number">1</span>, -<span class="number">1</span>), (<span class="number">1</span>, <span class="number">0</span>)])</span><br><span class="line">geo.GeometryCollection([ring])</span><br></pre></td></tr></table></figure>

<h5 id="1-4-Polygon对象"><a href="#1-4-Polygon对象" class="headerlink" title="1.4 Polygon对象"></a>1.4 Polygon对象</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> shapely.geometry <span class="keyword">import</span> Polygon</span><br><span class="line"></span><br><span class="line">poly1 = Polygon([(<span class="number">0</span>,<span class="number">0</span>),(<span class="number">2</span>,<span class="number">0</span>),(<span class="number">1</span>,-<span class="number">1</span>),(-<span class="number">3</span>,<span class="number">1</span>),(<span class="number">0</span>,<span class="number">0</span>)]) <span class="comment">#起点和终点相同</span></span><br><span class="line">poly1</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#通过numpy生成多边形</span></span><br><span class="line">coords = np.array([(<span class="number">0</span>,<span class="number">0</span>),(<span class="number">1</span>,<span class="number">0.1</span>),(<span class="number">2</span>,<span class="number">0</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">0</span>,<span class="number">0</span>)])</span><br><span class="line">poly2 = Polygon(coords)</span><br><span class="line">poly2 </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#多个polygon的集合</span></span><br><span class="line">geo.GeometryCollection([poly1,poly2])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#几何中心</span></span><br><span class="line">center = poly1.centroid <span class="comment">#几何中心</span></span><br><span class="line">geo.GeometryCollection([center,poly1]) </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#最小外接矩形</span></span><br><span class="line">rect = poly1.minimum_rotated_rectangle <span class="comment">#最小外接矩形</span></span><br><span class="line">geo.GeometryCollection([rect,poly1])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">poly1.boundary<span class="comment">#边缘</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">poly2.simplify(<span class="number">0.5</span>)<span class="comment">#简化面</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">r1 = poly2.contains(point2) <span class="comment">#面点关系</span></span><br><span class="line"><span class="built_in">print</span>(r1)</span><br><span class="line"></span><br><span class="line">r2 = poly2.intersects(line1) <span class="comment">#面线关系</span></span><br><span class="line"><span class="built_in">print</span>(r2)</span><br><span class="line"></span><br><span class="line">r3 = poly1.intersects(poly2) <span class="comment">#面面关系</span></span><br><span class="line"><span class="built_in">print</span>(r3)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">poly1.intersection(poly2) <span class="comment">#面面交集</span></span><br><span class="line">poly1.union(poly2) 		  <span class="comment">#面面并集</span></span><br><span class="line">poly2.difference(poly1)   <span class="comment">#面面补集</span></span><br></pre></td></tr></table></figure>
<h4 id="2-geopandas"><a href="#2-geopandas" class="headerlink" title="2. geopandas"></a>2. geopandas</h4><p>geopandas是pandas在地理数据处理领域的扩展包，其核心数据结构是GeoSeries和GeoDataFrame。<br>geopandasd 主要功能为:</p>
<blockquote>
<p>1.文件读写<br>2.空间查询<br>3.坐标转换<br>4.空间join<br>5.地理数据可视化</p>
</blockquote>
<h5 id="2-1-文件读写"><a href="#2-1-文件读写" class="headerlink" title="2.1 文件读写"></a>2.1 文件读写</h5><p>geopandas可读geojson和shp等空间文件，也可读含有geometry字段的csv文件。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> shapely </span><br><span class="line"><span class="keyword">import</span> geopandas <span class="keyword">as</span> gpd </span><br><span class="line"><span class="keyword">from</span> shapely <span class="keyword">import</span> wkt </span><br><span class="line"><span class="keyword">from</span> shapely <span class="keyword">import</span> geometry <span class="keyword">as</span> geo</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#读取geojson</span></span><br><span class="line">countries = gpd.read_file(<span class="string">&quot;dfcountries.geojson&quot;</span>,bbox = [-<span class="number">180</span>,-<span class="number">80</span>,<span class="number">180</span>,<span class="number">80</span>])</span><br><span class="line">countries.plot()<span class="comment">#显示图</span></span><br><span class="line">countries		<span class="comment">#显示表格</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#读取shp</span></span><br><span class="line">cities = gpd.GeoDataFrame.from_file(<span class="string">&#x27;./cities.shp&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">cities.plot()<span class="comment">#显示图</span></span><br><span class="line">cities		 <span class="comment">#显示表格</span></span><br></pre></td></tr></table></figure>
<p><img src="https://ae01.alicdn.com/kf/U1e0d77ed2da2455cb9957e8ed6f59c20a.jpg" alt="f1"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#保存成geojson</span></span><br><span class="line">countries.to_file(<span class="string">&quot;dfcountries.geojson&quot;</span>,driver = <span class="string">&quot;GeoJSON&quot;</span>)</span><br><span class="line"><span class="comment">#保存成csv</span></span><br><span class="line">cities.to_csv(<span class="string">&quot;dfcountries.csv&quot;</span>,index = <span class="literal">False</span>,sep = <span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="comment">#保存成shp</span></span><br><span class="line">cities.to_file(<span class="string">&quot;.a/cities.shp&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#按值的大小填充颜色</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">9</span>,<span class="number">6</span>),dpi = <span class="number">100</span>)</span><br><span class="line">cities.plot(<span class="string">&#x27;area&#x27;</span>,ax = ax,legend = <span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://ae01.alicdn.com/kf/U31c06924a773420bb62cc56d2ee9442ft.jpg" alt="f2"></p>
<h4 id="3-Folium"><a href="#3-Folium" class="headerlink" title="3.Folium"></a>3.Folium</h4><p>folium是一种交互式动态地图接口，可以用来画热力图、填充地图、路径图、散点标记等图。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> folium</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment">#指定坐标中心和缩放尺寸，生成交互式地图</span></span><br><span class="line">m = folium.Map(location=[<span class="number">30.33</span>,<span class="number">120.37</span>],zoom_start=<span class="number">10</span>)</span><br><span class="line">m</span><br></pre></td></tr></table></figure>
<p><img src="https://ae01.alicdn.com/kf/Uc6be8164498d4fda99801e6d30ac676eV.jpg" alt="f3"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> folium.plugins <span class="keyword">import</span> HeatMap</span><br><span class="line"><span class="comment">#生成随机数据，指定坐标中心和缩放尺寸，生成热力图</span></span><br><span class="line">data=(np.random.normal(size=(<span class="number">100</span>,<span class="number">3</span>))*np.array([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]])+np.array([[<span class="number">30.33</span>,<span class="number">120.37</span>,<span class="number">1</span>]])).tolist()</span><br><span class="line">m=folium.Map([<span class="number">30.33</span>,<span class="number">120.37</span>],tiles=<span class="string">&#x27;Stamen Toner&#x27;</span>,zoom_start=<span class="number">6</span>)</span><br><span class="line">HeatMap(data).add_to(m)</span><br><span class="line">m </span><br></pre></td></tr></table></figure>
<p><img src="https://ae01.alicdn.com/kf/U1fe179343a464ab1926cd10582207986X.jpg" alt="f4"></p>
<p>folium的MarkerCluster()聚类函数，可以用来反映一个区域的拥挤程度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> folium.plugins <span class="keyword">import</span> MarkerCluster</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建对象</span></span><br><span class="line">marker_cluster = MarkerCluster().add_to(m)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将经纬度加入聚类</span></span><br><span class="line"><span class="keyword">for</span> element <span class="keyword">in</span> data:</span><br><span class="line">    folium.Marker(location=[element[<span class="number">0</span>], element[<span class="number">1</span>]],icon=<span class="literal">None</span>).add_to(marker_cluster)</span><br><span class="line"></span><br><span class="line"><span class="comment">#添加至地图</span></span><br><span class="line">m.add_child(marker_cluster)</span><br><span class="line">m</span><br></pre></td></tr></table></figure>

<h4 id="4-Kepler-gl"><a href="#4-Kepler-gl" class="headerlink" title="4.Kepler.gl"></a>4.Kepler.gl</h4><p>Kepler.gl是Uber联合Mapbox推出的地理空间可视化工具，支持3种数据格式：CSV、JSON、GeoJSON。接下来将以杭州市OSM路网为例，制作路径流动动画。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keplergl <span class="keyword">import</span> KeplerGl</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#读取geojson</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./hzroad.json&#x27;</span>,encoding=<span class="string">&#x27;gb18030&#x27;</span>, errors=<span class="string">&#x27;ignore&#x27;</span>) <span class="keyword">as</span> g:</span><br><span class="line">    raw_roads = json.load(g)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#生成虚构时间戳信息和高度0</span></span><br><span class="line">start_time = time.mktime(time.strptime(<span class="string">&#x27;2020-05-29 20:00:00&#x27;</span>, <span class="string">&quot;%Y-%m-%d %H:%M:%S&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(raw_roads[<span class="string">&#x27;features&#x27;</span>].__len__()):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(raw_roads[<span class="string">&#x27;features&#x27;</span>][i][<span class="string">&#x27;geometry&#x27;</span>][<span class="string">&#x27;coordinates&#x27;</span>].__len__()):</span><br><span class="line">        <span class="comment"># 更新当前对应的时间戳</span></span><br><span class="line">        shift_time = <span class="built_in">int</span>((j / raw_roads[<span class="string">&#x27;features&#x27;</span>][i][<span class="string">&#x27;geometry&#x27;</span>][<span class="string">&#x27;coordinates&#x27;</span>].__len__())*<span class="number">3600</span>) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 高度设置为0</span></span><br><span class="line">        raw_roads[<span class="string">&#x27;features&#x27;</span>][i][<span class="string">&#x27;geometry&#x27;</span>][<span class="string">&#x27;coordinates&#x27;</span>][j] \</span><br><span class="line">            .extend([<span class="number">0</span>, </span><br><span class="line">                     <span class="built_in">int</span>(start_time) + shift_time])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keplergl <span class="keyword">import</span> KeplerGl</span><br><span class="line"><span class="comment"># 生成KeplerGl对象</span></span><br><span class="line">m = KeplerGl(height=<span class="number">400</span>, </span><br><span class="line">                data=&#123;<span class="string">&#x27;flow&#x27;</span>: raw_roads&#125;) <span class="comment"># data以图层名为键，对应的矢量数据为值</span></span><br><span class="line">m</span><br><span class="line">m.save_to_html(file_name=<span class="string">&#x27;./hangzhou.html&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="5-Geohash"><a href="#5-Geohash" class="headerlink" title="5.Geohash"></a>5.Geohash</h4><p>GeoHash是一种地址编码方法，可以将地理经纬度坐标编码为由字母和数字所构成的字符串。其原理类似哈希表：由于遍历列表查找时间复杂度高，而创建散列函数能够更高效地定位数据。而GeoHash将二维的经纬度坐标编码到一维的字符串中，在做地理位置索引时只需匹配字符串，便于信息的缓存和压缩。</p>
<p>GeoHash采用二分法不断缩小经度和纬度的区间来进行二进制编码，最后将经纬度分别产生的编码奇偶位交叉合并，再用字母数字表示。</p>
<p><img src="https://ae01.alicdn.com/kf/Uf288553e6d2c43038976764613dd1cc1x.jpg" alt="f6"></p>
<blockquote>
<p>p.s. 酷炫动图见公众号</p>
</blockquote>
]]></content>
      <categories>
        <category>Geospatial Data Analysis</category>
      </categories>
      <tags>
        <tag>geopandas</tag>
        <tag>shapely</tag>
        <tag>Kepler.gl</tag>
        <tag>folium</tag>
        <tag>geohash</tag>
      </tags>
  </entry>
  <entry>
    <title>异常检测——基于相似度的方法</title>
    <url>/2021/05/20/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>异常值通常具有更高的离群程度分数值，同时也更具有可解释性。在基于相似度的异常检测方法中，主要思想是异常点的表示与正常点不同。</p>
<h4 id="1-基于距离的度量"><a href="#1-基于距离的度量" class="headerlink" title="1.  基于距离的度量"></a>1.  基于距离的度量</h4><p>基于距离的异常检测有这样一个前提假设，即异常点的k近邻距离要远大于正常点。常见的方法是嵌套循环，第一层循环遍历每个数据，第二层循环进行异常判断计算当前点与其他点的距离，一旦发现多于k个数据点与当前点的距离在D之内，则将该点标记为非异常值。 此法计算的时间复杂度为O(N<sup>2</sup>)，当数据量比较大时需要修剪方法以加快距离计算。</p>
<span id="more"></span>
<h5 id="1-1-基于单元的方法"><a href="#1-1-基于单元的方法" class="headerlink" title="1.1 基于单元的方法"></a>1.1 基于单元的方法</h5><p>在基于单元格的技术中，数据空间被划分为单元格，单元格的宽度是阈值D和数据维数d的函数。<br><img src="https://z3.ax1x.com/2021/05/25/gzuzmn.jpg" alt="xx"><br>L1邻居表示通过最多1个单元间的边界可从该单元到达的单元格的集合，L2邻居表示通过跨越2个或3个边界而获得的那些单元格。<br>以二维数据为例，网格间的距离为D/2√d，其具有以下性质：<br>①单元格中两点之间的距离最多为D/2 。<br>②一个点与L1邻接点之间的距离最大为D。<br>③一个点与它的Lr邻居(其中r&gt;2)中的一个点之间的距离至少为D。</p>
<p>但是L2中并不能直接得出结论，需要另外定义规则，如：<br>①如果一个单元格中包含超过k个数据点及其L1邻居，那么这些数据点都不是异常值。<br>②如果单元A及其相邻L1和L2中包含少于k个数据点，则单元A中的所有点都是异常值。<br>利用第二条规则的修剪能力，对于包含至少一个数据点的每个单元格A，计算其中的点数及其L1和L2邻居的总和。 如果该数字不超过k，则将单元格A中的所有点标记为离群值。</p>
<p>对于此时仍未标记为异常值或非异常值的单元格中的数据点，需要明确计算其k最近邻距离，在L1和L2中不超过k个且距离小于D的数据点，则声明为异常值。</p>
<h5 id="1-2-基于索引的方法"><a href="#1-2-基于索引的方法" class="headerlink" title="1.2 基于索引的方法"></a>1.2 基于索引的方法</h5><p>对于一个给定数据集，基于索引的方法利用多维索引结构(如R树、k-d树)来搜索每个数据对象A在半径范围D内的相邻点。设M是一个异常值在其D-邻域内允许含有对象数的最大值，若发现某个数据对象A的D-邻域内出现M+1甚至更多个相邻点， 则判定对象A不是异常值。该算法时间复杂度在最坏情况下为O(kN<sup>2</sup>)，其中k是数据集维数，N是数据集包含对象的个数。但是构造索引的任务本身也需要密集复杂的计算量。</p>
<h4 id="2-基于密度的度量"><a href="#2-基于密度的度量" class="headerlink" title="2.  基于密度的度量"></a>2.  基于密度的度量</h4><p>基于密度的算法主要有局部离群因子(LocalOutlierFactor，LOF)，以及LOCI、CLOF等基于LOF的改进算法。LOF的关键步骤在于给每个数据点都分配一个离散度，其主要思想是：针对给定的数据集，对其中的任意一个数据点，如果在其局部邻域内的点都很密集，那么认为此数据点为正常数据点；而离群点则是距离正常数据点最近邻的点都比较远的数据点。</p>
<h5 id="2-1-k-距离"><a href="#2-1-k-距离" class="headerlink" title="2.1 k-距离"></a>2.1 k-距离</h5><p>类似k近邻的思路，首先定义一个k-距离，对于数据集D中的给定对象p，对象p与数据集D中任意点o的距离为d(po)。我们把数据集D中与对象p距离最近的k个相邻点的最远距离表示为k-distance(p)，把距离对象p距离第k近的点表示为o<sub>k</sub>，那么给定对象p和点o<sub>k</sub>之间的距离d(p,o<sub>k</sub>)=k-distance(p)，满足：<br>①在集合D中至少有不包括p在内的k个点 o’，其中o’∈D \ {p}，满足d(p,o’)≤d(p,o<sub>k</sub>)<br>②在集合D中最多有不包括p在内的k-1个点o’，其中o’∈D \ {p}，满足d(p,o’)&lt;d(po<sub>k</sub>)<br>直观一些理解，就是以对象p为中心，对数据集D中的所有点到p的距离进行排序，距离对象p第k近的点o<sub>k</sub>与p之间的距离就是k-距离。</p>
<h5 id="2-2-k-邻域"><a href="#2-2-k-邻域" class="headerlink" title="2.2 k-邻域"></a>2.2 k-邻域</h5><p>到对象p的距离小于等于k-距离的所有点的集合就是k-邻域：<br>$$<br>N_{k-dis\tan ce\left(p \right)}\left(p \right) =\left{ q\in D\backslash{q}\left| d\left(p,d \right) \right. \le k-ds\tan ce\left(p \right) \right}<br>$$<br>k-邻域包含对象p的第k距离以内的所有点，包括第k距离点；对象p的第k邻域点的个数丨N<sub>k</sub>(p)≥3丨。<br>在二维平面上展示，对象p的k-邻域是以对象p为圆心、k-距离为半径围成的圆形区域。<br><img src="https://www.hualigs.cn/image/608950de59262%E3%80%82jpg" alt="img"></p>
<h5 id="2-3-可达距离"><a href="#2-3-可达距离" class="headerlink" title="2.3  可达距离"></a>2.3  可达距离</h5><p>按照到对象o的距离远近，将数据集D内的点按照到o 的距离分为两类:<br>①若p<sub>i</sub>在对象o的k-邻域内，则可达距离就是给定点p<sub>i</sub>关于对象o的k-距离；<br>②若p<sub>i</sub>在对象o的k-邻域外，则可达距离就是给定点p<sub>i</sub>关于对象o的实际距离。<br>给定点p<sub>i</sub>关于对象o的可达距离用数学公式可以表示为：<br>$$<br>r e a c h−d i s t_ k (p,o) = max {k−distance(o),d (p,o)}<br>$$<br>这样的分类处理可以简化后续的计算，同时让得到的数值区分度更高。<br><img src="https://www.hualigs.cn/image/608951ddb06ae%E3%80%82jpg" alt="可达距离。jpg"><br>如图所示：<br>①p<sub>1</sub>在对象o的k-邻域内，d(p<sub>1</sub>,o)&lt;k−distance(o)，<br>可达距离reach−dist<sub>k</sub>(p<sub>1</sub>,o)=k−distance(o) ;<br>②p<sub>2</sub>在对象o的k-邻域外，d (p<sub>2</sub>,o)&gt;k−distance(o)，<br>可达距离reach−dist<sub>k</sub>(p<sub>2</sub>,o)= d(p<sub>2</sub>,o) ;</p>
<blockquote>
<p>需要注意的是:为了减少距离的计算开销，对象的k-邻域内的所有对象的k-距离计算量可以被显著降低，相当于使用一个阈值把需要计算的部分“截断”了， 的值越高，无需计算的邻近点越多，计算开销越小。但是另一方面，k的值变高，可能意味着可达距离变远，对集群点和离群点的区分度可能变低。因此，如何选择k值，是LOF算法能否达到效率与效果平衡的重要因素。</p>
</blockquote>
<h5 id="3-4-局部可达密度"><a href="#3-4-局部可达密度" class="headerlink" title="3.4 局部可达密度"></a>3.4 局部可达密度</h5><p>可以将“密度”直观地理解为点的聚集程度，简单来说，点与点之间距离越短，则密度越大。我们使用数据集D中对象p与对象o的k-邻域内所有点的可达距离平均值的倒数来定义局部可达密度。<br>在进行局部可达密度的计算的时候，我们需要避免数据集内所有数据落在同一点上，即所有可达距离之和为0的情况：此时局部密度为∞，后续计算将无法进行。</p>
<p>LOF算法中针对这一问题进行了如下的定义：对于数据集D内的给定对象p，存在至少MinPts(p)≥1个不同于p的点。因此，我们使用对象p到o∈N_{MinPts}(p)的可达距离reach-dist<sub>{MinPts}</sub>(p, o)作为度量对象p邻域的密度的值。点p的局部可达密度计算公式为：<br>$$<br>lrd_{MinPts}(p)=1/(\frac {\sum\limits_{o∈N_{MinPts}(p)} reach-dist_{MinPts}(p,o)} {\left\vert N_{MinPts}(p) \right\vert})<br>$$<br>由公式可看出，这里是对给定点p进行度量，计算其邻域内的所有对象o到给定点p的可达距离平均值。给定点p的局部可达密度越高，越可能与其邻域内的点p属于同一簇；密度越低，越可能是离群点。  </p>
<h5 id="3-5-局部异常因子"><a href="#3-5-局部异常因子" class="headerlink" title="3.5 局部异常因子"></a>3.5 局部异常因子</h5><p>得到lrd(局部可达密度)以后就可以将每个点的lrd将与它们的k个邻点的lrd进行比较，得到局部异常因子LOF。 LOF是对象p的邻居点o(o∈N<sub>MinPts</sub>(p))的lrd平均值与p的lrd的比值。</p>
<p>p的局部可达密度越低，且它的MinPts近邻的平均局部可达密度越高，则p的LOF值越高。如果这个比值越接近1，说明o的邻域点密度差不多，o可能和邻域同属一簇；如果这个比值小于1，说明o的密度高于其邻域点密度，o为密集点；如果这个比值大于1，说明o的密度小于其邻域点密度，o可能是异常点。<br><img src="https://www.hualigs.cn/image/6089531d9e0f7.jpg" alt="局部异常因子公式.png"><br>由上述公式计算出的LOF数值，就是我们所需要的离群点分数。</p>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://github.com/datawhalechina/team-learning-data-mining/tree/master/AnomalyDetection">datawhale-异常检测开源学习资料</a></p>
]]></content>
      <categories>
        <category>Anomaly Detection</category>
      </categories>
      <tags>
        <tag>LOF</tag>
      </tags>
  </entry>
  <entry>
    <title>异常检测——基于统计学的方法</title>
    <url>/2021/05/14/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E5%AD%A6%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>异常检测的统计方法包含两种主要类型：参数方法和非参数方法。<br><strong>参数方法：</strong>假定数据服从以θ为参数的参数分布，该参数分布的概率密度函数f(x,θ)给出x属于该分布的概率，该值越小，x越可能是异常点。(仅对数据做的统计假定满足实际约束时可行)<br><strong>非参数方法</strong>：不假定先验统计模型，假定参数的个数较为灵活，并非完全无参。</p>
<span id="more"></span>
<h4 id="1-参数方法"><a href="#1-参数方法" class="headerlink" title="1. 参数方法"></a>1. 参数方法</h4><h5 id="1-1-基于正态分布的一元异常点检测"><a href="#1-1-基于正态分布的一元异常点检测" class="headerlink" title="1.1 基于正态分布的一元异常点检测"></a>1.1 基于正态分布的一元异常点检测</h5><p>仅涉及单变量的数据称为一元数据，假定数据由正态分布产生，由输入数据学习正态分布的参数，并把低概率的点识别为异常点。<br>输入数据集<img src="https://ae01.alicdn.com/kf/U0f09d41be5f54bc4892bab4863e0cc3b6.jpg" alt="公式1"> ，根据公式可以求出参数均值μ和方差σ，和概率密度函数p(x)。<br>$$<br>\mu =\frac{1}{n}\sum_{i=1}^n{x_i}<br>$$<br>$$<br>\sigma ^2=\frac{1}{n}\sum_{i=1}^n{\left( x_i-\mu \right)}^2<br>$$<br>$$<br>p\left( x \right) =\frac{1}{\sqrt{2\pi}\sigma}\exp \left( -\frac{\left( x-\mu \right) ^2}{2\sigma ^2} \right)<br>$$<br>在正太分布的假设中，可以为p(x)取值设置阈值来判断异常点。<br>或者简单地用3σ法则来判断，(μ-3σ，μ+3σ)包含99.7%的数据，某数据超出这个范围，可以简单的标记为异常点(Outlier)。</p>
<h5 id="1-2-多元离群点的检测方法"><a href="#1-2-多元离群点的检测方法" class="headerlink" title="1.2 多元离群点的检测方法"></a>1.2 多元离群点的检测方法</h5><p>一元异常点检测的方法可以扩展为多元异常点检测。<br>对于一个n维数据集合<img src="https://ae01.alicdn.com/kf/U22473047196a4af9b6ebca2d9ed966d3A.jpg" alt="公式2">，当各个维度的特征之间相互独立，可以按上述公式计算每个维度的均值和方差。基于正态分布的假设，根据概率<img src="https://ae01.alicdn.com/kf/U98b94844e25b4646a6538fab43facfbc8.jpg" alt="公式3">的大小判断x是否属于异常值。<br>$$<br>p\left(\vec{x} \right) =\prod_{j=1}^n{p\left(x_j;\mu _j;\sigma <em>{j}^{2} \right)}=\prod</em>{j=1}^n{\frac{1}{\sqrt{2\pi}\sigma _j}}\exp \left(-\frac{\left(x_j-\mu <em>j \right) ^2}{2\sigma <em>{j}^{2}} \right)<br>$$<br>当各个维度的特征相关时，需要用到基于多元正太分布来进行异常点检测，首先计算n维的均值向量<br>$$<br>\vec{\mu}=\left( E\left( x_1 \right) ,…,E\left( x_n \right) \right)<br>$$<br>和n×n的协方差矩阵：<br>$$<br>\sum{=\frac{1}{m}}\sum</em>{i=1}^m{\left( \vec{x}-\vec{\mu} \right) \left( \vec{x}-\vec{\mu} \right) ^T}<br>$$<br>如有新数据，可以计算<img src="https://ae01.alicdn.com/kf/U98b94844e25b4646a6538fab43facfbc8.jpg" alt="公式3">,根据概率值大小判断是否属于异常值。<br>$$<br>p\left( \vec{x} \right) =\frac{1}{\left( 2\pi ^{\frac{n}{2}}\left| \sum{} \right|^{</em>{2}^{1}} \right)}\exp \left( -\frac{1}{2}\left( \vec{x}-\vec{\mu} \right) ^T\sum{^{-1}\left( \vec{x}-\vec{\mu} \right)} \right)<br>$$</p>
<h4 id="2-非参数方法"><a href="#2-非参数方法" class="headerlink" title="2. 非参数方法"></a>2. 非参数方法</h4><p>非参数方法是对数据做较少的假定，因而适合多数情况。</p>
<h5 id="2-1-基于角度的方法"><a href="#2-1-基于角度的方法" class="headerlink" title="2.1 基于角度的方法"></a>2.1 基于角度的方法</h5><p>基于角度的方法往往在高维空间里会很有效，其主要思想是：数据边界上的数据很可能将整个数据包围在一个较小的角度内，而内部的数据点则可能以不同的角度围绕着他们。如下图所示，其中点A是一个异常点，点B位于数据内部。<br><img src="https://z3.ax1x.com/2021/05/25/gzJbtA.png" alt="a"><br>如果数据点与其余点离得较远，则潜在角度可能越小。因此，具有较小角度谱的数据点较有可能异常值，而具有较大角度谱的数据点不太可能是异常值。</p>
<h5 id="2-2-基于频数直方图的无监督异常点检测算法-HBOS"><a href="#2-2-基于频数直方图的无监督异常点检测算法-HBOS" class="headerlink" title="2.2 基于频数直方图的无监督异常点检测算法(HBOS)"></a>2.2 基于频数直方图的无监督异常点检测算法(HBOS)</h5><p>HBOS(Histogram-based Outlier Score)是一种单变量方法组合，HBOS在全局异常检测问题上表现良好，但不能检测局部异常值。但是HBOS比标准算法快得多，尤其是在大数据集上。<br>HOBS不能对特征之间的依赖关系进行建模，其基本假设是数据集的每个维度相互独立。然后对每个维度进行区间(bin)划分，对每一个bin进行评分。区间的密度越低，异常评分越高，越可能是异常点。<br>①等宽分桶：标准直方图构建，在值范围内构造k个等宽箱，样本若如每个箱的概率作为密度的估计[时间复杂度O(n)]<br>②动态宽度分桶：对所有值进行排序，将固定数量的N/k个连续值装进一个箱。N是总实例数，k是箱个数，所有箱面积一样，跨度大的箱的高度低，即密度小(例外情况：超过k个数相等，此时允许在同一个箱里超过N/k值)<br>对每个维度都计算一个独立的直方图，其中每个箱子的高度表示密度的估计，对直方图进行归一化处理，最后计算每一个实例的HBOS值。<br>$$<br>HBOS\left( p \right) =\sum_{i=0}^d{\log \left( \frac{1}{hist_i\left( p \right)} \right)}<br>$$<br>详细推导过程：<br>假设样本o的第i个特征概率密度为<img src="https://ae01.alicdn.com/kf/U3264e024f94c45a0bbe5110c82bca661k.jpg" alt="x">，则o的概率密度为：<br>$$<br>P\left( p \right) =P_1\left( p \right) P_2\left( p \right) …P_d\left( p \right)<br>$$<br>两边取对数<br>$$<br>\log \left( P\left( p \right) \right) =\log \left( P_1\left( p \right) P_2\left( p \right) …P_d\left( p \right) \right) =\sum_{i=1}^d{\log \left( P_i\left( p \right) \right)}<br>$$<br>为了达到概率密度越大异常评分越小，两边同乘-1。<br>$$<br>-\log \left( P\left( p \right) \right) =-\sum_{i=1}^d{\log \left( P_i\left( p \right) \right)}=\sum_{i=1}^d{\frac{1}{\log \left( P_i\left( p \right) \right)}}<br>$$<br>最后：<br>$$<br>HOBS\left( p \right) =-\log \left( P\left( p \right) \right) =\sum_{i=1}^d{\frac{1}{\log \left( P_i\left( p \right) \right)}}<br>$$</p>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://github.com/datawhalechina/team-learning-data-mining/tree/master/AnomalyDetection">datawhale-异常检测开源学习资料</a></p>
]]></content>
      <categories>
        <category>Anomaly Detection</category>
      </categories>
      <tags>
        <tag>Normal distribution</tag>
        <tag>HBOS</tag>
      </tags>
  </entry>
  <entry>
    <title>异常检测——线性相关方法</title>
    <url>/2021/05/17/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>真实数据集中不同维度的特征可能具有高度相关性，这是因为不同的特征往往是由相同的基础过程以密切相关的方式产生的。这被称为——回归建模，是一种参数化的相关性分析。<br>一类相关性分析通过其他变量预测单独的属性值，另一类方法用一些潜在变量来代表整个数<br>据。前者的代表是<strong>线性回归</strong>，后者一个典型的例子是<strong>主成分分析</strong>。<br>线性相关分析基于的假设是：①近似线性相关假设；②子空间假设。为了确定线性模型是否适合数据集，需要进行探索性和可视化分析。</p>
<span id="more"></span>
<h4 id="0-探索性和可视化分析"><a href="#0-探索性和可视化分析" class="headerlink" title="0. 探索性和可视化分析"></a>0. 探索性和可视化分析</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data.describe()</span><br><span class="line">train_data.info()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#相关性分析</span></span><br><span class="line">numeric_features = [<span class="string">&#x27;f&#x27;</span>+<span class="built_in">str</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>)]</span><br><span class="line">numeric = train_data[numeric_features]</span><br><span class="line">correlation.numeric.corr()</span><br><span class="line"></span><br><span class="line">f,ax = plt.subplots(figsize=(<span class="number">15</span>,<span class="number">15</span>))</span><br><span class="line">sns.heatmap(correlation,square=<span class="literal">True</span>)</span><br><span class="line">plt.tile(<span class="string">&#x27;Correlation of Numeric features&#x27;</span>,y=<span class="number">1</span>,size=<span class="number">16</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://z3.ax1x.com/2021/05/25/gzuhyd.jpg" alt="xx"></p>
<h4 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1. 线性回归"></a>1. 线性回归</h4><p>线性回归：假设不同维度的变量具有一定的相关性，并可以通过一个相关系数矩阵进行衡量。<br>在线性回归中，异常值是根据自变量对因变量的影响来定义的，自变量之间相互关系中的异常则不那么重要。(此处的异常点检测是基于数据点的整体分布)</p>
<h5 id="1-1-最小二乘法"><a href="#1-1-最小二乘法" class="headerlink" title="1.1 最小二乘法"></a>1.1 最小二乘法</h5><p>基于最小二乘拟合的线性回归可以如下定义：带标签数据集<br>$$<br>D=\left{ \left( x_1,y_1 \right) ,…,\left( x_m,y_m \right) \right}<br>$$<br>m个样本的特征组成矩阵X；m个样本的标签组成标签向量<br>$$<br>y=\left( y_1,…,y_m \right) ^T<br>$$<br>目的是求出θ，使得目标函数J(θ)最小。<br>$$<br>J\left( \theta \right) =\lVert \left. X\theta -y \rVert <em>{2}^{2} \right.<br>$$<br>为什么目标函数J(θ)是残差向量的2-范数的平方？可以这样解释：<br>某个样本(x<sub>i</sub>,y<sub>i</sub>)在θ确定情况下满足：<br>$$<br>y_i=\theta ^Tx_i+\epsilon <em>i<br>$$<br>ϵi为误差项，可以看作独立同分布的随机变量，且服从高斯分布。可以以计算yi的概率密度函数：<br>$$<br>L\left( \theta \right) =p\left( y_i\left| x_i;\theta \right. \right) =\frac{1}{\sqrt{2\pi}\sigma}\exp \left( -\frac{\left( y_i-\theta ^Tx_i \right) ^2}{2\sigma ^2} \right)<br>$$<br>要使J(θ)最小，就要使使L(θ)最大，可使用最大似然估计法。<br>$$<br>L\left( \theta \right) =\log \prod</em>{i=1}^m{\frac{1}{\sqrt{2\pi}\sigma}\exp \left( -\frac{\left( y_i-\theta ^Tx_i \right) ^2}{2\sigma ^2} \right)}=m\log \frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{\sigma ^2}·\frac{1}{2}\sum</em>{i-=1}^m{\left( y_i-\theta ^Tx_i \right)}^2<br>$$<br>通常做法是对似然函数求导，使导数为0。上述也证明了要让L(θ)最大就是要让J(θ)最小。<br>$$<br>\sum_{i-=1}^m{\left( y_i-\theta ^Tx_i \right)}^2=\lVert \left. X\theta -y \rVert \right. _{2}^{2}<br>$$<br>说回最小二乘，目的是求出最优点θ使J(θ)最小。<br>设残差向量r=Xθ−y，在二维空间中讨论，Xθ为一条直线，y为一个向量，存在最优点θ<sup>✲</sup>使得J(θ)最小，这个θ<sup>✲</sup>是最小二乘解集合中的一个元素，如图所示：<br><img src="https://z3.ax1x.com/2021/05/25/gzufQH.png" alt="xx"><br>由图看出这个θ<sup>✲</sup>是r垂直于Xθ时取到。由定义，Xθ是矩阵X的列空间C(X)，与C(X)垂直的是矩阵X的左零空间N(X<sup>T</sup>)，由左零空间定义：<br>$$<br>X^Tr=0<br>$$<br>把r=Xθ−y带入，得到：<br>$$<br>X^T\left( X\theta -y \right) =0<br>$$<br>整理得最优参数θ为：<br>$$<br>\theta =\left( X^T·X \right) ^{-1}·\left( X^T·y \right)<br>$$<br>个人觉得J(θ)也可以认为是某种异常得分。</p>
<h5 id="1-2-梯度下降"><a href="#1-2-梯度下降" class="headerlink" title="1.2 梯度下降"></a>1.2 梯度下降</h5><p>线性回归的优化目标是损失函数，就需要一种优化算法。<br>线性回归中的常用损失函数是均方误差MSE，表达式为：<br>$$<br>L\left( w,b \right) =\frac{1}{2}\left( \hat{y}^{\left( i \right)}-y^{\left( i \right)} \right) ^2 = \frac{1}{n}\sum_{i=1}^n{l^{\left( i \right)}}\left( w,b \right) =\frac{1}{n}\sum_{i=1}{\frac{1}{2}\left( W^Tx^{\left( i \right)}+b-y^{\left( i \right)} \right)}^2<br>$$<br>当模型和损失函数形式较为简单时，面的误差最小化问题的解可以直接用公式表达出来。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。<br>在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）被广泛使用。他的算法是：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch），然后求小批量中数据样本的平均损失和有关模型参数的导数（梯度），最后用此结果与预先设定的学习率的乘积作为模型参数在本次迭代的减小量。如下式所示：<br>$$<br>\left( w,b \right) \gets \left( w,b \right) -\frac{\eta}{\left| \left. \mathbb{B} \right| \right.}\sum_{i\in \mathbb{B}}{\partial _{\left( w,b \right)}}l^{\left( i \right)}\left( w,b \right)<br>$$<br>B表示批量大小batch size；η表示学习率，代表每次学习的步长大小。</p>
<h4 id="2-主成分分析"><a href="#2-主成分分析" class="headerlink" title="2. 主成分分析"></a>2. 主成分分析</h4><p>最小二乘法试图找到一个与数据具有最佳匹配(d-1)维超平面；主成分分析方法可用于解决这一问题的广义版本，它可以找到任意k(k&lt;d)维的最优表示超平面，从而使平方投<br>影误差最小化。</p>
<h5 id="2-1-原理"><a href="#2-1-原理" class="headerlink" title="2.1 原理"></a>2.1 原理</h5><p>对于d维，包含N个样本的数据，用R<sub>i</sub>表示其中第i行[x<sub>i1</sub>,…x<sub>id</sub>]，可得到d×d的协方差矩阵：<br>$$<br>\sum{=\left( R-\bar{R} \right)}^T·\left( R-\bar{R} \right)<br>$$<br>∑是对称并且半正定的，因此可以进行相似对角化：<br>$$<br>\sum{=P·D·P^T}<br>$$<br>这里的D为对角矩阵，对角元素为特征值。P为标准正交矩阵，每一行为对应的特征向量；这些标准正交向量提供了数据应该投影的轴线方向。<br>将样本的协方差矩阵特征值分解以后，特征值就是样本投影到这个轴上后对应的方差，特征值越小，说明投影以后在这个轴上样本点分布集中，而异常点在这种情况下更容易偏移，利用这一点可以作为衡量样本异常的一个指标。在PCA做降维的时候，起作用的是大的特征值对应的特征向量，而在异常检测中，起作用的是特征值小的对应的特征向量。根据以上思想，我们可以定义PCA中一个点x的异常评分：<br>$$<br>Score\left( x \right) =\sum_{i=1}^d{\frac{\left| \left. x^Te_i \right| \right.}{\lambda _i}}<br>$$<br>其中，e<sub>i</sub>为第i个特征向量，λ为沿该方向的方差(也是特征值)，可以看出对异常得分的大部分贡献是由λ值较小的主成分的提供的。</p>
<h5 id="2-2-PyOD实例"><a href="#2-2-PyOD实例" class="headerlink" title="2.2 PyOD实例"></a>2.2 PyOD实例</h5><p>测试PCA和HBOS的性能对比。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> pyod.models.pca <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> pyod.models.hbos <span class="keyword">import</span> HBOS</span><br><span class="line"><span class="keyword">from</span> pyod.utils.data <span class="keyword">import</span> evaluate_print</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    data = pd.read_csv(<span class="string">&#x27;dataverse_files/breast-cancer-unsupervised-ad.csv&#x27;</span>, header=<span class="literal">None</span>)</span><br><span class="line">    data_x=data[[i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">30</span>)]].values <span class="comment">#特征</span></span><br><span class="line">    data_y=data[<span class="number">30</span>].<span class="built_in">map</span>(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x==<span class="string">&#x27;o&#x27;</span> <span class="keyword">else</span> <span class="number">0</span>).values <span class="comment">#标签</span></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=<span class="number">0.3</span>,stratify=data_y)<span class="comment">#使用分层抽样，构建训练集和测试集</span></span><br><span class="line">    train_number=X_train.shape[<span class="number">0</span>]<span class="comment">#训练集样本的个数</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">#PCA方法</span></span><br><span class="line">    clf1 = PCA(n_components=<span class="number">15</span>)</span><br><span class="line">    clf1.fit(X_train)</span><br><span class="line">    y_train_scores_clf1 = clf1.decision_scores_</span><br><span class="line">    y_test_scores_clf1 = clf1.decision_function(X_test)</span><br><span class="line"></span><br><span class="line">	<span class="comment">#HBOS方法</span></span><br><span class="line">    clf2 = HBOS(n_bins=(<span class="built_in">int</span>)(np.sqrt(train_number)))</span><br><span class="line">    clf2.fit(X_train)</span><br><span class="line">    y_train_scores_clf2 = clf2.decision_scores_</span><br><span class="line">    y_test_scores_clf2 = clf2.decision_function(X_test)</span><br><span class="line"></span><br><span class="line">	<span class="comment">#结果对比</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Training Data:&quot;</span>)</span><br><span class="line">    evaluate_print(<span class="string">&#x27;PCA&#x27;</span>, y_train, y_train_scores_clf1)</span><br><span class="line">    evaluate_print(<span class="string">&#x27;HBOS&#x27;</span>, y_train, y_train_scores_clf2)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Test Data:&quot;</span>)</span><br><span class="line">    evaluate_print(<span class="string">&#x27;PCA&#x27;</span>, y_test, y_test_scores_clf1)</span><br><span class="line">    evaluate_print(<span class="string">&#x27;HBOS&#x27;</span>, y_test, y_test_scores_clf2)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果发现，PCA的性能优于HBOS。</p>
<blockquote>
<p>局限性:<br>1.回归分析作为检测离群值的工具有一些局限性：为了使回归分析技术有效，数据需要高度相关，并沿着低维子空间对齐。当数据不相关，但在某些区域高度聚集时，这种方法可能不会有效。<br>2.数据中的相关性在本质上可能不是全局性的。子空间相关性可能是特定于数<br>据的特定位置的。在这种情况下，由主成分分析发现的全局子空间对于异常检测是次优的。因此，为了创建更一般的局部子空间模型，有时将线性模型与邻近模型结合起来是有用的。</p>
</blockquote>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://github.com/datawhalechina/team-learning-data-mining/tree/master/AnomalyDetection">datawhale-异常检测开源学习资料</a></p>
]]></content>
      <categories>
        <category>Anomaly Detection</category>
      </categories>
      <tags>
        <tag>OLSE</tag>
        <tag>Linear Regression</tag>
        <tag>Gradient descent</tag>
        <tag>PCA</tag>
        <tag>PyOD</tag>
      </tags>
  </entry>
  <entry>
    <title>异常检测——集成方法</title>
    <url>/2021/05/23/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<blockquote>
<p>一般情况下，可以把异常检测看成是数据不平衡下的分类问题，如果数据条件允许，优先使用有监督异常检测如XGBOOST；仅有少量标签时，可以用无监督学习作为一种特征抽取方式，最后喂给有监督的分类模型。<br>之前介绍的统计与概率模型、线性模型、基于相似度的模型，和今天要介绍的集成学习都是无监督模型。</p>
</blockquote>
<p>为什么要使用集成学习？——①适合高维度数据(空间稀疏)，②提高模型鲁棒性。<br>值得注意的时，异常检测没有标签，所以feature bagging、bagging比boosting多。而使用boosting进行异常检测，需要生成伪标签。</p>
<span id="more"></span>
<h4 id="1-Feature-Bagging"><a href="#1-Feature-Bagging" class="headerlink" title="1. Feature Bagging"></a>1. Feature Bagging</h4><p>Feature Bagging，基本思想与bagging相似，只是对象是feature。<br>基本检测器可以彼此完全不同，或不同的参数设置，或使用不同采样的子数据集。Feature bagging常用lof算法为基算法。</p>
<h5 id="1-1-算法原理"><a href="#1-1-算法原理" class="headerlink" title="1.1 算法原理"></a>1.1 算法原理</h5><p><img src="https://z3.ax1x.com/2021/05/25/gzMCDA.png" alt="x"></p>
<h5 id="1-2-不同检测器的分数标准化"><a href="#1-2-不同检测器的分数标准化" class="headerlink" title="1.2 不同检测器的分数标准化"></a>1.2 不同检测器的分数标准化</h5><p>不同检测器可能会在不同的尺度上产生分数。例如，平均k近邻检测器会输出原始距离分数，而LOF算法会输出归一化值。另外，检测器输出的异常值分数大小不一，因此需要归一化，常见方法包括平均和最大化组合函数。<br>以累积求和法和广度优先法为例，简单介绍下：<br>①累积求和：针对每组样本，分别累加对应的全部基本检测器的结果；共输出T组求和结果。<br>②广度优先：枚举每一种基本检测器(共m种)，再嵌套枚举每一组异常值输出（共T种），每种异常检测算法分别检测T组样本数据，共有m*T种组合，按照广度优先搜索，先到先占位，重复数据取后来的异常值的最大值、平均值等。</p>
<h5 id="1-3-Feature-Bagging的缺陷"><a href="#1-3-Feature-Bagging的缺陷" class="headerlink" title="1.3 Feature Bagging的缺陷"></a>1.3 Feature Bagging的缺陷</h5><p>①bagging有放回抽样，会损失部分特征的信息，或者特征始终无法被采样也无法被模型训练到。因此使用bagging实际上需要处理偏差和方差之间的一种微妙的均衡。<br>②bagging的比例通常在0.5到0.99之间，如果存在过多的相关性特征，会导致泛化能力下降。所以进行异常检测之前，必须进行严格的相关性分析或者主成分分析。</p>
<h4 id="2-Isolation-Forest"><a href="#2-Isolation-Forest" class="headerlink" title="2. Isolation Forest"></a>2. Isolation Forest</h4><p>周志华老师在2008年提出的算法，在工业界应用广泛，其不需要定义数学模型也不需要训练数据有标签。<br>这种算法在全局和局部都通过采样引入了随机性，往往要比单纯的feature bagging的效果更好。</p>
<h5 id="2-1-关于异常的定义"><a href="#2-1-关于异常的定义" class="headerlink" title="2.1 关于异常的定义"></a>2.1 关于异常的定义</h5><p>使用孤立森林(Isolation Forests)的前提是，将异常点定义为那些 <em>“容易被孤立的离群点”</em> —— 可以理解为分布稀疏，且距离高密度群体较远的点。即默认①异常数据占总样本量的比例很小，②异常点的特征值与正常点的差异很大。</p>
<h5 id="2-2-查找孤立点的策略"><a href="#2-2-查找孤立点的策略" class="headerlink" title="2.2 查找孤立点的策略"></a>2.2 查找孤立点的策略</h5><p>用一个随机超平面来切割数据空间，切一次可以生成两个子空间。继续用随机超平面来切割每个子空间并循环，直到每个子空间只有一个数据点为止。<br>以二维空间作为演示，点B’跟其他数据点比较疏离，只需要很少的几次操作就可以将它细分出来；点A’需要的切分次数会更多一些。<br><img src="https://z3.ax1x.com/2021/05/25/gzK31e.png" alt="xx"></p>
<h5 id="2-3-树的构造方法"><a href="#2-3-树的构造方法" class="headerlink" title="2.3 树的构造方法"></a>2.3 树的构造方法</h5><p><strong>训练：</strong>构建一棵 iTree 时，先从全量数据中抽取一批样本，然后随机选择一个特征作为起始节点，并在该特征的最大值和最小值之间随机选择一个值，将样本中小于该取值的数据划到左分支，大于等于该取值的划到右分支。然后，在左右两个分支数据中，重复上述步骤，直到满足如下条件：①数据不可再分，即：只包含一条数据，或者全部数据相同。②二叉树达到限定的最大深度。</p>
<p><strong>预测：</strong>计算数据 x 的异常分值时，先要估算它在每棵 iTree 中的路径深度。先沿着一棵 iTree，从根节点开始按不同特征的取值从上往下，直到到达某叶子节点。假设 iTree 的训练样本中同样落在 x 所在叶子节点的样本数为 T.size，则数据 x 在这棵 iTree 上的路径深度 h(x)，可以用下面这个公式计算：<br>$$<br>h\left( x \right) =e+C\left( T.size \right)<br>$$<br>式中，e 表示数据 x 从 iTree 的根节点到叶节点过程中经过的边的数目，C(T.size) 可以认为是一个修正值，它表示在一棵用 T.size 条样本数据构建的二叉树的平均路径长度。C(n) 的计算公式如下：<br>$$<br>C\left( n \right) =2H\left( n-1 \right) -\frac{2\left( n-1 \right)}{n}<br>$$<br>其中，H(n-1) 可用 ln(n-1)+0.5772156649(欧拉常数)估算。数据 x 最终的异常分值 Score(x) 综合了多棵 iTree 的结果：<br>$$<br>Score\left( x \right) =2^{-\frac{E\left( h\left( x \right) \right)}{C\left( \varPsi \right)}}<br>$$<br>公式中，E(h(x)) 表示数据 x 在多棵 iTree 的路径长度的均值，ψ表示单棵 iTree 的训练样本的样本数，C(ψ)表示用ψ条数据构建的二叉树的平均路径长度，它在这里主要用来做归一化。</p>
<p>从异常分值的公式看，如果数据 x 在多棵 iTree 中的平均路径长度越短，得分越接近 1，表明数据 x 越异常；如果数据 x 在多棵 iTree 中的平均路径长度越长，得分越接近 0，表示数据 x 越正常；如果数据 x 在多棵 iTree 中的平均路径长度接近整体均值，则打分会在 0.5 附近。</p>
<blockquote>
<p>在实际情况中，不同模型在不同的数据集上表现不一。但是总体而言KNN等基于位置和距离度量的算法原理不太复杂，表现也较为稳定。<br>唯一的缺点是，KNN等基于距离度量模型受到数据维度的影响较大，当维度比较低时表现很好。如果异常特征隐藏在少数维度上时，KNN和LOF类的效果就不会太好，此时该选择Isolation Forest(适合高维空间)。</p>
</blockquote>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://github.com/datawhalechina/team-learning-data-mining/tree/master/AnomalyDetection">datawhale-异常检测开源学习资料</a></p>
]]></content>
      <categories>
        <category>Anomaly Detection</category>
      </categories>
      <tags>
        <tag>Feature Bagging</tag>
        <tag>Isolation Forest</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅机器学习——反向传播</title>
    <url>/2021/07/18/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</url>
    <content><![CDATA[<h3 id="李宏毅机器学习——反向传播"><a href="#李宏毅机器学习——反向传播" class="headerlink" title="李宏毅机器学习——反向传播"></a>李宏毅机器学习——反向传播</h3><h4 id="1-反向传播原理"><a href="#1-反向传播原理" class="headerlink" title="1 反向传播原理"></a>1 反向传播原理</h4><p>神经网络中求解权重W的算法，分为信号的前向传播（Forward propagation，FP）和反向传播（Back propagation，BP）。前向传播得到预测值，计算输出误差，然后计算每个神经元节点对误差的贡献；反向传播根据前向传播的误差来求梯度，然后根据贡献调整原来的权重，最终达到最小化损失函数的目的。<br><a href="https://imgtu.com/i/W2hxoV"><img src="https://z3.ax1x.com/2021/07/25/W2hxoV.png" alt="W2hxoV.png"></a></p>
<span id="more"></span>

<p>假设只有一个隐含层，设L为损失函数。<br>在输出层上的误差（这里$O_{k}$就是输出值$y_{out}$。）：<br>$$<br>L_{total}=\frac{1}{2}(d-O)^{2}=\frac{1}{2} \sum_{k=1}^{\ell}\left(d_{k}-O_{k}\right)^{2}<br>$$</p>
<p>在隐含层上的误差（这里$net_{k}=\sum(w_i*O_{k})+b$。）：<br>$$<br>L_{total}=\frac{1}{2} \sum_{k=1}^{c}(d_{k}-f[\sum_{j=0}^{m} w_{j k} f(n e t_{j})])^{2}=\frac{1}{2} \sum_{k=1}^{c}(d_{k}-f[\sum_{j=0}^{m} w_{j k} f(\sum_{i=1}^{n} v_{i j} x_{i}))^{2}<br>$$</p>
<p>在输入层上的误差：<br>$$<br>L_{total}=\frac{1}{2} \sum_{k=1}^{c}\left(d_{k}-f\left[\sum_{j=0}^{m} w_{j k} f\left(n e t_{j}\right)\right]\right)^{2}=\frac{1}{2} \sum_{k=1}^{c}\left(d_{k}-f\left[\sum_{j=0}^{m} w_{j k} f\left(\sum_{i=1}^{n} v_{i j} x_{i}\right)\right]\right)^{2}<br>$$</p>
<p>用误差L对每一个权重$w$求偏导（链式法则），就是代表该权重$w$对总体误差产生了多少影响。<br>$$<br>w_{i}^{+}=w_{i}-\eta * \frac{\partial L_{\text {total }}}{\partial w_{i}}<br>$$<br>用同样的方法更新所有权重$w$，最后不停地迭代使误差降低。</p>
<h4 id="2-numpy实现反向传播"><a href="#2-numpy实现反向传播" class="headerlink" title="2 numpy实现反向传播"></a>2 numpy实现反向传播</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scaling</span>(<span class="params">data</span>):</span>	<span class="comment">#数据归一化</span></span><br><span class="line">    <span class="built_in">max</span> = np.<span class="built_in">max</span>(data,<span class="number">0</span>)</span><br><span class="line">    <span class="built_in">min</span> = np.<span class="built_in">min</span>(data,<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> (data-<span class="built_in">min</span>)/(<span class="built_in">max</span>-<span class="built_in">min</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">data</span>):</span>	<span class="comment">#激活函数</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-data))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wb_calc</span>(<span class="params">X,y,alpha=<span class="number">0.1</span>,maxIter=<span class="number">10000</span>,n_hidden_dim=<span class="number">3</span>,reg_lamda=<span class="number">0</span></span>):</span></span><br><span class="line">	<span class="comment">#FP</span></span><br><span class="line">	<span class="comment">#初始化</span></span><br><span class="line">    W1 = np.mat(np.random.randn(<span class="number">2</span>,n_hidden_dim))</span><br><span class="line">    b1 = np.mat(np.random.randn(<span class="number">1</span>,n_hidden_dim))</span><br><span class="line">    W2 = np.mat(np.random.randn(n_hidden_dim,<span class="number">1</span>))</span><br><span class="line">    b2 = np.mat(np.random.randn(<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> stepi <span class="keyword">in</span> <span class="built_in">range</span>(maxIter):</span><br><span class="line">        z1 = x*W1 + b1</span><br><span class="line">        a1 = sigmoid(z1)</span><br><span class="line">        z2 = a1*W2 + b2</span><br><span class="line">        a2 = sigmoid(z2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#BP</span></span><br><span class="line">    	<span class="comment">#初始化</span></span><br><span class="line">        a0 = X.copy()</span><br><span class="line">		deltal2 = a2 - ymat</span><br><span class="line">        deltal1 = np.mat((deltal2*W2.T).A * a1.A(<span class="number">1</span>-a1).A)	<span class="comment">#.A表示转化为数组</span></span><br><span class="line">    	dW1 = a0.T+deltal1 + reg_lambda*W1</span><br><span class="line">        db1 = np.<span class="built_in">sum</span>(deltal1,<span class="number">0</span>)</span><br><span class="line">        dW2 = a1.T+deltal2 + reg_lambda*W2</span><br><span class="line">        db2 = np.<span class="built_in">sum</span>(deltal2,<span class="number">0</span>)</span><br><span class="line">        <span class="comment">#更新w,b</span></span><br><span class="line">        W1 = W1 - alpha*dW1</span><br><span class="line">        b1 = b1 - alpha*db1</span><br><span class="line">        W2 = W2 - alpha*dW2</span><br><span class="line">        b2 = b2 - alpha*db2</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> W1,b1,W2,b2</span><br></pre></td></tr></table></figure>


<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://www.bilibili.com/video/av59538266">b站李宏毅《机器学习》</a><br>2.<a href="https://datawhalechina.github.io/leeml-notes/#/">datawhale李宏毅机器学习笔记(LeeML-Notes)</a></p>
]]></content>
      <categories>
        <category>Neural Networks</category>
      </categories>
      <tags>
        <tag>Back Propagation</tag>
      </tags>
  </entry>
  <entry>
    <title>异常检测(Anomaly Detection)简介</title>
    <url>/2021/05/12/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<p>异常检测顾名思义就是检测出与正常数据不同的数据，检测给定的新数据是否属于这组数据集。</p>
<h4 id="1-异常检测的任务类型"><a href="#1-异常检测的任务类型" class="headerlink" title="1. 异常检测的任务类型"></a>1. 异常检测的任务类型</h4><p><strong>无监督</strong>：训练集无标签<br><strong>有监督</strong>：训练集的正例和反例均有标签<br><strong>半监督</strong>：在训练集中只有正例，异常实例不参与训练</p>
<span id="more"></span>
<h4 id="2-异常检测的难点"><a href="#2-异常检测的难点" class="headerlink" title="2. 异常检测的难点"></a>2. 异常检测的难点</h4><p><strong>数据量少</strong>：异常检测任务通常情况下负样本（abnormal）较少，0、1两类样本严重不均衡，有时候依赖于人工标签。<br><strong>噪音</strong>：异常和噪音有时容易混淆，如两幅图中的A点识别难度就不一样。<br><img src="https://z3.ax1x.com/2021/05/25/gzuSPO.png" alt="xx"></p>
<h4 id="3-异常检测方法简介"><a href="#3-异常检测方法简介" class="headerlink" title="3. 异常检测方法简介"></a>3. 异常检测方法简介</h4><h5 id="3-1-基于统计学的方法"><a href="#3-1-基于统计学的方法" class="headerlink" title="3.1 基于统计学的方法"></a>3.1 基于统计学的方法</h5><p>通常做法是假设样本服从高斯分布，计算p(x)=ϵ作为判定正常和异常的阈值。<br><img src="https://z3.ax1x.com/2021/05/25/gzu9Re.png" alt="xx"></p>
<h5 id="3-2-线性模型"><a href="#3-2-线性模型" class="headerlink" title="3.2 线性模型"></a>3.2 线性模型</h5><p>在线性模型中，有2个假设：<br>①不同维度的变量近似线性相关，可以通过一个相关系数矩阵进行衡量。<br>②数据是镶嵌在低维子空间中的，线性建模是为了找到某个低维子空间使异常点区别于正常点。<br>常用模型的包括：<br><strong>最小二乘法</strong>——最小化目标函数得到直线方程,计算每个变量与回归方程之间的残差,可通过3σ 法则判断异常值.<br><strong>PCA</strong>——将样本的协方差矩阵特征值分解后，特征值是样本投影到轴上对应的方差，特征值越小，投影后样本点分布越集中，异常点容易偏离，以此作为判定异常值的一个指标。</p>
<h5 id="3-3-基于邻近度的方法"><a href="#3-3-基于邻近度的方法" class="headerlink" title="3.3 基于邻近度的方法"></a>3.3 基于邻近度的方法</h5><p>这类算法适用于数据点的聚集程度高、离群点较少的情况，不适用于数据量大、维度高的数据。<br>①基于集群（簇）的检测——DBSCAN，核心点距离ℇ内最小包含点数、距离ℇ与阈值比较。<br>②基于距离的度量——KNN，将K-近邻距离与阈值比较。<br>③基于密度的度量——LOF(局部离群因子)，邻域点密度与阈值比较。<br><img src="https://z3.ax1x.com/2021/05/25/gzupGD.png" alt="xx"></p>
<h5 id="3-4-集成方法"><a href="#3-4-集成方法" class="headerlink" title="3.4 集成方法"></a>3.4 集成方法</h5><p>①<strong>孤立森林（Isolation Forest）</strong>——不停地使用随机超平面分隔每个子空间，直到每个子空间只有一个数据点为止，低密度的点被单独分配到一个子空间，孤立数低于阈值时，定义为异常值。<br><img src="https://z3.ax1x.com/2021/05/25/gznxIK.png" alt="xx"><br>②树模型——通过树不断划分子空间,数据点在多个树上的平均深度越浅,越可能为异常值。</p>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://github.com/datawhalechina/team-learning-data-mining/tree/master/AnomalyDetection">datawhale-异常检测开源学习资料</a></p>
]]></content>
      <categories>
        <category>Anomaly Detection</category>
      </categories>
      <tags>
        <tag>LOF</tag>
        <tag>OLSE</tag>
        <tag>Isolation Forest</tag>
        <tag>PCV</tag>
        <tag>KNN</tag>
        <tag>DBSCAN</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅机器学习——机器学习介绍</title>
    <url>/2021/07/12/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<h3 id="李宏毅机器学习——机器学习介绍"><a href="#李宏毅机器学习——机器学习介绍" class="headerlink" title="李宏毅机器学习——机器学习介绍"></a>李宏毅机器学习——机器学习介绍</h3><h4 id="1-机器学习简介"><a href="#1-机器学习简介" class="headerlink" title="1. 机器学习简介"></a>1. 机器学习简介</h4><p>机器学习是人工智能的一部分, 研究如何让计算机从数据学习某种规律。机器学习的目的是通过计算机程序根据数据去优化某一个评价指标，自动的从数据发现规律, 使用这些规律做出预测。<br><a href="https://imgtu.com/i/W2huKs"><img src="https://z3.ax1x.com/2021/07/25/W2huKs.png" alt="W2huKs.png"></a><br>如图，机器学习可以简化为三个步骤，一、找一个function，二、让machine评价这个function，三、让machine自动地挑出最好的function。</p>
<h4 id="2-机器学习分类"><a href="#2-机器学习分类" class="headerlink" title="2. 机器学习分类"></a>2. 机器学习分类</h4><p><a href="https://imgtu.com/i/W2hlV0"><img src="https://z3.ax1x.com/2021/07/25/W2hlV0.png" alt="W2hlV0.png"></a><br>机器学习可大致分为监督学习、无监督学习、半监督学习、迁移学习和强化学习。</p>
<h5 id="2-1-监督学习"><a href="#2-1-监督学习" class="headerlink" title="2.1 监督学习"></a>2.1 监督学习</h5><p>监督学习是机器学习任务的一种。它从有标签的训练数据中推导出预测函数。有标签的训练数据是指每个训练实例都包括输入和期望的输出。简单来说就是给定数据，预测标签。<br>监督学习包括<code>分类</code>和<code>回归</code>。</p>
<span id="more"></span>

<h5 id="2-2-无监督学习"><a href="#2-2-无监督学习" class="headerlink" title="2.2 无监督学习"></a>2.2 无监督学习</h5><p>无监督学习是机器学习任务的一种。它从无标签的训练数据中推断结论。它可以在探索性数据分析阶段用于发现隐藏的模式或者对数据进行分组。简单来说就是给定数据，寻找隐藏的结构。<br>无监督学习包括<code>聚类</code>和<code>降维</code>。</p>
<h5 id="2-3-半监督学习"><a href="#2-3-半监督学习" class="headerlink" title="2.3 半监督学习"></a>2.3 半监督学习</h5><p>半监督学习介于监督学习与无监督学习之间。半监督学习的任务与监督学习一致，任务中包含有明确的目标，如分类。但其所采用的数据既包括有标签的数据，也包括无标签的数据。简单来说，半监督学习是同时运用了标签数据和无标签数据来进行训练的监督学习。</p>
<h5 id="2-5-迁移学习"><a href="#2-5-迁移学习" class="headerlink" title="2.5 迁移学习"></a>2.5 迁移学习</h5><p>迁移学习是把已训练好的或者模型（预训练模型）参数或者标注数据迁移到新的模型来帮助新模型训练。其中需要关注的是源领域与目标领域之间相关性。</p>
<h5 id="2-6-强化学习"><a href="#2-6-强化学习" class="headerlink" title="2.6 强化学习"></a>2.6 强化学习</h5><p>强化学习是机器学习的另一个领域。它关注的是软件代理如何在一个环境中采取行动以便最大化某种累积的回报。简单来说就是给定数据，学习如何选择一系列行动，以最大化长期收益。<br>强化学习包括<code>马尔可夫决策过程</code>和<code>动态规划</code>。</p>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://www.bilibili.com/video/av59538266">b站李宏毅《机器学习》</a></p>
]]></content>
      <categories>
        <category>Neural Networks</category>
      </categories>
  </entry>
  <entry>
    <title>李宏毅机器学习——回归</title>
    <url>/2021/07/14/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h3 id="李宏毅机器学习——回归"><a href="#李宏毅机器学习——回归" class="headerlink" title="李宏毅机器学习——回归"></a>李宏毅机器学习——回归</h3><p>回归大致可以理解为根据数据集$D$，拟合出近似的曲线，所以回归也常称为拟合（Fit）。按照自变量的数量可以分为一元回归和多元回归，按照自变量与因变量之间的函数表达式可以分为线性回归(Linear Regression)和非线性回归(Non-linear Regression)。</p>
<h4 id="1-模型步骤"><a href="#1-模型步骤" class="headerlink" title="1 模型步骤"></a>1 模型步骤</h4><ul>
<li>Step1：模型假设，选择模型框架（线性模型）</li>
<li>Step2：模型评估，如何判断众多模型的好坏（损失函数）</li>
<li>Step3：模型优化，如何筛选最优的模型（梯度下降）</li>
</ul>
<h5 id="1-1-模型假设（多元线性模型）"><a href="#1-1-模型假设（多元线性模型）" class="headerlink" title="1.1 模型假设（多元线性模型）"></a>1.1 模型假设（多元线性模型）</h5><p>多元线性模型就是特征x的线性组合的函数，可以表示为$y=\sum{w_ix_i}+b$，$x_i$为特征，$w_i$为权重，b为偏移量。</p>
<span id="more"></span>

<h5 id="1-2-模型评估（损失函数）"><a href="#1-2-模型评估（损失函数）" class="headerlink" title="1.2 模型评估（损失函数）"></a>1.2 模型评估（损失函数）</h5><p>常用的损失函数是均方误差（mean-square error, MSE），也被称为最小二乘法 (least square method)，其几何意义就是欧氏距离。</p>
<blockquote>
<p>不同的loss函数，具有不同的拟合特性：<br>Hinge Loss——SVM；<br>exp-Loss ——Boosting；<br>log-Loss——Logistic Regression。</p>
</blockquote>
<p>Square loss损失函数表示为：<br>$$<br>L\left( w,b \right) =\sum_{i=1}^m{\left( f\left( x_i \right) -y_i \right) ^2}，<br>$$<br>我们需要找到令结果最小的$w^*$和$b^*$，即<br>$$<br>\left( w^*,b^* \right) =arg\min <em>{\left( w,b \right)}\sum</em>{i=1}^m{\left( f\left( x_i \right) -y_i \right) ^2}。<br>$$</p>
<h5 id="1-3-最佳模型（解析解）"><a href="#1-3-最佳模型（解析解）" class="headerlink" title="1.3 最佳模型（解析解）"></a>1.3 最佳模型（解析解）</h5><p>多元（多个特征）的情况下，损失函数可以引入矩阵表示为：<br>$$<br>L\left( w,b \right) =\sum_{i=1}^m{\left( f\left( x_i \right) -y_i \right) ^2}=\left( XW-y \right) ^T\left( XW-y \right)<br>$$<br>式中，$X$是一组样本形成的样本矩阵，$W$是权重矩阵，$y$是样本真实值形成的矩阵。<br>上式可以展开为：<br>$$<br>L\left( w,b \right) =\left( XW-y \right) ^T\left( XW+y \right)<br>$$<br>$$<br>=\left( \left( XW \right) ^T-y^T \right) \left( XW-y \right)<br>$$<br>$$<br>=\left( X^TW^T-y^T \right) \left( XW-y \right)<br>$$<br>$$<br>=X^TW^TXW-y^TXW-W^TX^Ty+y^Ty<br>$$<br>对$w$求导：<br>$$<br>\frac{\partial L\left( W \right)}{\partial W}=2X^TXW-\left( y^TX \right) ^T-X^Ty<br>$$<br>$$<br>=2X^TXW-2X^Ty<br>$$<br>偏导等于零时可以求得损失函数最小时对应的$W$，即我们最终想要获得的系数矩阵：<br>$$<br>X^TXW-X^Ty=0<br>$$<br>$$<br>X^TXW=X^Ty<br>$$<br>$$<br>((X^TX)^{-1}X^TX)W=(X^TX)^{-1}X^Ty<br>$$<br>$$<br>IW=(X^TX)^{-1}X^Ty<br>$$<br>$$<br>W=(X^TX)^{-1}X^Ty<br>$$</p>
<p>上式必须满足矩阵的逆存在，而矩阵可逆需要方正且满秩。但是因为不能保证所选的特征是完全线性无关的，所以矩阵不一定满秩，这导致线性回归有解析解但存在无穷多解，还是无法求得一个准确的解，这时候就需要使用梯度下降法来逼近一个解。</p>
<h5 id="1-4-最佳模型（梯度下降法）"><a href="#1-4-最佳模型（梯度下降法）" class="headerlink" title="1.4 最佳模型（梯度下降法）"></a>1.4 最佳模型（梯度下降法）</h5><p>首先设置初始的$w_0$和$b_0$，然后通过求$L$对$w$和$b$的偏导不断地更新$w$和$b$，直到找到$L$的最小值。<br>整个过程可以表示为：<br>$$<br>repeat,,until,,converge{w_i:=w_i-\eta \frac{\partial L\left( w_i \right)}{w_i};\ b_i:=b_i-\eta \frac{\partial L\left( b_i \right)}{b_i}}<br>$$<br>其中$\eta $是学习率，用来控制每步下降的距离（太小收敛会很慢，太大则可能跳过最优点），一般按照对数的方法来选择，例如0.1, 0.03, 0.01, 0.003…这种方法称为批量梯度下降（batch gradient descent, BGD）</p>
<p>original-loss存在很多奇点，而BGD每次下降的方向就是original-loss的负梯度，因此BGD很容易陷入某个奇点，而无法达到global minimal（或者比较好的local minimal）。随机梯度下降（stochastic gradient descent, SGD）一定程度上可以避免这个情况——不容易陷入original-loss和minibatch-loss的奇点：</p>
<ul>
<li>引入randomness，SGD中计算的梯度是对original-loss梯度的近似，相当于在original-loss梯度的基础上加了randomness，因此即使当前走到了original-loss的奇点，SGD计算的梯度因为引入了randomness，所以也不接近0，比较容易跳出奇点。</li>
<li>SGD计算的不是original-loss的梯度，而是minibatch-loss的梯度，original-loss和minibatch-loss的形状不同，奇点分布也不同，如果当前这个点在original-loss上是奇点，但这个点在minibatch-loss中并不是奇点，此时使用minibatch-loss的负梯度作为下降方向，自然不会陷入这个点。</li>
<li>每次迭代，都会使用不同的mini batch，而不同的minibatch-loss的形状不同。就算此时陷入了当前minibatch-loss的奇点，那么下一次迭代，这个点也不一定就是下一个minibatch-loss的奇点，如果不是的话，自然就跳出来了。</li>
</ul>
<h5 id="1-5-正则化"><a href="#1-5-正则化" class="headerlink" title="1.5 正则化"></a>1.5 正则化</h5><p>当我们筛选出模型进行拟合数据时，可能会没有很好地拟合数据，称为欠拟合（underfitting），或者叫做高偏差（bias）；也可能对训练集拟合得很好，但在测试集上拟合不好，称为过拟合（overfitting），也叫高方差（variance）。</p>
<p>对于过拟合，我们可以</p>
<ul>
<li>1.减少选取变量的数量（通过特征选择保留更为重要的特征变量）</li>
</ul>
<p>特征选择的缺点在于：舍弃一部分特征变量的同时，也舍弃了问题中的一些信息。<br>如果我们不想舍弃这些信息，可以使用正则化。</p>
<ul>
<li>2.正则化</li>
</ul>
<p>当线性回归从1次变成高次的过程中，在训练集上的拟合越来越好，但在测试集上并不是这样。于是我们可以采用正则化，去惩罚$w_1$到$w_n$，以L2正则化为例，原始的损失函数$L$上加上$\lambda \sum{\left( w_i \right) ^2}$，使得权重的绝对值大小整体倾向于减小（权重衰减），这样可以使函数更加平滑，也就是更加简单，不容易发生过拟合。形象一点看，损失函数曲面通过L2正则化的加持，最小值所在的位置从一条山岭变成一个山谷，故L2正则化在机器学习中也叫做“岭回归”（ridge regression）。<br><a href="https://imgtu.com/i/W2hcxH"><img src="https://z3.ax1x.com/2021/07/25/W2hcxH.png" alt="W2hcxH.png"></a></p>
<p>L1正则化则是加上$\lambda \sum{\left| \left. w_i \right| \right.}$，L1 正则化除了和L2正则化一样可以约束数量级外，还能使参数更加稀疏，参数一部分为0，另一部分为非零实值。非零实值的那部分参数值可起到选择重要参数或特征维度的作用，同时可起到去除噪声的效果。</p>
<h4 id="2-sklearn实现多元线性回归"><a href="#2-sklearn实现多元线性回归" class="headerlink" title="2 sklearn实现多元线性回归"></a>2 sklearn实现多元线性回归</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> pltfrom sklearn.linear_model</span><br><span class="line"><span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;./data.csv&#x27;</span>)</span><br><span class="line">X_train,X_test,Y_train,Y_test = train_test_split(train_size = <span class="number">0.7</span>)</span><br><span class="line">model = LinearRegression()model.fit(X_train,Y_train)</span><br><span class="line"></span><br><span class="line">a = model.intercept_</span><br><span class="line">b = model.coef_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最佳拟合线:截距&quot;</span>,a,<span class="string">&quot;,回归系数：&quot;</span>,b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;拟合函数: y = %f * x+%f * x+%f * x+ %f&#x27;</span>%(b[<span class="number">0</span>],b[<span class="number">1</span>],b[<span class="number">2</span>],a))</span><br><span class="line"></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = <span class="string">&#x27;SimHei&#x27;</span></span><br><span class="line">score = model.score(X_test,Y_test)</span><br><span class="line"><span class="built_in">print</span>(score)</span><br><span class="line">Y_test_pred = model.predict(X_test)</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="built_in">len</span>(Y_test_pred))</span><br><span class="line">Y_test,label = <span class="string">&#x27;test&#x27;</span>,color = <span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="built_in">len</span>(Y_test_pred)),Y_test_pred,color = <span class="string">&#x27;r&#x27;</span>,label = <span class="string">&#x27;predict&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;X&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>


<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://www.bilibili.com/video/av59538266">b站李宏毅《机器学习》</a><br>2.<a href="https://datawhalechina.github.io/leeml-notes/#/">datawhale李宏毅机器学习笔记(LeeML-Notes)</a></p>
]]></content>
      <categories>
        <category>Neural Networks</category>
      </categories>
      <tags>
        <tag>Regression</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅机器学习——梯度下降</title>
    <url>/2021/07/16/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</url>
    <content><![CDATA[<h3 id="李宏毅机器学习——梯度下降"><a href="#李宏毅机器学习——梯度下降" class="headerlink" title="李宏毅机器学习——梯度下降"></a>李宏毅机器学习——梯度下降</h3><h4 id="1-偏差与方差"><a href="#1-偏差与方差" class="headerlink" title="1 偏差与方差"></a>1 偏差与方差</h4><p>使用样本数据拟合函数，由于样本数据使采样而并非真实值本身，采样本身会带来误差。经过研究发现，其误差的期望值可以分解为三个部分：样本<strong>噪音</strong>、模型预测值的<strong>方差</strong>、预测值相对真实值的<strong>偏差</strong>，公式为：<br>$$<br>E\left((y-\hat{f}(x))^{2}\right)=\sigma^{2}+\operatorname{Var}[\hat{f}(x)]+(\operatorname{Bias}[\hat{f}(x)])^{2}<br>$$<br>其中$Bias[\hat{f}(x)] = E[\hat{f}(x) - f(x)]$，即误差的期望值 = 噪音的方差 + 模型预测值的方差 + 预测值相对真实值的偏差的平方，如下图所示。</p>
<p><a href="https://imgtu.com/i/W2h4dP"><img src="https://z3.ax1x.com/2021/07/25/W2h4dP.png" alt="W2h4dP.png"></a></p>
<span id="more"></span>
<p>靶心（红点）是测试样本的真实值，测试样本的y（橙色点）是真实值加上噪音，特定模型重复多次训练会得到多个具体的模型，每一个具体模型对测试样本进行一次预测，就在靶上打出一个预测值（图上蓝色的点）。所有预测值的平均就是预测值的期望（较大的浅蓝色点），浅蓝色的圆圈表示预测值的离散程度，即预测值的方差。</p>
<h4 id="2-调整梯度下降的学习率"><a href="#2-调整梯度下降的学习率" class="headerlink" title="2 调整梯度下降的学习率"></a>2 调整梯度下降的学习率</h4><p>如果每次调的太大，loss变化就很快：调的太小，loss变化的太慢，这样都找不到最小的loss。<br><a href="https://imgtu.com/i/W2h5If"><img src="https://z3.ax1x.com/2021/07/25/W2h5If.png" alt="W2h5If.png"></a><br>此时我们可以采用自适应学习率，即刚开始时，初始点距离最低点较远，采用较大的学习率，在每次更新参数后，更接近最低点了，调小学习率。于是就有了不同的优化梯度下降的方法，这类优化梯度下降的方法有SGD、AdaGrad、Adam等等。</p>
<h5 id="2-1-SGD"><a href="#2-1-SGD" class="headerlink" title="2.1 SGD"></a>2.1 SGD</h5><p>批量梯度下降（Batch Gradient Descent，BGD）是在全部训练集上计算准确的梯度，即<br>$$<br>\sum_{i=1}^{n} \nabla_{\theta} f\left(\theta ; x_{i}, y_{i}\right)+\nabla_{\theta} \phi(\theta)<br>$$<br>而随机梯度下降（Stochastic Gradient Descent，SGD）则是采样单个样本来估计当前的梯度，即<br>$$<br>\nabla_{\theta} f\left(\theta ; x_{i}, y_{i}\right)+\nabla_{\theta} \phi(\theta)<br>$$<br>SGD最大的缺点是下降速度慢和不稳定，而且可能会在沟壑的两边持续震荡，停留在一个局部最优点。</p>
<h5 id="2-2-SGD-with-Momentum"><a href="#2-2-SGD-with-Momentum" class="headerlink" title="2.2 SGD with Momentum"></a>2.2 SGD with Momentum</h5><p>为了抑制SGD的震荡，SGDM认为梯度下降过程可以加入惯性。下坡的时候，如果发现是陡坡，那就利用惯性跑的快一些。SGDM在SGD基础上引入了一阶动量：<br>$$<br>m_{t}=\beta_{1} \cdot m_{t-1}+\left(1-\beta_{1}\right) \cdot g_{t}<br>$$<br>一阶动量是各个时刻梯度方向的指数移动平均值，约等于最近$1 /\left(1-\beta_{1}\right)$个时刻的梯度向量和的平均值。也就是说，t时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定。这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向。</p>
<h5 id="2-3-SGD-with-Nesterov-Acceleration"><a href="#2-3-SGD-with-Nesterov-Acceleration" class="headerlink" title="2.3 SGD with Nesterov Acceleration"></a>2.3 SGD with Nesterov Acceleration</h5><p>SGD 还有一个问题是困在局部最优的沟壑里面震荡。因此，我们不能停留在当前位置去观察未来的方向，而要向前一步、多看一步、看远一些。<br>NAG全称Nesterov Accelerated Gradient，是在SGD、SGD-M的基础上的进一步改进。NAG不计算当前位置的梯度方向，而是计算如果按照累积动量走了一步，那个时候的下降方向：<br>$$<br>g_{t}=\nabla f\left(w_{t}-\alpha \cdot m_{t-1} / \sqrt{V_{t-1}}\right)<br>$$</p>
<h5 id="2-5-Adgrad"><a href="#2-5-Adgrad" class="headerlink" title="2.5 Adgrad"></a>2.5 Adgrad</h5><p>Adagrad是解决不同参数应该使用不同的更新速率的问题。Adagrad自适应地为各个参数分配不同学习率的算法。<br>其公式如下：<br><a href="https://imgtu.com/i/W2hTJS"><img src="https://z3.ax1x.com/2021/07/25/W2hTJS.png" alt="W2hTJS.png"></a><br>Adagrad设置全局学习率之后，每次通过全局学习率逐参数的除以历史梯度平方和的平方根，使得每个参数的学习率不同。这里的梯度平方和就是二阶动量。</p>
<p>Adagrad起到的效果是在参数空间更为平缓的方向会取得更大的进步（因为平缓，所以历史梯度平方和较小，对应学习下降的幅度较小），并且能够使得陡峭的方向变得平缓，从而加快训练速度。</p>
<p>Adagrad的缺点是由于是累积平方梯度，平方梯度是单调递增函数，会使得学习率单调递减至0，从而使训练过程提前结束。</p>
<h5 id="2-6-AdaDelta-RMSProp"><a href="#2-6-AdaDelta-RMSProp" class="headerlink" title="2.6 AdaDelta / RMSProp"></a>2.6 AdaDelta / RMSProp</h5><p>由于AdaGrad单调递减的学习率变化过于激进，可以改进二阶动量计算方法：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。（这也是AdaDelta名称中Delta的来历。）</p>
<p>指数移动平均值大约就是过去一段时间的平均值，因此可以用这一方法来计算二阶累积动量：<br>$$<br>V_{t}=\beta_{2} * V_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2}<br>$$<br>这可以避免二阶动量持续累积、导致训练过程提前结束的问题。</p>
<h5 id="2-7-Adam"><a href="#2-7-Adam" class="headerlink" title="2.7 Adam"></a>2.7 Adam</h5><p>SGD-M在SGD基础上增加了一阶动量，AdaGrad和AdaDelta在SGD基础上增加了二阶动量。把一阶动量和二阶动量都用起来，就是Adam了——Adaptive + Momentum。<br>SGD的一阶动量：<br>$$<br>m_{t}=\beta_{1} \cdot m_{t-1}+\left(1-\beta_{1}\right) \cdot g_{t}<br>$$<br>加上AdaDelta的二阶动量：<br>$$<br>V_{t}=\beta_{2} * V_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2}<br>$$<br>其中$\beta_{1}$控制一阶动量，$\beta_{2}$控制二阶动量。</p>
<h5 id="2-8-Nadam"><a href="#2-8-Nadam" class="headerlink" title="2.8 Nadam"></a>2.8 Nadam</h5><p>在Adam的基础上增加了Nesterov，即同时用上一二阶动量，且按照累积动量走了一步。<br>$$<br>g_{t}=\nabla f\left(w_{t}-\alpha \cdot m_{t-1} / \sqrt{V_{t}}\right)<br>$$</p>
<h4 id="3-特征缩放（Feature-Scaling）"><a href="#3-特征缩放（Feature-Scaling）" class="headerlink" title="3 特征缩放（Feature Scaling）"></a>3 特征缩放（Feature Scaling）</h4><p>特征缩放（Feature Scaling）包括标准化（Standardization/Z-Score Normalization）和归一化（Normalization）。特征缩放可以同一量纲和数量级，可以提高某些基于距离的算法（Distance-Based Algorithms），如KNN、K-Means、SVM、PCA等距离计算算法的性能，但对基于树的算法（Tree-Based Algorithms）、线性判别分析、朴素贝叶斯等算法无帮助。</p>
<p>特征缩放对于基于梯度下降的算法（Gradient Descent Based Algorithms）有加速作用——未经标准化的特征的损失函数是椭圆形，梯度下降方向垂直于等高线，形成zigzag的路线，并非指向local mininum。对特征进行归一化Normalization后，其损失函数的等高线图更接近圆形，梯度下降的方向震荡更小，收敛更快。<br><a href="https://imgtu.com/i/W2hHzQ"><img src="https://z3.ax1x.com/2021/07/25/W2hHzQ.png" alt="W2hHzQ.png"></a></p>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://www.bilibili.com/video/av59538266">b站李宏毅《机器学习》</a><br>2.<a href="https://datawhalechina.github.io/leeml-notes/#/">datawhale李宏毅机器学习笔记(LeeML-Notes)</a></p>
]]></content>
      <categories>
        <category>Neural Networks</category>
      </categories>
      <tags>
        <tag>Gradient Descent</tag>
      </tags>
  </entry>
  <entry>
    <title>李宏毅机器学习——神经网络设计技巧</title>
    <url>/2021/07/20/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%BE%E8%AE%A1%E6%8A%80%E5%B7%A7/</url>
    <content><![CDATA[<h3 id="李宏毅机器学习——神经网络设计技巧"><a href="#李宏毅机器学习——神经网络设计技巧" class="headerlink" title="李宏毅机器学习——神经网络设计技巧"></a>李宏毅机器学习——神经网络设计技巧</h3><h4 id="1-鞍点-Saddle-Point"><a href="#1-鞍点-Saddle-Point" class="headerlink" title="1 鞍点(Saddle Point)"></a>1 鞍点(<em>Saddle</em> <em>Point</em>)</h4><p>神经网络中，Gradient为0(或不存在)的点称为临界点(critical point)，其有两种可能性：局部极小(local minima)和鞍点(saddle point)。</p>
<p>长期以来，人们普遍认为，神经网络优化问题困难是因为较大的神经网络中包含很多局部极小值，使得算法容易陷入到其中某些点。2014年的一篇论文中提出高维非凸优化问题之所以困难，是因为存在大量的鞍点而不是局部极值。</p>
<blockquote>
<p>鞍点：</p>
<ul>
<li>一个维度向上倾斜且另一维度向下倾斜的点。这些鞍点通常被相同误差值的平面所包围，这使得算法陷入其中很难脱离出来，因为梯度在所有维度上接近于零</li>
<li>梯度等于零，在其附近Hessian矩阵有正的和负的特征值，行列式小于0，即是不定的。</li>
</ul>
</blockquote>
<p><a href="https://imgtu.com/i/W24pJU"><img src="https://z3.ax1x.com/2021/07/25/W24pJU.png" alt="W24pJU.png"></a></p>
<span id="more"></span>

<p>在鞍点附近，基于梯度的优化算法会遇到较为严重的问题：鞍点处的梯度为零，鞍点通常被相同误差值的平面所包围（这个平面称为Plateaus，Plateaus是梯度接近于零的平缓区域，会降低神经网络学习速度），在高维的情形，这个鞍点附近的平坦区域范围可能非常大，这使得SGD算法很难脱离区域，即可能会长时间卡在该点附近（因为梯度在所有维度上接近于零）。</p>
<p>在鞍点数目极大的时候，这个问题会变得非常严重。高维非凸优化问题之所以困难，是因为高维参数空间存在大量的鞍点。</p>
<p>鞍点和局部极小值相同的是，在该点处的梯度都等于零，不同在于在鞍点附近Hessian矩阵是不定的(行列式小于0)，而在局部极值附近的Hessian矩阵是正定的。</p>
<blockquote>
<p>Hessian矩阵是一个多元函数的二阶偏导数构成的方阵，描述了函数的局部曲率。可用于判定多元函数的极值。</p>
</blockquote>
<blockquote>
<p>$$<br>H(f)=\left[\begin{array}{cccc}<br>\frac{\partial^{2} f}{\partial x_{1}^{2}} &amp; \frac{\partial^{2} f}{\partial x_{1} \partial x_{2}} &amp; \cdots &amp; \frac{\partial^{2} f}{\partial x_{1} \partial x_{n}} \<br>\frac{\partial^{2} f}{\partial x_{2} \partial x_{1}} &amp; \frac{\partial^{2} f}{\partial x_{2}^{2}} &amp; \cdots &amp; \frac{\partial^{2} f}{\partial x_{2} \partial x_{n}} \<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>\frac{\partial^{2} f}{\partial x_{n} \partial x_{1}} &amp; \frac{\partial^{2} f}{\partial x_{n} \partial x_{2}} &amp; \cdots &amp; \frac{\partial^{2} f}{\partial x_{n}^{2}}<br>\end{array}\right]<br>$$</p>
</blockquote>
<ul>
<li>当函数的Hessian矩阵在梯度为零的位置上的特征值全为正(正定矩阵)时，该函数得到局部最小值。</li>
<li>当函数的Hessian矩阵在梯度为零的位置上的特征值全为负(负定矩阵)时，该函数得到局部最⼤值。</li>
<li>当函数的Hessian矩阵在梯度为零的位置上的特征值有正有负(不定矩阵)时，该函数得到鞍点。</li>
</ul>
<h4 id="2-批次和动量-Batch-and-Momentum"><a href="#2-批次和动量-Batch-and-Momentum" class="headerlink" title="2 批次和动量(Batch and Momentum)"></a>2 批次和动量(Batch and Momentum)</h4><h5 id="2-1-Batch"><a href="#2-1-Batch" class="headerlink" title="2.1 Batch"></a>2.1 Batch</h5><p>批梯度下降（Batch gradient descent，BGD）遍历全部数据集算一次损失函数，然后算函数对各个参数的梯度，然后更新梯度。BGD的优点在于由全数据集确定的方向能够更好地代表样本总体，如果函数为凸函数，BGD一定能得到全局最优。其缺点在于每更新一次参数都要把数据集里的所有样本都看一遍，计算量开销大，计算速度慢。</p>
<p>随机梯度下降（stochastic gradient descent，SGD）每看一个数据就算一下损失函数，然后求梯度更新参数，这个称为随机梯度下降。SGD方法速度比较快，但是由于单个样本并不能代表全体样本的趋势，所以其收敛性能不太好，遇上噪声则容易陷入局部最优解，也可能在最优点附近晃来晃去，收敛不到最优点。两次参数的更新也有可能互相抵消掉，造成目标函数震荡的比较剧烈。</p>
<p>小批量梯度下降（mini-batch gradient descent，mini-batch GD）每次选取一定数目(mini-batch)的样本组成一个小批量样本，然后用这个小批量来更新梯度。每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果，可以提高算法稳定性。</p>
<h5 id="2-2-Momentum"><a href="#2-2-Momentum" class="headerlink" title="2.2 Momentum"></a>2.2 Momentum</h5><p>动量梯度下降法（Gradient descent with Momentum）或者叫做Momentum，通过计算梯度的指数加权平均数，并利用该梯度更新权重。<br><a href="https://imgtu.com/i/W2TLVA"><img src="https://z3.ax1x.com/2021/07/25/W2TLVA.png" alt="W2TLVA.png"></a></p>
<p>其每一次梯度下降都会有一个之前的速度的作用，如果这次的方向与之前相同，则会因为之前的速度继续加速；如果这次的方向与之前相反，则会由于之前存在速度的作用不会产生一个急转弯。简而言之，Momentum对原始梯度做了修正和平滑，具有更快的收敛速度。</p>
<h4 id="3-自动调整学习速率-Adaptive-Learning-rate"><a href="#3-自动调整学习速率-Adaptive-Learning-rate" class="headerlink" title="3 自动调整学习速率(Adaptive Learning rate)"></a>3 自动调整学习速率(Adaptive Learning rate)</h4><h5 id="2-1-AdaGrad"><a href="#2-1-AdaGrad" class="headerlink" title="2.1 AdaGrad"></a>2.1 AdaGrad</h5><p>设置全局学习率之后，每次通过全局学习率逐参数的除以历史梯度平方和的平方根，使得每个参数的学习率不同，可以使参数空间更为平缓的方向，会取得更大的进步（因为平缓，所以历史梯度平方和较小，对应学习下降的幅度较小），并且能够使得陡峭的方向变得平缓，从而加快训练速度。<br><a href="https://imgtu.com/i/W27CrQ"><img src="https://z3.ax1x.com/2021/07/25/W27CrQ.png" alt="W27CrQ.png"></a></p>
<h5 id="2-2-RMSprop"><a href="#2-2-RMSprop" class="headerlink" title="2.2 RMSprop"></a>2.2 RMSprop</h5><p>Adagrad会记录过去所有梯度的平方和。因此，学习越深入，更新的幅度就越小。实际上，如果无止境地学习，更新量就会变为0，完全不再更新。为了改善这个问题，可以使用 RMSProp方法。RMSProp方法并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来。这种方法也被称为“指数移动平均”，呈指数函数式地减小过去的梯度的尺度。</p>
<p>RMSProp算法不是像AdaGrad算法那样暴力直接的累加平方梯度，而是加了一个衰减系数来控制历史信息的获取多少。相比于AdaGrad的历史梯度$r \leftarrow r+g \odot g$，RMSProp增加了一个衰减系数来控制历史信息的获取多少：$r \leftarrow \rho r+(1-\rho) \boldsymbol{g} \odot \boldsymbol{g}$。</p>
<p><a href="https://imgtu.com/i/W27FVs"><img src="https://z3.ax1x.com/2021/07/25/W27FVs.png" alt="W27FVs.png"></a></p>
<p>第一个等式类似Momentum，计算了梯度平方的指数平均；第二个等式根据指数平均决定步幅大小，初始化学习率η，接着除以平均数；第三个等式是权重更新步骤。其中超参数ρ一般取0.9，ε一般取1e-10。</p>
<p>RMSProp隐式地应用了模拟退火。在向最小值移动的过程中，RMSProp会自动降低学习步幅，以免跳过最小值。</p>
<h5 id="2-3-Adam"><a href="#2-3-Adam" class="headerlink" title="2.3 Adam"></a>2.3 Adam</h5><p>Adam（Adaptive Moment Estimation）相当于RMSprop+Momentum，</p>
<p><a href="https://imgtu.com/i/W27Kr4"><img src="https://z3.ax1x.com/2021/07/25/W27Kr4.png" alt="W27Kr4.png"></a></p>
<p>等式1和等式2计算了梯度的指数平均和梯度平方的指数平均；等式3在学习率上乘以梯度的平均（类似动量），除以梯度平方平均的均方根（类似RMSProp），得出学习步幅；等式4是权重更新步骤。其中超参数β1一般取0.9，β2一般取0.99，ε一般取1e-10。</p>
<h5 id="2-4-Learning-Rate-Scheduling"><a href="#2-4-Learning-Rate-Scheduling" class="headerlink" title="2.4 Learning Rate Scheduling"></a>2.4 Learning Rate Scheduling</h5><ul>
<li>学习率衰减（Learning Rate Decay）——为了防止学习率过大，在收敛到全局最优点的时候会来回摆荡，所以要让学习率随着训练轮数不断下降，收敛梯度下降的学习步长。其中包括线性衰减（例如：每过5个epochs学习率减半）、指数衰减（随着迭代轮数的增加学习率自动发生衰减）、分段常数衰减等方法。</li>
<li>热启动（Warm up）——warmup是初始阶段使用较小学习率启动，后期恢复正常；而decay是初始时使用较大的学习率，之后进行衰减。其有助于减缓模型在初始阶段对mini-batch的提前过拟合现象，保持分布的平稳。</li>
</ul>
<h4 id="4-损失函数-Loss-的影响"><a href="#4-损失函数-Loss-的影响" class="headerlink" title="4 损失函数(Loss)的影响"></a>4 损失函数(Loss)的影响</h4><p>均方误差（Mean Square Error，MSE）适用于回归问题，而交叉熵（Cross Entropy）适用于分类问题。<br>在分类问题中，MSE损失函数为$\text { loss }=\frac{1}{2 m} \sum_{i}^{m}\left(y_{i}-y^{\prime}\right)^{2}$，加上softmax之后为$\operatorname{los} s_{i}=\left(c_{1}-\frac{e^{y i^{\prime}}}{c_{2}}\right)^{2}$，是一个非凸函数，梯度下降算法难以达到全局最优解。所以MSE在分类问题中，并不是一个好的loss函数；而交叉熵损失函数为$\operatorname{loss}=-\sum_{i=1}^{n} y_{i} * \log \left(y_{i}\right)$，加入softmax得到$\operatorname{loss}<em>{i}=-\log \left(c</em>{1}-\frac{e^{x}}{c_{2}}\right)$。</p>
<p><a href="https://imgtu.com/i/W27DII"><img src="https://z3.ax1x.com/2021/07/25/W27DII.jpg" alt="W27DII.jpg"></a></p>
<p>相对MSE而言，交叉熵曲线整体呈单调性，loss越大，梯度越大。便于梯度下降反向传播，利于优化。所以一般针对分类问题采用交叉熵作为loss函数。（P.S.在广义伯努利分布下，最小化交叉熵损失等同于极大似然估计）</p>
<h4 id="5-批量标准化-归一化-Batch-Normalization，BN"><a href="#5-批量标准化-归一化-Batch-Normalization，BN" class="headerlink" title="5 批量标准化/归一化(Batch Normalization，BN)"></a>5 批量标准化/归一化(Batch Normalization，BN)</h4><p>Batch Normalization，简称BatchNorm或BN，翻译为“批归一化”，是神经网络中一种特殊的层，如今已是各种流行网络的标配。在原始论文中，BN被建议插入在（每个）ReLU激活层前面。</p>
<p><a href="https://imgtu.com/i/W27cz8"><img src="https://z3.ax1x.com/2021/07/25/W27cz8.png" alt="W27cz8.png"></a></p>
<p>BN操作分为两步：</p>
<ol>
<li>Standardization：首先对$m$个$x$进行Standardization，得到zero mean unit variance的分布$\hat{x}$。</li>
<li>scale and shift：然后再对$\hat{x}$进行scale and shift，缩放并变换回原始的分布$y$。（这样的目的是为了补偿网络的非线性表达能力，因为经过标准化之后，偏移量丢失。均值$\beta$方差$\gamma$相当于输入数据分布的方差和偏移。）</li>
</ol>
<p>Batch Normalization有以下几点优势：</p>
<ul>
<li>BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度</li>
<li>BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定</li>
<li>BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题</li>
<li>BN具有一定的正则化效果</li>
</ul>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://www.bilibili.com/video/av59538266">b站李宏毅《机器学习》</a><br>2.<a href="https://datawhalechina.github.io/leeml-notes/#/">datawhale李宏毅机器学习笔记(LeeML-Notes)</a></p>
]]></content>
      <categories>
        <category>Neural Networks</category>
      </categories>
      <tags>
        <tag>Momentum</tag>
        <tag>Adam</tag>
      </tags>
  </entry>
  <entry>
    <title>模型融合_1</title>
    <url>/2021/03/15/%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88_1/</url>
    <content><![CDATA[<p>Kaggle和天池比赛中常用提高成绩的三种方法：</p>
<blockquote>
<p>1.特征工程<br>2.模型调参<br>3.模型融合<br>模型融合主要有以下几种方式：</p>
</blockquote>
<h4 id="简单加权融合"><a href="#简单加权融合" class="headerlink" title="简单加权融合:"></a>简单加权融合:</h4><blockquote>
<p>①回归（分类概率）：算术平均融合（Arithmetic mean），几何平均融合（Geometric mean）；<br>②分类：投票（Voting)<br>③综合：排序融合(Rank averaging)，log融合</p>
</blockquote>
<h4 id="stacking-blending"><a href="#stacking-blending" class="headerlink" title="stacking/blending:"></a>stacking/blending:</h4><blockquote>
<p>构建多层模型，把初级学习器的输出当作下一层的输入。</p>
</blockquote>
<span id="more"></span>
<h4 id="boosting-bagging（在xgboost-Adaboost-GBDT中已经用到）"><a href="#boosting-bagging（在xgboost-Adaboost-GBDT中已经用到）" class="headerlink" title="boosting/bagging（在xgboost,Adaboost,GBDT中已经用到）:"></a>boosting/bagging（在xgboost,Adaboost,GBDT中已经用到）:</h4><blockquote>
<p>多个分类器的整合</p>
</blockquote>
<h4 id="部分代码案例"><a href="#部分代码案例" class="headerlink" title="部分代码案例:"></a>部分代码案例:</h4><h5 id="1-简单加权平均"><a href="#1-简单加权平均" class="headerlink" title="1.简单加权平均"></a>1.简单加权平均</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Weighted_method</span>(<span class="params">test_pre1,test_pre2,test_pre3,w=[<span class="number">1</span>/<span class="number">3</span>,<span class="number">1</span>/<span class="number">3</span>,<span class="number">1</span>/<span class="number">3</span>]</span>):</span></span><br><span class="line">    Weighted_result = w[<span class="number">0</span>]*pd.Series(test_pre1)+w[<span class="number">1</span>]*pd.Series(test_pre2)+w[<span class="number">2</span>]*pd.Series(test_pre3)</span><br><span class="line">    <span class="keyword">return</span> Weighted_result</span><br><span class="line">    </span><br><span class="line">Weighted_pre = Weighted_method(test_pre1,test_pre2,test_pre3,w)</span><br></pre></td></tr></table></figure>
<h5 id="2-Stacking融合-回归"><a href="#2-Stacking融合-回归" class="headerlink" title="2.Stacking融合(回归)"></a>2.Stacking融合(回归)</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Stacking_method</span>(<span class="params">train_reg1,train_reg2,train_reg3,y_train_true,test_pre1,test_pre2,test_pre3,model_L2= linear_model.LinearRegression(<span class="params"></span>)</span>):</span>    model_L2.fit(pd.concat([pd.Series(train_reg1),pd.Series(train_reg2),pd.Series(train_reg3)],axis=<span class="number">1</span>).values,y_train_true)</span><br><span class="line">    Stacking_result = model_L2.predict(pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=<span class="number">1</span>).values)</span><br><span class="line">    <span class="keyword">return</span> Stacking_result</span><br><span class="line"></span><br><span class="line">Stacking_pre = Stacking_method(train_reg1,train_reg2,train_reg3,y_train_true,</span><br><span class="line">                               test_pre1,test_pre2,test_pre3,model_L2)</span><br></pre></td></tr></table></figure>
<h5 id="3-Voting投票机制"><a href="#3-Voting投票机制" class="headerlink" title="3.Voting投票机制"></a>3.Voting投票机制</h5><blockquote>
<p>硬投票：对多个模型直接进行投票，不区分模型结果的相对重要度，最终投票数最多的类为最终被预测的类。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eclf = VotingClassifier(estimators=[(<span class="string">&#x27;lgb&#x27;</span>, clf1), (<span class="string">&#x27;rf&#x27;</span>, clf2), (<span class="string">&#x27;svc&#x27;</span>, clf3)], voting=<span class="string">&#x27;hard&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> clf, label <span class="keyword">in</span> <span class="built_in">zip</span>([clf1, clf2, clf3, eclf], [<span class="string">&#x27;LGB&#x27;</span>, <span class="string">&#x27;Random Forest&#x27;</span>, <span class="string">&#x27;SVM&#x27;</span>, <span class="string">&#x27;Ensemble&#x27;</span>]):</span><br><span class="line">    scores = cross_val_score(clf, x, y, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Accuracy: %0.2f (+/- %0.2f) [%s]&quot;</span> % (scores.mean(), scores.std(), label))</span><br></pre></td></tr></table></figure>
<h5 id="4-分类模型的Stacking融合"><a href="#4-分类模型的Stacking融合" class="headerlink" title="4.分类模型的Stacking融合"></a>4.分类模型的Stacking融合</h5><blockquote>
<p>Stacking与Blending相比存在一定优势:<br>1.充分使用数据<br>2.使用多次的交叉验证会比较稳健<br>3.不容易过拟合</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clfs = [LogisticRegression(solver=<span class="string">&#x27;lbfgs&#x27;</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;entropy&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j, clf <span class="keyword">in</span> <span class="built_in">enumerate</span>(clfs):</span><br><span class="line">    <span class="comment">#依次训练各个单模型</span></span><br><span class="line">    clf.fit(X_d1, y_d1)</span><br><span class="line">    y_submission = clf.predict_proba(X_d2)[:, <span class="number">1</span>]</span><br><span class="line">    dataset_d1[:, j] = y_submission</span><br><span class="line">    <span class="comment">#对于测试集，直接用这k个模型的预测值作为新的特征。</span></span><br><span class="line">    dataset_d2[:, j] = clf.predict_proba(X_predict)[:, <span class="number">1</span>]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;val auc Score: %f&quot;</span> % roc_auc_score(y_predict, dataset_d2[:, j]))</span><br><span class="line"></span><br><span class="line"><span class="comment">#融合使用的模型</span></span><br><span class="line">clf = GradientBoostingClassifier(learning_rate=<span class="number">0.02</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">30</span>)</span><br><span class="line">clf.fit(dataset_d1, y_d2)</span><br></pre></td></tr></table></figure>
<h5 id="5-其他Stacking"><a href="#5-其他Stacking" class="headerlink" title="5.其他Stacking"></a>5.其他Stacking</h5><blockquote>
<p>将特征放进模型中预测，并将预测结果变换并作为新的特征加入原有特征中再经过模型预测结果</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Ensemble_add_feature</span>(<span class="params">train,test,target,clfs</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> j,clf <span class="keyword">in</span> <span class="built_in">enumerate</span>(clfs):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;依次训练各个单模型&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># print(j, clf)</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;使用第1个部分作为预测，第2部分来训练模型，获得其预测的输出作为第2部分的新特征。&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]</span></span><br><span class="line"></span><br><span class="line">        clf.fit(train,target)</span><br><span class="line">        y_train = clf.predict(train)</span><br><span class="line">        y_test = clf.predict(test)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## 新特征生成</span></span><br><span class="line">        train_[:,j*<span class="number">2</span>] = y_train**<span class="number">2</span></span><br><span class="line">        test_[:,j*<span class="number">2</span>] = y_test**<span class="number">2</span></span><br><span class="line">        train_[:, j+<span class="number">1</span>] = np.exp(y_train)</span><br><span class="line">        test_[:, j+<span class="number">1</span>] = np.exp(y_test)</span><br><span class="line">        <span class="comment"># print(&quot;val auc Score: %f&quot; % r2_score(y_predict, dataset_d2[:, j]))</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Method &#x27;</span>,j)</span><br><span class="line"></span><br><span class="line">    train_ = pd.DataFrame(train_)</span><br><span class="line">    test_ = pd.DataFrame(test_)</span><br><span class="line">    <span class="keyword">return</span> train_,test_</span><br><span class="line"></span><br><span class="line"><span class="comment">#模型融合中使用到的各个单模型</span></span><br><span class="line">clfs = [LogisticRegression(),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;entropy&#x27;</span>),</span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"></span><br><span class="line">New_train,New_test = Ensemble_add_feature(x_train,x_test,y_train,clfs)</span><br><span class="line">clf = GradientBoostingClassifier(learning_rate=<span class="number">0.02</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">30</span>)</span><br><span class="line">clf.fit(New_train, y_train)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Ensemble Learning</category>
      </categories>
      <tags>
        <tag>voting</tag>
        <tag>Stacking</tag>
        <tag>Blending</tag>
      </tags>
  </entry>
  <entry>
    <title>特征工程_1</title>
    <url>/2021/04/21/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B_1/</url>
    <content><![CDATA[<p>特征工程就是将原始数据空间变换到新的特征空间，在新的特征空间中，模型能够更好地学习数据中的规律。特征的选择和构造，就是人为地帮助模型学习到原本很难学好的东西，从而使模型达到更好的效果。</p>
<h4 id="1-根据现实情况构造特征"><a href="#1-根据现实情况构造特征" class="headerlink" title="1. 根据现实情况构造特征"></a>1. 根据现实情况构造特征</h4><h5 id="1-1-各点与特定点的距离"><a href="#1-1-各点与特定点的距离" class="headerlink" title="1.1 各点与特定点的距离"></a>1.1 各点与特定点的距离</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;x_dis&#x27;</span>] = (df[<span class="string">&#x27;x&#x27;</span>] - <span class="number">6165599</span>).<span class="built_in">abs</span>()</span><br><span class="line">df[<span class="string">&#x27;y_dis&#x27;</span>] = (df[<span class="string">&#x27;y&#x27;</span>] - <span class="number">5202660</span>).<span class="built_in">abs</span>()</span><br><span class="line">df[<span class="string">&#x27;base_dis] = (df[&#x27;</span>y_dis<span class="string">&#x27;]**2))**0.5 + ((df[&#x27;</span>x_dis<span class="string">&#x27;]**2)</span></span><br><span class="line"><span class="string">del df[&#x27;</span>x_dis<span class="string">&#x27;],df[&#x27;</span>y_dis<span class="string">&#x27;] </span></span><br><span class="line"><span class="string">df[&#x27;</span>base_dis_dif<span class="string">f&#x27;].head()</span></span><br></pre></td></tr></table></figure>
<h5 id="1-2-将时间划分为白天与黑夜"><a href="#1-2-将时间划分为白天与黑夜" class="headerlink" title="1.2 将时间划分为白天与黑夜"></a>1.2 将时间划分为白天与黑夜</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;day_night&#x27;</span>] = <span class="number">0</span></span><br><span class="line">df.loc[(df[<span class="string">&#x27;hour&#x27;</span>] &gt; <span class="number">5</span>) &amp; (df[<span class="string">&#x27;hour&#x27;</span>] &lt; <span class="number">20</span>),<span class="string">&#x27;day_night&#x27;</span>] = <span class="number">1</span></span><br><span class="line">df[<span class="string">&#x27;day_night&#x27;</span>].head()</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h5 id="1-3-将月份划分为季度"><a href="#1-3-将月份划分为季度" class="headerlink" title="1.3 将月份划分为季度"></a>1.3 将月份划分为季度</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;quarter&#x27;</span>] = <span class="number">0</span></span><br><span class="line">df.loc[(df[<span class="string">&#x27;month&#x27;</span>].isin([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])), <span class="string">&#x27;quarter&#x27;</span>] = <span class="number">1</span></span><br><span class="line">df.loc[(df[<span class="string">&#x27;month&#x27;</span>].isin([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, ])), <span class="string">&#x27;quarter&#x27;</span>] = <span class="number">2</span></span><br><span class="line">df.loc[(df[<span class="string">&#x27;month&#x27;</span>].isin([<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])), <span class="string">&#x27;quarter&#x27;</span>] = <span class="number">3</span></span><br><span class="line">df.loc[(df[<span class="string">&#x27;month&#x27;</span>].isin([<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>])), <span class="string">&#x27;quarter&#x27;</span>] = <span class="number">4</span></span><br></pre></td></tr></table></figure>
<h5 id="1-4-特征变化量之间的相似性"><a href="#1-4-特征变化量之间的相似性" class="headerlink" title="1.4 特征变化量之间的相似性"></a>1.4 特征变化量之间的相似性</h5><p>①统计每个ship的对应速度等级的个数.<br>②对方位进行16均分.<br>③统计速度为0的个数，以及速度不为0的统计量.<br>④加入x，v，d，y的中位数和各种位数,并删去count\mean\min\max\std等多余统计特征.<br>⑤以shift为主键,求相邻差异值(偏移量).</p>
<h4 id="2-构造分箱特征"><a href="#2-构造分箱特征" class="headerlink" title="2. 构造分箱特征"></a>2. 构造分箱特征</h4><h5 id="2-1-经纬度和速度的分箱特征"><a href="#2-1-经纬度和速度的分箱特征" class="headerlink" title="2.1 经纬度和速度的分箱特征"></a>2.1 经纬度和速度的分箱特征</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;v_bin&#x27;</span>] = pd.qcut(df[<span class="string">&#x27;v&#x27;</span>], <span class="number">200</span>, duplicates=<span class="string">&#x27;drop&#x27;</span>) <span class="comment"># 速度进行 200分位数分箱</span></span><br><span class="line">df[<span class="string">&#x27;v_bin&#x27;</span>] = df[<span class="string">&#x27;v_bin&#x27;</span>].<span class="built_in">map</span>(<span class="built_in">dict</span>(<span class="built_in">zip</span>(df[<span class="string">&#x27;v_bin&#x27;</span>].unique().<span class="built_in">range</span>(df[<span class="string">&#x27;v_bin&#x27;</span>].nunique())))) <span class="comment"># 分箱后映射编码</span></span><br></pre></td></tr></table></figure>

<h5 id="2-2-经纬度分箱后并构造区域"><a href="#2-2-经纬度分箱后并构造区域" class="headerlink" title="2.2 经纬度分箱后并构造区域"></a>2.2 经纬度分箱后并构造区域</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">traj.sort_values(by=<span class="string">&#x27;x&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">x_res = np.zeros((<span class="built_in">len</span>(traj), ))</span><br><span class="line">j = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, col_bins + <span class="number">1</span>):</span><br><span class="line">    low, high = x_bins[i-<span class="number">1</span>], x_bins[i]</span><br><span class="line">    <span class="keyword">while</span>( j &lt; <span class="built_in">len</span>(traj)):</span><br><span class="line">        <span class="keyword">if</span> (traj[<span class="string">&quot;x&quot;</span>].iloc[j] &lt;= high) &amp; (traj[<span class="string">&quot;x&quot;</span>].iloc[j] &gt; low - <span class="number">0.001</span>):</span><br><span class="line">            x_res[j] = i</span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<h4 id="3-构造DataFramte特征"><a href="#3-构造DataFramte特征" class="headerlink" title="3. 构造DataFramte特征"></a>3. 构造DataFramte特征</h4><h5 id="3-1-count计数值"><a href="#3-1-count计数值" class="headerlink" title="3.1 count计数值"></a>3.1 count计数值</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_save_unique_visit_count_table</span>(<span class="params">traj_data_df=<span class="literal">None</span>, bin_to_coord_df=<span class="literal">None</span></span>):</span></span><br><span class="line">    unique_boat_count_df = traj_data_df.groupby([<span class="string">&quot;no_bin&quot;</span>])[<span class="string">&quot;id&quot;</span>].nunique().reset_index()</span><br><span class="line">    unique_boat_count_df.rename(&#123;<span class="string">&quot;id&quot;</span>:<span class="string">&quot;visit_boat_count&quot;</span>&#125;, axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    unique_boat_count_df_save = pd.merge(bin_to_coord_df, unique_boat_count_df,</span><br><span class="line">                                         on=<span class="string">&quot;no_bin&quot;</span>, how=<span class="string">&quot;left&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> unique_boat_count_d</span><br></pre></td></tr></table></figure>
<h5 id="3-2-shift偏移量"><a href="#3-2-shift偏移量" class="headerlink" title="3.2 shift偏移量"></a>3.2 shift偏移量</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> [<span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;y&#x27;</span>]:</span><br><span class="line">    <span class="comment">#对x,y坐标进行时间平移 1 -1 2</span></span><br><span class="line">    df[f + <span class="string">&#x27;_prev_diff&#x27;</span>] = df[f] - g[f].shift(<span class="number">1</span>)</span><br><span class="line">    df[f + <span class="string">&#x27;_next_diff&#x27;</span>] = df[f] - g[f].shift(-<span class="number">1</span>)</span><br><span class="line">    df[f + <span class="string">&#x27;_prev_next_diff&#x27;</span>] = g[f].shift(<span class="number">1</span>) - g[f].shift(-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h5 id="3-3-统计特征"><a href="#3-3-统计特征" class="headerlink" title="3.3 统计特征"></a>3.3 统计特征</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">group_feature</span>(<span class="params">df, key, target, aggs,flag</span>):</span>   </span><br><span class="line">    <span class="comment">#通过字典的形式来构建方法和重命名</span></span><br><span class="line">    agg_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> ag <span class="keyword">in</span> aggs:</span><br><span class="line">        agg_dict[<span class="string">&#x27;&#123;&#125;_&#123;&#125;_&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(target,ag,flag)] = ag</span><br><span class="line">        </span><br><span class="line">    t = df.groupby(key)[target].agg(agg_dict).reset_index()</span><br><span class="line">    <span class="keyword">return</span> t</span><br><span class="line"></span><br><span class="line">t = group_feature(df, <span class="string">&#x27;ship&#x27;</span>,<span class="string">&#x27;x&#x27;</span>,[<span class="string">&#x27;max&#x27;</span>,<span class="string">&#x27;min&#x27;</span>,<span class="string">&#x27;mean&#x27;</span>,<span class="string">&#x27;median&#x27;</span>,<span class="string">&#x27;std&#x27;</span>,<span class="string">&#x27;skew&#x27;</span>],flag)</span><br><span class="line">train = pd.merge(train, t, on=<span class="string">&#x27;ship&#x27;</span>, how=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line">t = group_feature(df, <span class="string">&#x27;ship&#x27;</span>,<span class="string">&#x27;y&#x27;</span>,[<span class="string">&#x27;max&#x27;</span>,<span class="string">&#x27;min&#x27;</span>,<span class="string">&#x27;mean&#x27;</span>,<span class="string">&#x27;median&#x27;</span>,<span class="string">&#x27;std&#x27;</span>,<span class="string">&#x27;skew&#x27;</span>],flag)</span><br><span class="line">train = pd.merge(train, t, on=<span class="string">&#x27;ship&#x27;</span>, how=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line">t = group_feature(df, <span class="string">&#x27;ship&#x27;</span>,<span class="string">&#x27;base_dis_diff&#x27;</span>,[<span class="string">&#x27;max&#x27;</span>,<span class="string">&#x27;min&#x27;</span>,<span class="string">&#x27;mean&#x27;</span>,<span class="string">&#x27;std&#x27;</span>,<span class="string">&#x27;skew&#x27;</span>],flag)</span><br><span class="line">train = pd.merge(train, t, on=<span class="string">&#x27;ship&#x27;</span>, how=<span class="string">&#x27;left&#x27;</span>)</span><br></pre></td></tr></table></figure>


<h4 id="4-构造Embedding特征"><a href="#4-构造Embedding特征" class="headerlink" title="4.构造Embedding特征"></a>4.构造Embedding特征</h4><p>word embedding就是将词映射到另外一个空间,相同类型的词在投影之后的向量空间距离更近,倾向于归到一起.</p>
<h5 id="4-1-Word2vec构造词向量"><a href="#4-1-Word2vec构造词向量" class="headerlink" title="4.1 Word2vec构造词向量"></a>4.1 Word2vec构造词向量</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(num_runs)):</span><br><span class="line">    model = Word2Vec(sentences, size=embedding_size,</span><br><span class="line">                              min_count=min_count,</span><br><span class="line">                              workers=mp.cpu_count(),</span><br><span class="line">                              window=window_size,</span><br><span class="line">                              seed=seed, <span class="built_in">iter</span>=iters, sg=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    embedding_vec = []</span><br><span class="line">    <span class="keyword">for</span> ind, seq <span class="keyword">in</span> <span class="built_in">enumerate</span>(sentences):</span><br><span class="line">        seq_vec, word_count = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> seq:</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> model:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                seq_vec += model[word]</span><br><span class="line">                word_count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> word_count == <span class="number">0</span>:</span><br><span class="line">            embedding_vec.append(embedding_size * [<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            embedding_vec.append(seq_vec / word_count)</span><br></pre></td></tr></table></figure>

<h5 id="4-2-NMF提取文本的主题分布"><a href="#4-2-NMF提取文本的主题分布" class="headerlink" title="4.2 NMF提取文本的主题分布"></a>4.2 NMF提取文本的主题分布</h5><p>TF-IDF是衡量字词的出现频率来定义其重要性的加权技术.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用tfidf对元素进行处理</span></span><br><span class="line">tfidf_vectorizer = TfidfVectorizer(ngram_range=(tf_n,tf_n))</span><br><span class="line">tfidf = tfidf_vectorizer.fit_transform(self.data[<span class="string">&#x27;title_feature&#x27;</span>].values)</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用nmf算法，提取文本的主题分布</span></span><br><span class="line">text_nmf = NMF(n_components=self.nmf_n).fit_transform(tfidf)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Feature Engineering</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
        <tag>Embedding</tag>
        <tag>Word2vec</tag>
        <tag>NMF</tag>
        <tag>DataFramte</tag>
      </tags>
  </entry>
  <entry>
    <title>特征选择_1</title>
    <url>/2021/03/20/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9_1/</url>
    <content><![CDATA[<p>在数据预处理过程中，特征选择是一个重要的过程，选择出重要的特征可以加快模型训练速度。通常可以从以下两方面来选择特征：</p>
<blockquote>
<p>1.特征是否发散（对于样本区分作用的大小）<br>2.特征与标签的相关性</p>
</blockquote>
<span id="more"></span>
<p>特征选择的方法主要有3种：</p>
<blockquote>
<p>1.Filter Method：先根据统计量设置阈值选择特征，之后再训练模型。<br>2.Wrapper Method：把最终将要使用的模型的性能作为特征子集的评价标准，多次训练模型选择有利于模型性能的特征子集。<br>3.Embedding Method：将特征选择过程与模型训练过程融为一体，在模型训练的过程中自动进行特征选择。</p>
</blockquote>
<p>常用sklearn中的feature_selection库来进行特征选择。</p>
<h4 id="1-Fliter-过滤法"><a href="#1-Fliter-过滤法" class="headerlink" title="1. Fliter 过滤法:"></a>1. Fliter 过滤法:</h4><blockquote>
<p>Fliter的优点在于只训练一次模型，速度快。但是选择与标签相关性最强的特征子集不一定是最佳特征，甚至可能对结果负优化。</p>
</blockquote>
<h5 id="1-1-方差选择法"><a href="#1-1-方差选择法" class="headerlink" title="1.1 方差选择法"></a>1.1 方差选择法</h5><p>计算各个特征的方差，设置阈值，选择方差大于阈值的特征。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line"><span class="comment">#参数threshold为方差的阈值</span></span><br><span class="line">VarianceThreshold(threshold=<span class="number">3</span>).fit_transform(iris.data)</span><br></pre></td></tr></table></figure>

<h5 id="1-2-Pearson相关系数法"><a href="#1-2-Pearson相关系数法" class="headerlink" title="1.2 Pearson相关系数法"></a>1.2 Pearson相关系数法</h5><p>计算各个特征对于标签的Pearson相关系数和p值，选择前k名的特征。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"><span class="comment">#参数k为选择的特征个数</span></span><br><span class="line">SelectKBest(<span class="keyword">lambda</span> X, Y: array(<span class="built_in">map</span>(<span class="keyword">lambda</span> x:pearsonr(x, Y), X.T)).T, k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Pearson法的缺陷在于只对线性相关敏感，对非线性关系不敏感。</p>
</blockquote>
<h5 id="1-3-卡方检验-互信息法等方法"><a href="#1-3-卡方检验-互信息法等方法" class="headerlink" title="1.3 卡方检验\互信息法等方法"></a>1.3 卡方检验\互信息法等方法</h5><p>也是用来评价X与y的相关性，先构建评价函数，再选择前K名的特征。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#chi2</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line">SelectKBest(chi2, k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br><span class="line"></span><br><span class="line"><span class="comment">#MIC</span></span><br><span class="line"><span class="keyword">from</span> minepy <span class="keyword">import</span> MINE</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mic</span>(<span class="params">x, y</span>):</span></span><br><span class="line">     m = MINE()</span><br><span class="line">     m.compute_score(x, y)</span><br><span class="line">     <span class="keyword">return</span> (m.mic(), <span class="number">0.5</span>)</span><br><span class="line">SelectKBest(<span class="keyword">lambda</span> X, Y: array(<span class="built_in">map</span>(<span class="keyword">lambda</span> x:mic(x, Y), X.T)).T, k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>

<h4 id="2-Wrapper-包装法"><a href="#2-Wrapper-包装法" class="headerlink" title="2. Wrapper 包装法:"></a>2. Wrapper 包装法:</h4><blockquote>
<p>Wrapper的优点在于能够识别模型最适宜的特征子集，缺点在于训练多次模型，算法复杂性高，且特征子集不一定是<u>大多数解释变量</u>。</p>
</blockquote>
<p>Wrapper最具代表性的方法就是RFE递归消除特征法，即使用一个基模型来进行多轮训练，每轮训练都遍历所有特征，之后消除重要性(feature_importances_)低的特征，再基于新的特征集进行下一轮训练。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#RFE</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">rfe = RFE(estimator=LogisticRegression(), n_features_to_select=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br><span class="line"></span><br><span class="line"><span class="comment">#RFECV</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFECV</span><br><span class="line">rfecv = RFECV(estimator=svc, step=<span class="number">1</span>, cv=StratifiedKFold(<span class="number">2</span>), scoring=<span class="string">&#x27;roc_auc&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="3-Embedded-嵌入法"><a href="#3-Embedded-嵌入法" class="headerlink" title="3. Embedded 嵌入法"></a>3. Embedded 嵌入法</h4><h5 id="3-1-基于惩罚项的特征选择法"><a href="#3-1-基于惩罚项的特征选择法" class="headerlink" title="3.1 基于惩罚项的特征选择法"></a>3.1 基于惩罚项的特征选择法</h5><p>使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。由于L1正则化会产生稀疏权值矩阵，所以其自带特征选择的特性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#L1正则</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">SelectFromModel(LogisticRegression(penalty=<span class="string">&quot;l1&quot;</span>, C=<span class="number">0.1</span>)).fit_transform(iris.data, iris.target)</span><br><span class="line"></span><br><span class="line"><span class="comment">#L2正则</span></span><br><span class="line">SelectFromModel(LogisticRegression(penalty=<span class="string">&quot;l2&quot;</span>,threshold=<span class="number">0.5</span>, C=<span class="number">0.1</span>)).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>

<h5 id="3-2-基于树模型的特征选择法"><a href="#3-2-基于树模型的特征选择法" class="headerlink" title="3.2 基于树模型的特征选择法"></a>3.2 基于树模型的特征选择法</h5><p>树模型中GBDT也可用来作为基模型进行特征选择。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line">SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>

<h4 id="4-基于SHAP值的特征筛选"><a href="#4-基于SHAP值的特征筛选" class="headerlink" title="4. 基于SHAP值的特征筛选"></a>4. 基于SHAP值的特征筛选</h4><p>SHAP是由Shapley value启发的可加性解释模型。对于每条样本，每个特征都会对应一个SHAP value值体现其对结果的贡献。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> shap</span><br><span class="line"><span class="comment"># 创建模型解释器</span></span><br><span class="line">explainer_xgb = shap.TreeExplainer(model1)</span><br><span class="line">explainer_lgb = shap.TreeExplainer(model2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取训练集每个样本每个特征特征的SHAP值，并对特征进行整体的可视化</span></span><br><span class="line">shape_values = explainer_lgb.shap_values(data[cols])</span><br><span class="line">shap.summary_plot(shape_values, data[cols], plot_type=<span class="string">&#x27;bar&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="5-基于对抗验证-Adversarial-Validation-的特征筛选"><a href="#5-基于对抗验证-Adversarial-Validation-的特征筛选" class="headerlink" title="5. 基于对抗验证(Adversarial Validation)的特征筛选"></a>5. 基于对抗验证(Adversarial Validation)的特征筛选</h4><p>常用于训练集与测试集相差非常大的情况。实现步骤：<br>1.将训练集和测试集合并，分别打上0和1的标签。<br>2.构建模型进行训练，逐个将特征输入模型，记录AUC。<br>3.最后将AUC高的特征删除(将测试和训练样本差别很大的特征删除)，通过删掉这些特征实现模型效果提升。</p>
]]></content>
      <categories>
        <category>Feature Engineering</category>
      </categories>
      <tags>
        <tag>Embedding</tag>
        <tag>Wapper</tag>
        <tag>Filter</tag>
      </tags>
  </entry>
</search>
