<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>LightGBM调参_1</title>
    <url>/2021/03/26/lightGBM%E8%B0%83%E5%8F%82/</url>
    <content><![CDATA[<p>#1简单列举一下日常调参过程中常用的几种方法，具体的原理下次补上。</p>
<h4 id="1-经验法"><a href="#1-经验法" class="headerlink" title="1. 经验法:"></a>1. 经验法:</h4><blockquote>
<p>往两个方向调：</p>
<p>1.提高准确率：max_depth, num_leaves, learning_rate</p>
<p>2.降低过拟合：max_bin, min_data_in_leaf；L1, L2正则化；数据抽样, 列采样</p>
</blockquote>
<p>1.使用较小的num_leaves，max_depth和max_bin，降低复杂度。</p>
<p>2.使用min_data_in_leaf和min_sum_hessian_in_leaf，该值越大，模型的学习越保守。</p>
<span id="more"></span>

<p>3.设置bagging_freq和bagging_fraction使用bagging。</p>
<p>4.设置feature_fraction进行特征采样。</p>
<p>5.使用lambda_l1,lambda_l2和min_gain_to_split正则化。</p>
<h4 id="2-贪心调参"><a href="#2-贪心调参" class="headerlink" title="2. 贪心调参:"></a>2. 贪心调参:</h4><p>先调整对模型影响最大的参数，再调整对模型影响次大的参数，缺点是容易调成局部最优，需要多次调试。日常调参顺序如下:</p>
<p>① num_leaves, max_depth</p>
<p>② min_data_in_leaf, min_child_weight</p>
<p>③ bagging_freq, bagging_fraction,  feature_fraction,</p>
<p>④ reg_lambda, reg_alpha</p>
<p>⑤ min_split_gain</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="comment"># 调objective</span></span><br><span class="line">best_obj = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> obj <span class="keyword">in</span> objective:</span><br><span class="line">    model = LGBMRegressor(objective=obj)</span><br><span class="line">    score = cross_val_score(model, X_train, y_train, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;f1&#x27;</span>).mean()</span><br><span class="line">    best_obj[obj] = score</span><br><span class="line"></span><br><span class="line"><span class="comment"># num_leaves</span></span><br><span class="line">best_leaves = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> leaves <span class="keyword">in</span> num_leaves:</span><br><span class="line">    model = LGBMRegressor(objective=<span class="built_in">min</span>(best_obj.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])[<span class="number">0</span>], num_leaves=leaves)</span><br><span class="line">    score = cross_val_score(model, X_train, y_train, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;f1&#x27;</span>).mean()</span><br><span class="line">    best_leaves[leaves] = score</span><br><span class="line"></span><br><span class="line"><span class="comment"># max_depth</span></span><br><span class="line">best_depth = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> depth <span class="keyword">in</span> max_depth:</span><br><span class="line">    model = LGBMRegressor(objective=<span class="built_in">min</span>(best_obj.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])[<span class="number">0</span>],</span><br><span class="line">                          num_leaves=<span class="built_in">min</span>(best_leaves.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])[<span class="number">0</span>],</span><br><span class="line">                          max_depth=depth)</span><br><span class="line">    score = cross_val_score(model, X_train, y_train, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;f1&#x27;</span>).mean()</span><br><span class="line">    best_depth[depth] = score</span><br></pre></td></tr></table></figure>

<p>以此类推，按调参顺序依次调整优化，并且可以对每一个最优参数下模型的得分进行可视化。</p>
<h4 id="3-网格搜索"><a href="#3-网格搜索" class="headerlink" title="3. 网格搜索"></a>3. 网格搜索</h4><p>即穷举搜索，在参数数组里循环遍历，一般大数据集不会用到，因为速度太慢。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_best_cv_params</span>(<span class="params">learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">581</span>, num_leaves=<span class="number">31</span>, max_depth=-<span class="number">1</span>, bagging_fraction=<span class="number">1.0</span>, feature_fraction=<span class="number">1.0</span>, bagging_freq=<span class="number">0</span>, min_data_in_leaf=<span class="number">20</span>, min_child_weight=<span class="number">0.001</span>, min_split_gain=<span class="number">0</span>, reg_lambda=<span class="number">0</span>, reg_alpha=<span class="number">0</span>, param_grid=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    cv_fold = KFold(n_splits=<span class="number">5</span>, shuffle=<span class="literal">True</span>, random_state=<span class="number">2021</span>)</span><br><span class="line"></span><br><span class="line">    model_lgb = lgb.LGBMClassifier(learning_rate=learning_rate,</span><br><span class="line">                                   n_estimators=n_estimators,</span><br><span class="line">                                   num_leaves=num_leaves,</span><br><span class="line">                                   max_depth=max_depth,</span><br><span class="line">                                   bagging_fraction=bagging_fraction,</span><br><span class="line">                                   feature_fraction=feature_fraction,</span><br><span class="line">                                   bagging_freq=bagging_freq,</span><br><span class="line">                                   min_data_in_leaf=min_data_in_leaf,</span><br><span class="line">                                   min_child_weight=min_child_weight,</span><br><span class="line">                                   min_split_gain=min_split_gain,</span><br><span class="line">                                   reg_lambda=reg_lambda,</span><br><span class="line">                                   reg_alpha=reg_alpha,</span><br><span class="line">                                   n_jobs= <span class="number">8</span></span><br><span class="line">                                  )</span><br><span class="line"></span><br><span class="line">    f1 = make_scorer(f1_score, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">    grid_search = GridSearchCV(estimator=model_lgb, </span><br><span class="line">                               cv=cv_fold,</span><br><span class="line">                               param_grid=param_grid,</span><br><span class="line">                               scoring=f1</span><br><span class="line"></span><br><span class="line">                              )</span><br><span class="line">    grid_search.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;模型当前最优参数为:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(grid_search.best_params_))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;模型当前最优得分为:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(grid_search.best_score_))</span><br></pre></td></tr></table></figure>
<p>总体思路是先粗调再细调。在一开始调整时，可设置较大的学习率如0.1，先确定树的个数，再依次调整参数，最后设置较小的学习率如0.05，确定最终参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lgb_params = &#123;<span class="string">&#x27;num_leaves&#x27;</span>: <span class="built_in">range</span>(<span class="number">10</span>, <span class="number">80</span>, <span class="number">5</span>), <span class="string">&#x27;max_depth&#x27;</span>: <span class="built_in">range</span>(<span class="number">3</span>,<span class="number">10</span>,<span class="number">2</span>)&#125;</span><br><span class="line">get_best_cv_params()</span><br><span class="line"><span class="comment">#----------------------------------------</span></span><br><span class="line">lgb_params = &#123;<span class="string">&#x27;num_leaves&#x27;</span>: <span class="built_in">range</span>(<span class="number">25</span>, <span class="number">35</span>, <span class="number">1</span>), <span class="string">&#x27;max_depth&#x27;</span>: <span class="built_in">range</span>(<span class="number">5</span>,<span class="number">9</span>,<span class="number">1</span>)&#125;</span><br><span class="line">get_best_cv_params(n_estimators=<span class="number">85</span>)</span><br><span class="line"><span class="comment">#----------------------------------------</span></span><br><span class="line">lgb_params = &#123;<span class="string">&#x27;bagging_fraction&#x27;</span>: [i/<span class="number">10</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>,<span class="number">10</span>,<span class="number">1</span>)], </span><br><span class="line">              <span class="string">&#x27;feature_fraction&#x27;</span>: [i/<span class="number">10</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>,<span class="number">10</span>,<span class="number">1</span>)],</span><br><span class="line">              <span class="string">&#x27;bagging_freq&#x27;</span>: <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">81</span>,<span class="number">10</span>)&#125;</span><br><span class="line">get_best_cv_params(n_estimators=<span class="number">85</span>, num_leaves=<span class="number">29</span>, max_depth=<span class="number">7</span>, min_data_in_leaf=<span class="number">45</span>）</span><br><span class="line"><span class="comment">#----------------------------------------</span></span><br><span class="line">lgb_params = &#123;<span class="string">&#x27;reg_lambda&#x27;</span>: [<span class="number">0</span>,<span class="number">0.001</span>,<span class="number">0.01</span>,<span class="number">0.03</span>,<span class="number">0.08</span>,<span class="number">0.3</span>,<span class="number">0.5</span>], <span class="string">&#x27;reg_alpha&#x27;</span>: [<span class="number">0</span>,<span class="number">0.001</span>,<span class="number">0.01</span>,<span class="number">0.03</span>,<span class="number">0.08</span>,<span class="number">0.3</span>,<span class="number">0.5</span>]&#125;</span><br><span class="line">get_best_cv_params(n_estimators=<span class="number">85</span>, num_leaves=<span class="number">29</span>, max_depth=<span class="number">7</span>, min_data_in_leaf=<span class="number">45</span>, bagging_fraction=<span class="number">0.9</span>, feature_fraction=<span class="number">0.9</span>, bagging_freq=<span class="number">40</span>)</span><br><span class="line"><span class="comment">#----------------------------------------</span></span><br><span class="line">lgb_params = &#123;<span class="string">&#x27;min_split_gain&#x27;</span>: [i/<span class="number">10</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">11</span>,<span class="number">1</span>)]&#125;</span><br><span class="line">get_best_cv_params(n_estimators=<span class="number">85</span>, num_leaves=<span class="number">29</span>, max_depth=<span class="number">7</span>, min_data_in_leaf=<span class="number">45</span>, bagging_fraction=<span class="number">0.9</span>, feature_fraction=<span class="number">0.9</span>, bagging_freq=<span class="number">40</span>, min_split_gain=<span class="literal">None</span>)</span><br><span class="line"><span class="comment">#----------------------------------------</span></span><br><span class="line">final_params = &#123;</span><br><span class="line">                <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.01</span>,</span><br><span class="line">                <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">29</span>,</span><br><span class="line">                <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">7</span>,</span><br><span class="line">                <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;multiclass&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;num_class&#x27;</span>: <span class="number">4</span>,</span><br><span class="line">                <span class="string">&#x27;min_data_in_leaf&#x27;</span>:<span class="number">45</span>,</span><br><span class="line">                <span class="string">&#x27;min_child_weight&#x27;</span>:<span class="number">0.001</span>,</span><br><span class="line">                <span class="string">&#x27;bagging_fraction&#x27;</span>: <span class="number">0.9</span>,</span><br><span class="line">                <span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.9</span>,</span><br><span class="line">                <span class="string">&#x27;bagging_freq&#x27;</span>: <span class="number">40</span>,</span><br><span class="line">                <span class="string">&#x27;min_split_gain&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">                <span class="string">&#x27;reg_lambda&#x27;</span>:<span class="number">0</span>,</span><br><span class="line">                <span class="string">&#x27;reg_alpha&#x27;</span>:<span class="number">0</span>,</span><br><span class="line">                <span class="string">&#x27;nthread&#x27;</span>: <span class="number">6</span></span><br><span class="line">               &#125;</span><br><span class="line"></span><br><span class="line">cv_result = lgb.cv(train_set=lgb_train,</span><br><span class="line">                   early_stopping_rounds=<span class="number">20</span>,</span><br><span class="line">                   num_boost_round=<span class="number">5000</span>,</span><br><span class="line">                   nfold=<span class="number">5</span>,</span><br><span class="line">                   stratified=<span class="literal">True</span>,</span><br><span class="line">                   shuffle=<span class="literal">True</span>,</span><br><span class="line">                   params=final_params,</span><br><span class="line">                   feval=f1_score_vali,</span><br><span class="line">                   seed=<span class="number">0</span>,</span><br><span class="line">                  )</span><br></pre></td></tr></table></figure>
<h4 id="4-贝叶斯调参"><a href="#4-贝叶斯调参" class="headerlink" title="4. 贝叶斯调参"></a>4. 贝叶斯调参</h4><p>是一种用模型找到目标函数最小值的方法，比网格和随机搜索省时。步骤如下：</p>
<p>① 定义优化函数(rf_cv）</p>
<p>② 建立模型</p>
<p>③ 定义待优化的参数</p>
<p>④ 得到优化结果，并返回要优化的分数指标</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="comment">#定义优化函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rf_cv_lgb</span>(<span class="params">num_leaves, max_depth, bagging_fraction, feature_fraction, bagging_freq, min_data_in_leaf, </span></span></span><br><span class="line"><span class="function"><span class="params">              min_child_weight, min_split_gain, reg_lambda, reg_alpha</span>):</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立模型</span></span><br><span class="line">    model_lgb = lgb.LGBMClassifier(boosting_type=<span class="string">&#x27;gbdt&#x27;</span>, objective=<span class="string">&#x27;multiclass&#x27;</span>, num_class=<span class="number">4</span>,learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">5000</span>,num_leaves=<span class="built_in">int</span>(num_leaves), max_depth=<span class="built_in">int</span>(max_depth), bagging_fraction=<span class="built_in">round</span>(bagging_fraction, <span class="number">2</span>), feature_fraction=<span class="built_in">round</span>(feature_fraction, <span class="number">2</span>),bagging_freq=<span class="built_in">int</span>(bagging_freq), min_data_in_leaf=<span class="built_in">int</span>(min_data_in_leaf),min_child_weight=min_child_weight)</span><br><span class="line">    f1 = make_scorer(f1_score, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">    val = cross_val_score(model_lgb, X_train_split, y_train_split, cv=<span class="number">5</span>, scoring=f1).mean()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> val</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bayes_opt <span class="keyword">import</span> BayesianOptimization</span><br><span class="line"><span class="comment">#定义优化参数</span></span><br><span class="line">bayes_lgb = BayesianOptimization(</span><br><span class="line">    rf_cv_lgb, </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&#x27;num_leaves&#x27;</span>:(<span class="number">10</span>, <span class="number">200</span>),</span><br><span class="line">        <span class="string">&#x27;max_depth&#x27;</span>:(<span class="number">3</span>, <span class="number">20</span>),</span><br><span class="line">        <span class="string">&#x27;bagging_fraction&#x27;</span>:(<span class="number">0.5</span>, <span class="number">1.0</span>),</span><br><span class="line">        <span class="string">&#x27;feature_fraction&#x27;</span>:(<span class="number">0.5</span>, <span class="number">1.0</span>),</span><br><span class="line">        <span class="string">&#x27;bagging_freq&#x27;</span>:(<span class="number">0</span>, <span class="number">100</span>),</span><br><span class="line">        <span class="string">&#x27;min_data_in_leaf&#x27;</span>:(<span class="number">10</span>,<span class="number">100</span>),</span><br><span class="line">        <span class="string">&#x27;min_child_weight&#x27;</span>:(<span class="number">0</span>, <span class="number">10</span>),</span><br><span class="line">        <span class="string">&#x27;min_split_gain&#x27;</span>:(<span class="number">0.0</span>, <span class="number">1.0</span>),</span><br><span class="line">        <span class="string">&#x27;reg_alpha&#x27;</span>:(<span class="number">0.0</span>, <span class="number">10</span>),</span><br><span class="line">        <span class="string">&#x27;reg_lambda&#x27;</span>:(<span class="number">0.0</span>, <span class="number">10</span>),</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#开始优化</span></span><br><span class="line">bayes_lgb.maximize(n_iter=<span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#显示优化结果</span></span><br><span class="line">bayes_lgb.<span class="built_in">max</span></span><br></pre></td></tr></table></figure>

<p>参数优化完成后，可根据优化后的参数建立新的模型，降低学习率并寻找最优模型迭代次数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#设置较小的学习率，并通过cv函数确定当前最优的迭代次数</span></span><br><span class="line">base_params_lgb = &#123;</span><br><span class="line">                    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">                    <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;multiclass&#x27;</span>,</span><br><span class="line">                    <span class="string">&#x27;num_class&#x27;</span>: <span class="number">4</span>,</span><br><span class="line">                    <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.01</span>,</span><br><span class="line">                    <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">138</span>,</span><br><span class="line">                    <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">11</span>,</span><br><span class="line">                    <span class="string">&#x27;min_data_in_leaf&#x27;</span>: <span class="number">43</span>,</span><br><span class="line">                    <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">6.5</span>,</span><br><span class="line">                    <span class="string">&#x27;bagging_fraction&#x27;</span>: <span class="number">0.64</span>,</span><br><span class="line">                    <span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.93</span>,</span><br><span class="line">                    <span class="string">&#x27;bagging_freq&#x27;</span>: <span class="number">49</span>,</span><br><span class="line">                    <span class="string">&#x27;reg_lambda&#x27;</span>: <span class="number">7</span>,</span><br><span class="line">                    <span class="string">&#x27;reg_alpha&#x27;</span>: <span class="number">0.21</span>,</span><br><span class="line">                    <span class="string">&#x27;min_split_gain&#x27;</span>: <span class="number">0.288</span>,</span><br><span class="line">                    <span class="string">&#x27;nthread&#x27;</span>: <span class="number">10</span>,</span><br><span class="line">                    <span class="string">&#x27;verbose&#x27;</span>: -<span class="number">1</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">cv_result_lgb = lgb.cv(</span><br><span class="line">    train_set=train_matrix,</span><br><span class="line">    early_stopping_rounds=<span class="number">1000</span>, </span><br><span class="line">    num_boost_round=<span class="number">20000</span>,</span><br><span class="line">    nfold=<span class="number">5</span>,</span><br><span class="line">    stratified=<span class="literal">True</span>,</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    params=base_params_lgb,</span><br><span class="line">    feval=f1_score_vali,</span><br><span class="line">    seed=<span class="number">0</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;迭代次数&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(cv_result_lgb[<span class="string">&#x27;f1_score-mean&#x27;</span>])))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;最终模型的f1为&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">max</span>(cv_result_lgb[<span class="string">&#x27;f1_score-mean&#x27;</span>])))</span><br></pre></td></tr></table></figure>

<p>模型参数确定之后，建立最终模型并对验证集进行验证。</p>
]]></content>
      <categories>
        <category>Machine learning</category>
      </categories>
      <tags>
        <tag>Grid search</tag>
        <tag>Bayesian optimization</tag>
        <tag>Cross validation</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas的一些常用操作_1</title>
    <url>/2021/02/21/pandas_1/</url>
    <content><![CDATA[<blockquote>
<p>今天介绍几个常用的Pandas操作。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;./economics.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="1-DataFrame-to-markdown-latex"><a href="#1-DataFrame-to-markdown-latex" class="headerlink" title="1.DataFrame to markdown/latex"></a>1.DataFrame to markdown/latex</h4><p>dataframe可以转换为许多常用格式，如csv,excel,sql,json,html,latex等等，这里以markdown和latex为例。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(df.to_markdown())</span><br><span class="line"><span class="built_in">print</span>(df.to_latex())</span><br></pre></td></tr></table></figure>
<p>or</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.to_markdown(<span class="string">&#x27;table.md&#x27;</span>)</span><br><span class="line">df.to_latex(<span class="string">&#x27;table.tex&#x27;</span>)</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<p>也可以自定义输出latex格式，如表格宽度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.to_latex(<span class="string">&#x27;tb.tex&#x27;</span>,column_format=<span class="string">&#x27;lp&#123;1.8cm&#125;p&#123;1.8cm&#125;p&#123;1.8cm&#125;p&#123;1.8cm&#125;p&#123;1.8cm&#125;p&#123;1.8cm&#125;&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>除此以外，dataframe还可以保存为图片。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> dataframe_image <span class="keyword">as</span> dfi</span><br><span class="line">dfi.export(obj = df, filename = <span class="string">&#x27;table.jpg&#x27;</span>, fontsize=<span class="number">15</span>)</span><br></pre></td></tr></table></figure>
<h4 id="2-DataFrame常用属性查询"><a href="#2-DataFrame常用属性查询" class="headerlink" title="2.DataFrame常用属性查询"></a>2.DataFrame常用属性查询</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.values	<span class="comment">#值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.index	<span class="comment">#索引号</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.columns	<span class="comment">#列标签</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.dtypes	<span class="comment">#数据类型</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.shape	<span class="comment">#形状(几行几列)</span></span><br></pre></td></tr></table></figure>
<h4 id="3-DataFrame常用基本函数"><a href="#3-DataFrame常用基本函数" class="headerlink" title="3.DataFrame常用基本函数"></a>3.DataFrame常用基本函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.head(<span class="number">5</span>)	<span class="comment">#前5行</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.tail(<span class="number">5</span>)	<span class="comment">#后5行</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.info()	<span class="comment">#信息概况</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.describe()	<span class="comment">#主要统计量(count、mean、std、max、min、quartile)</span></span><br></pre></td></tr></table></figure>
<h4 id="4-DataFrame唯一值函数"><a href="#4-DataFrame唯一值函数" class="headerlink" title="4.DataFrame唯一值函数"></a>4.DataFrame唯一值函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].unique()	<span class="comment">#唯一值组成的数组</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].nunique()    <span class="comment">#唯一值个数</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].value_counts()    <span class="comment">#唯一值及其频数</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.describe()    <span class="comment">#主要统计量(count、mean、std、max、min、quartile)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].duplicated()    <span class="comment">#重复行的布尔值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[df[<span class="string">&#x27;psavert&#x27;</span>].duplicated()]    <span class="comment">#单列去重(删除重复行)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.drop_duplicates(subset=[<span class="string">&#x27;psavert&#x27;</span>, <span class="string">&#x27;pop&#x27;</span>], keep=<span class="string">&#x27;first&#x27;</span>)    <span class="comment">#多列去重(保留first唯一值)</span></span><br></pre></td></tr></table></figure>
<h4 id="5-DataFrame替换函数"><a href="#5-DataFrame替换函数" class="headerlink" title="5.DataFrame替换函数"></a>5.DataFrame替换函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].replace(<span class="number">12.5</span>, <span class="string">&#x27;A&#x27;</span>, inplace = <span class="literal">True</span>)    <span class="comment">#替换某列的单个值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].replace(&#123;<span class="number">12.5</span>:<span class="string">&#x27;A&#x27;</span>, <span class="number">11.7</span>:<span class="string">&#x27;B&#x27;</span>&#125;, inplace = <span class="literal">True</span>)    <span class="comment">#替换某列的多个值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;date&#x27;</span>].replace(&#123;<span class="string">r&#x27;2\d+&#x27;</span>: <span class="string">&#x27;The 21st century&#x27;</span>&#125;, regex=<span class="literal">True</span>, inplace = <span class="literal">True</span>)    <span class="comment">#正则替换</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].mask(df[<span class="string">&#x27;psavert&#x27;</span>]&gt;<span class="number">12.0</span> ,<span class="string">&#x27;A&#x27;</span>, inplace = <span class="literal">True</span>)    <span class="comment">#条件符合，进行替换</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].where(df[<span class="string">&#x27;psavert&#x27;</span>]&lt;<span class="number">12.0</span> ,<span class="string">&#x27;A&#x27;</span>, inplace = <span class="literal">True</span>)    <span class="comment">#条件不符合，进行替换</span></span><br></pre></td></tr></table></figure>
<h4 id="5-DataFrame排序函数"><a href="#5-DataFrame排序函数" class="headerlink" title="5.DataFrame排序函数"></a>5.DataFrame排序函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.sort_values(<span class="string">&#x27;psavert&#x27;</span>,ascending = <span class="literal">False</span>)    <span class="comment">#单列降序排序</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.sort_values([<span class="string">&#x27;psavert&#x27;</span>,<span class="string">&#x27;uempmed&#x27;</span>],ascending=[<span class="literal">True</span>,<span class="literal">False</span>])    <span class="comment">#前者升序情况下，后降序</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>今天先写到这，下一期接着写DataFrame的apply方法。</p>
</blockquote>
]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>DataFrame</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas的一些常用操作_2</title>
    <url>/2021/02/21/pandas_2/</url>
    <content><![CDATA[<blockquote>
<p>今天继续介绍几个常用的Pandas操作。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;./economics.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="1-DataFrame的apply方法"><a href="#1-DataFrame的apply方法" class="headerlink" title="1.DataFrame的apply方法"></a>1.DataFrame的apply方法</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[[<span class="string">&#x27;psavert&#x27;</span>,<span class="string">&#x27;uempmed&#x27;</span>]].apply(<span class="keyword">lambda</span> x:x.<span class="built_in">max</span>()-x.<span class="built_in">min</span>(), axis=<span class="number">1</span>)<span class="comment">#axis=1 将函数应用到列</span></span><br></pre></td></tr></table></figure>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.applymap(<span class="keyword">lambda</span> x:x*<span class="number">10</span>)<span class="comment">#applymap 将函数应用到每个元素</span></span><br></pre></td></tr></table></figure>
<h4 id="2-DataFrame的分组"><a href="#2-DataFrame的分组" class="headerlink" title="2.DataFrame的分组"></a>2.DataFrame的分组</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupby(<span class="string">&#x27;unemploy&#x27;</span>)[<span class="string">&#x27;psavert&#x27;</span>].median()<span class="comment">#样例:df.groupby(分组依据)[数据来源].使用操作</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">condition = df.unemploy &gt; df.unemploy.mean()<span class="comment">#使用condition定义分组依据</span></span><br><span class="line">df.groupby(condition)[<span class="string">&#x27;psavert&#x27;</span>].mean()<span class="comment">#分为True和False两组</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupby([condition, df[<span class="string">&#x27;unemploy&#x27;</span>]])[<span class="string">&#x27;psavert&#x27;</span>].mean()<span class="comment">#True组和False两组分别细分</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupby([df[<span class="string">&#x27;unemploy&#x27;</span>], df[<span class="string">&#x27;uempmed&#x27;</span>]])[<span class="string">&#x27;psavert&#x27;</span>].mean()<span class="comment">#两级分组</span></span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gb = df.groupby([<span class="string">&#x27;unemploy&#x27;</span>])<span class="comment">#Groupby对象</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gb.size()<span class="comment">#每组的元素个数</span></span><br><span class="line"><span class="comment">#和DataFrame一样，Groupby对象也有max\idxmin\all\\nunique\quantile\prod等函数，这里不一一列举。</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gb.agg([<span class="string">&#x27;skew&#x27;</span>, <span class="string">&#x27;sum&#x27;</span>, <span class="string">&#x27;idxmax&#x27;</span>])<span class="comment">#agg聚合函数，查看每个分组的三个统计量</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gb.agg(<span class="keyword">lambda</span> x: x.mean()-x.<span class="built_in">min</span>())<span class="comment">#在agg中自定义函数</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gb.<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x.shape[<span class="number">0</span>] &gt; <span class="number">100</span>)<span class="comment">#组过滤</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gb.apply(<span class="keyword">lambda</span> x: x**<span class="number">2</span>)<span class="comment">#组的apply方法</span></span><br></pre></td></tr></table></figure>
<h4 id="3-DataFrame的连接"><a href="#3-DataFrame的连接" class="headerlink" title="3.DataFrame的连接"></a>3.DataFrame的连接</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df1 = df[<span class="number">0</span>:<span class="number">10</span>]</span><br><span class="line">df2 = df[<span class="number">10</span>:<span class="number">20</span>]</span><br><span class="line">df1.merge(df2, on=<span class="string">&#x27;date&#x27;</span>, how=<span class="string">&#x27;outer&#x27;</span>)<span class="comment">#merge表示关系型连接，包括左连接、右连接、内连接和外(全)连接</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.concat([df1, df2], axis=<span class="number">0</span>)<span class="comment">#concat是方向性连接，axis=0表示纵向连接</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_min</span>(<span class="params">x1, x2</span>):</span></span><br><span class="line">    <span class="built_in">min</span> = x1.where(x1&lt;x2, x1)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">min</span></span><br><span class="line">df1.combine(df2, choose_min)<span class="comment">#使用combine函数自定义连接规则</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>下一期是Pandas的常见数据处理，包括缺失数据、文本数据、分类数据和时序数据。</p>
</blockquote>
]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>DataFrame</tag>
      </tags>
  </entry>
  <entry>
    <title>Tsfresh——自动化特征工程工具</title>
    <url>/2021/03/25/tsfresh/</url>
    <content><![CDATA[<blockquote>
<p>改进模型的潜在途径之一是：生成更多的潜在特征，输入更多的样本。</p>
</blockquote>
<p>Tsfresh是处理时间序列数据的特征工程工具，能够自动计算大量时间序列特征，如平均值、最大值、峰度等。之后，可以使用这些特征集构建机器学习模型。</p>
<p>本文以<em>天池-心跳信号分类预测</em>为例，演示tsfresh工具的用法。</p>
<h2 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h2><h5 id="1-合并train和test数据"><a href="#1-合并train和test数据" class="headerlink" title="1. 合并train和test数据"></a>1. 合并train和test数据</h5><p>合并数据集，对整体数据做统一的特征工程。(注意需要为test数据添加label列，值为-1，方便后续操作)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_test[<span class="string">&#x27;label&#x27;</span>] = -<span class="number">1</span></span><br><span class="line">all_data = pd.concat((data_train, data_test)).reset_index(drop = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h5 id="2-对原特征一列拆成多列，并为每条数据添加时间特征time"><a href="#2-对原特征一列拆成多列，并为每条数据添加时间特征time" class="headerlink" title="2. 对原特征一列拆成多列，并为每条数据添加时间特征time"></a>2. 对原特征一列拆成多列，并为每条数据添加时间特征time</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_heatbeat_df = all_data[<span class="string">&#x27;heartbeat_signals&#x27;</span>].<span class="built_in">str</span>.split(<span class="string">&#x27;,&#x27;</span>, expand = <span class="literal">True</span>).stack()</span><br></pre></td></tr></table></figure>
<h5 id="3-Index处理"><a href="#3-Index处理" class="headerlink" title="3. Index处理"></a>3. Index处理</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_heatbeat_df = all_heatbeat_df.reset_inex()</span><br><span class="line">all_heatbeat_df = all_heatbeat_df.set_inex(<span class="string">&#x27;level_0&#x27;</span>)</span><br><span class="line">all_heatbeat_df.index.name = <span class="literal">None</span></span><br><span class="line">all_heatbeat_df.rename(columns=&#123;<span class="string">&#x27;level_1&#x27;</span>:<span class="string">&#x27;time&#x27;</span>, <span class="number">0</span>:<span class="string">&#x27;heartbeat_signals&#x27;</span>, inpalce = <span class="literal">True</span>&#125;)</span><br><span class="line">all_heatbeat_df[<span class="string">&#x27;heartbeat_signals&#x27;</span>].astype(<span class="built_in">float</span>)</span><br></pre></td></tr></table></figure>

<h5 id="4-label列单独存储，不进入tsfresh"><a href="#4-label列单独存储，不进入tsfresh" class="headerlink" title="4. label列单独存储，不进入tsfresh"></a>4. label列单独存储，不进入tsfresh</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_data_label = all_data[<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">all_data = all_data.drop[<span class="string">&#x27;label&#x27;</span>, axis=<span class="number">1</span>].drop[<span class="string">&#x27;heartbeat_signals&#x27;</span>, axis=<span class="number">1</span>]</span><br><span class="line">all_data = all_data.join(all_heatbeat_df)</span><br></pre></td></tr></table></figure>

<h5 id="5-tsfresh特征抽取"><a href="#5-tsfresh特征抽取" class="headerlink" title="5. tsfresh特征抽取"></a>5. tsfresh特征抽取</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tsfresh <span class="keyword">import</span> extract_features</span><br><span class="line">all_features = extract_features(all_data, column_id=<span class="string">&#x27;id&#x27;</span>, column_sort=<span class="string">&#x27;time&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h5 id="6-特征选择"><a href="#6-特征选择" class="headerlink" title="6.特征选择"></a>6.特征选择</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#删除nan值</span></span><br><span class="line"><span class="keyword">from</span> tsfresh.utilities.dataframe_functions <span class="keyword">import</span> impute</span><br><span class="line">impute(all_features)</span><br></pre></td></tr></table></figure>

<h5 id="7-相关性特征提取"><a href="#7-相关性特征提取" class="headerlink" title="7.相关性特征提取"></a>7.相关性特征提取</h5><p>衍生众多特征之后，许多特征之间可能有很多相关性，需进一步筛选。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tsfresh <span class="keyword">import</span> select_features</span><br><span class="line">all_features_filtered = select_features(all_features, all_data_label)</span><br></pre></td></tr></table></figure>
<h5 id="8-特征重命名，重新添加label"><a href="#8-特征重命名，重新添加label" class="headerlink" title="8.特征重命名，重新添加label"></a>8.特征重命名，重新添加label</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num = all_features_filtered.columns.size</span><br><span class="line">all_features_filtered.columns = [<span class="string">&#x27;f_&#x27;</span> + <span class="built_in">str</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span> (<span class="number">0</span>, num)]</span><br><span class="line">all_features_filtered[<span class="string">&#x27;label&#x27;</span>] = all_data_label</span><br></pre></td></tr></table></figure>

<h4 id="tsfresh包深入探究"><a href="#tsfresh包深入探究" class="headerlink" title="tsfresh包深入探究"></a>tsfresh包深入探究</h4><h5 id="1-筛选特征的方法"><a href="#1-筛选特征的方法" class="headerlink" title="1. 筛选特征的方法"></a>1. 筛选特征的方法</h5><p>上文采用了手工打标签的方式划分训练集和测试集，略显麻烦，可以通过tsfresh的内置方法来提取训练数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">kind_to_fc_parameters = tsfresh.feature_extration.settings.from_columns(directed_features)</span><br></pre></td></tr></table></figure>
<p>生成的params为训练集的特征字典，可以当作参数传入后续计算中。</p>
<h5 id="2-自定义特征衍生规则"><a href="#2-自定义特征衍生规则" class="headerlink" title="2. 自定义特征衍生规则"></a>2. 自定义特征衍生规则</h5><p>tsfresh自带的衍生规则以字典的形式存放，可以直接调用，也可以自定义。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#单个特征计算</span></span><br><span class="line">tsfresh.feature_extraction.feature_calculators.abs_energy(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">#设定衍生规则</span></span><br><span class="line"><span class="keyword">from</span> tsfresh.featureextraction <span class="keyword">import</span> extractfeatures</span><br><span class="line">params = &#123;<span class="string">&#x27;fft_coefficient:[&#123;&#x27;</span>coe<span class="string">f&#x27;:0, &#x27;</span>att<span class="string">r&#x27;:&#x27;</span><span class="built_in">abs</span><span class="string">&#x27;&#125;], &#x27;</span>kurtosis<span class="string">&#x27;: None, &#x27;</span>skewness<span class="string">&#x27;: None&#125;</span></span><br></pre></td></tr></table></figure>

<p>后续可以在extractfeatures中设置defaultparameters = params，具体用法见<a href="https://tsfresh.readthedocs.io/en/latest/text/feature_extraction_settings.html">操作文档</a>。</p>
<h5 id="3-减小内存使用"><a href="#3-减小内存使用" class="headerlink" title="3. 减小内存使用"></a>3. 减小内存使用</h5><p>tsfresh默认参数太吃内存，且耗时长，笔者i7的16G笔记本最多只能跑到60%的进度就会卡住，天池notebook和谷歌colab都是没跑出结果就断线了，硬要跑的话只能租个高配服务器，或者调整衍生特征的数量，通过<strong>chunksize</strong>设置，具体用法见<a href="https://tsfresh.readthedocs.io/en/latest/text/feature_extraction_settings.html">操作文档</a>。</p>
]]></content>
      <categories>
        <category>Feature engineering</category>
      </categories>
      <tags>
        <tag>tsfresh</tag>
      </tags>
  </entry>
  <entry>
    <title>模型融合_1</title>
    <url>/2021/03/15/%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88_1/</url>
    <content><![CDATA[<p>Kaggle和天池比赛中常用提高成绩的三种方法：</p>
<blockquote>
<p>1.特征工程<br>2.模型调参<br>3.模型融合<br>模型融合主要有以下几种方式：</p>
</blockquote>
<h4 id="简单加权融合"><a href="#简单加权融合" class="headerlink" title="简单加权融合:"></a>简单加权融合:</h4><blockquote>
<p>①回归（分类概率）：算术平均融合（Arithmetic mean），几何平均融合（Geometric mean）；<br>②分类：投票（Voting)<br>③综合：排序融合(Rank averaging)，log融合</p>
</blockquote>
<h4 id="stacking-blending"><a href="#stacking-blending" class="headerlink" title="stacking/blending:"></a>stacking/blending:</h4><blockquote>
<p>构建多层模型，把初级学习器的输出当作下一层的输入。</p>
</blockquote>
<span id="more"></span>
<h4 id="boosting-bagging（在xgboost-Adaboost-GBDT中已经用到）"><a href="#boosting-bagging（在xgboost-Adaboost-GBDT中已经用到）" class="headerlink" title="boosting/bagging（在xgboost,Adaboost,GBDT中已经用到）:"></a>boosting/bagging（在xgboost,Adaboost,GBDT中已经用到）:</h4><blockquote>
<p>多个分类器的整合</p>
</blockquote>
<h4 id="部分代码案例"><a href="#部分代码案例" class="headerlink" title="部分代码案例:"></a>部分代码案例:</h4><h5 id="1-简单加权平均"><a href="#1-简单加权平均" class="headerlink" title="1.简单加权平均"></a>1.简单加权平均</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Weighted_method</span>(<span class="params">test_pre1,test_pre2,test_pre3,w=[<span class="number">1</span>/<span class="number">3</span>,<span class="number">1</span>/<span class="number">3</span>,<span class="number">1</span>/<span class="number">3</span>]</span>):</span></span><br><span class="line">    Weighted_result = w[<span class="number">0</span>]*pd.Series(test_pre1)+w[<span class="number">1</span>]*pd.Series(test_pre2)+w[<span class="number">2</span>]*pd.Series(test_pre3)</span><br><span class="line">    <span class="keyword">return</span> Weighted_result</span><br><span class="line">    </span><br><span class="line">Weighted_pre = Weighted_method(test_pre1,test_pre2,test_pre3,w)</span><br></pre></td></tr></table></figure>
<h5 id="2-Stacking融合-回归"><a href="#2-Stacking融合-回归" class="headerlink" title="2.Stacking融合(回归)"></a>2.Stacking融合(回归)</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Stacking_method</span>(<span class="params">train_reg1,train_reg2,train_reg3,y_train_true,test_pre1,test_pre2,test_pre3,model_L2= linear_model.LinearRegression(<span class="params"></span>)</span>):</span>    model_L2.fit(pd.concat([pd.Series(train_reg1),pd.Series(train_reg2),pd.Series(train_reg3)],axis=<span class="number">1</span>).values,y_train_true)</span><br><span class="line">    Stacking_result = model_L2.predict(pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=<span class="number">1</span>).values)</span><br><span class="line">    <span class="keyword">return</span> Stacking_result</span><br><span class="line"></span><br><span class="line">Stacking_pre = Stacking_method(train_reg1,train_reg2,train_reg3,y_train_true,</span><br><span class="line">                               test_pre1,test_pre2,test_pre3,model_L2)</span><br></pre></td></tr></table></figure>
<h5 id="3-Voting投票机制"><a href="#3-Voting投票机制" class="headerlink" title="3.Voting投票机制"></a>3.Voting投票机制</h5><blockquote>
<p>硬投票：对多个模型直接进行投票，不区分模型结果的相对重要度，最终投票数最多的类为最终被预测的类。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eclf = VotingClassifier(estimators=[(<span class="string">&#x27;lgb&#x27;</span>, clf1), (<span class="string">&#x27;rf&#x27;</span>, clf2), (<span class="string">&#x27;svc&#x27;</span>, clf3)], voting=<span class="string">&#x27;hard&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> clf, label <span class="keyword">in</span> <span class="built_in">zip</span>([clf1, clf2, clf3, eclf], [<span class="string">&#x27;LGB&#x27;</span>, <span class="string">&#x27;Random Forest&#x27;</span>, <span class="string">&#x27;SVM&#x27;</span>, <span class="string">&#x27;Ensemble&#x27;</span>]):</span><br><span class="line">    scores = cross_val_score(clf, x, y, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Accuracy: %0.2f (+/- %0.2f) [%s]&quot;</span> % (scores.mean(), scores.std(), label))</span><br></pre></td></tr></table></figure>
<h5 id="4-分类模型的Stacking融合"><a href="#4-分类模型的Stacking融合" class="headerlink" title="4.分类模型的Stacking融合"></a>4.分类模型的Stacking融合</h5><blockquote>
<p>Stacking与Blending相比存在一定优势:<br>1.充分使用数据<br>2.使用多次的交叉验证会比较稳健<br>3.不容易过拟合</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clfs = [LogisticRegression(solver=<span class="string">&#x27;lbfgs&#x27;</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;entropy&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j, clf <span class="keyword">in</span> <span class="built_in">enumerate</span>(clfs):</span><br><span class="line">    <span class="comment">#依次训练各个单模型</span></span><br><span class="line">    clf.fit(X_d1, y_d1)</span><br><span class="line">    y_submission = clf.predict_proba(X_d2)[:, <span class="number">1</span>]</span><br><span class="line">    dataset_d1[:, j] = y_submission</span><br><span class="line">    <span class="comment">#对于测试集，直接用这k个模型的预测值作为新的特征。</span></span><br><span class="line">    dataset_d2[:, j] = clf.predict_proba(X_predict)[:, <span class="number">1</span>]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;val auc Score: %f&quot;</span> % roc_auc_score(y_predict, dataset_d2[:, j]))</span><br><span class="line"></span><br><span class="line"><span class="comment">#融合使用的模型</span></span><br><span class="line">clf = GradientBoostingClassifier(learning_rate=<span class="number">0.02</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">30</span>)</span><br><span class="line">clf.fit(dataset_d1, y_d2)</span><br></pre></td></tr></table></figure>
<h5 id="5-其他Stacking"><a href="#5-其他Stacking" class="headerlink" title="5.其他Stacking"></a>5.其他Stacking</h5><blockquote>
<p>将特征放进模型中预测，并将预测结果变换并作为新的特征加入原有特征中再经过模型预测结果</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Ensemble_add_feature</span>(<span class="params">train,test,target,clfs</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> j,clf <span class="keyword">in</span> <span class="built_in">enumerate</span>(clfs):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;依次训练各个单模型&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># print(j, clf)</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;使用第1个部分作为预测，第2部分来训练模型，获得其预测的输出作为第2部分的新特征。&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]</span></span><br><span class="line"></span><br><span class="line">        clf.fit(train,target)</span><br><span class="line">        y_train = clf.predict(train)</span><br><span class="line">        y_test = clf.predict(test)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## 新特征生成</span></span><br><span class="line">        train_[:,j*<span class="number">2</span>] = y_train**<span class="number">2</span></span><br><span class="line">        test_[:,j*<span class="number">2</span>] = y_test**<span class="number">2</span></span><br><span class="line">        train_[:, j+<span class="number">1</span>] = np.exp(y_train)</span><br><span class="line">        test_[:, j+<span class="number">1</span>] = np.exp(y_test)</span><br><span class="line">        <span class="comment"># print(&quot;val auc Score: %f&quot; % r2_score(y_predict, dataset_d2[:, j]))</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Method &#x27;</span>,j)</span><br><span class="line"></span><br><span class="line">    train_ = pd.DataFrame(train_)</span><br><span class="line">    test_ = pd.DataFrame(test_)</span><br><span class="line">    <span class="keyword">return</span> train_,test_</span><br><span class="line"></span><br><span class="line"><span class="comment">#模型融合中使用到的各个单模型</span></span><br><span class="line">clfs = [LogisticRegression(),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;entropy&#x27;</span>),</span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"></span><br><span class="line">New_train,New_test = Ensemble_add_feature(x_train,x_test,y_train,clfs)</span><br><span class="line">clf = GradientBoostingClassifier(learning_rate=<span class="number">0.02</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">30</span>)</span><br><span class="line">clf.fit(New_train, y_train)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Machine learning</category>
      </categories>
      <tags>
        <tag>voting</tag>
        <tag>Stacking</tag>
        <tag>Blending</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas的一些常用操作_3</title>
    <url>/2021/02/21/pandas_3/</url>
    <content><![CDATA[<blockquote>
<p>今天介绍Pandas对一些常见数据的处理方法。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;./economics.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="1-缺失数据处理"><a href="#1-缺失数据处理" class="headerlink" title="1.缺失数据处理"></a>1.缺失数据处理</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.isna()<span class="comment">#是否有缺失值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.isna().mean()<span class="comment">#缺失的比例 </span></span><br></pre></td></tr></table></figure>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[df.psavert.isna()]<span class="comment">#查看某列是否有缺失值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[df[[<span class="string">&#x27;psavert&#x27;</span>,<span class="string">&#x27;pop&#x27;</span>,<span class="string">&#x27;uempmed&#x27;</span>]].isna().<span class="built_in">any</span>(<span class="number">1</span>)]<span class="comment">#查看所有列至少有一个缺失值的行“any()至少有一个为空,all()都为空”</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[df[[<span class="string">&#x27;psavert&#x27;</span>,<span class="string">&#x27;pop&#x27;</span>,<span class="string">&#x27;uempmed&#x27;</span>]].notna().<span class="built_in">all</span>(<span class="number">1</span>)]<span class="comment">#查看所有没有缺失值的行</span></span><br><span class="line">df.loc[df[[<span class="string">&#x27;psavert&#x27;</span>,<span class="string">&#x27;pop&#x27;</span>,<span class="string">&#x27;uempmed&#x27;</span>]].notna().<span class="built_in">all</span>(<span class="number">1</span>)]<span class="comment">#查看所有没有缺失值的区域</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.dropna(axis = <span class="number">0</span>, how = <span class="string">&#x27;any&#x27;</span>, subset = [<span class="string">&#x27;psavert&#x27;</span>, <span class="string">&#x27;pop&#x27;</span>])<span class="comment">#axis=0(删除)行,how=&#x27;any&#x27;至少有一个缺失的行</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.dropna(axis = <span class="number">1</span>, thresh = df.shape[<span class="number">0</span>]-<span class="number">5</span>)<span class="comment">#删除超过5个缺失值的列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.fillna(method = <span class="string">&#x27;ffill&#x27;</span>, limit = <span class="number">1</span>)<span class="comment">#method=&#x27;ffill&#x27;用前面的元素填充/method=&#x27;bfill&#x27;用后面的元素填充,limit=1连续缺失值的最大填充次数为1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.fillna(df.mean())<span class="comment">#用每列的均值填充</span></span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.interpolate(limit_direction=<span class="string">&#x27;both&#x27;</span>, limit=<span class="number">1</span>)<span class="comment">#用线性插值填充(both为双向限制插值)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.interpolate(<span class="string">&#x27;nearest&#x27;</span>).values<span class="comment">#用最近邻插值填充</span></span><br></pre></td></tr></table></figure>
<p>此外，也可以<a href="https://blog.csdn.net/wj1298250240/article/details/103600075">使用KNN来填充缺失值</a>。</p>
<h4 id="2-文本数据处理"><a href="#2-文本数据处理" class="headerlink" title="2.文本数据处理"></a>2.文本数据处理</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts = pd.Series(df[<span class="string">&#x27;Area&#x27;</span>].values, index=df[<span class="string">&#x27;pct_2014&#x27;</span>])<span class="comment">#DataFrame转换为Series</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>[<span class="number">0</span>]<span class="comment">#查看第一个字符</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.<span class="built_in">len</span>()<span class="comment">#查看字符长度</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.split(<span class="string">&#x27;[aon]&#x27;</span>, n=<span class="number">3</span>, expand=<span class="literal">True</span>)<span class="comment">#从左到右拆分字符串，最大拆分次数3次，生成多列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.join(<span class="string">&#x27;-&#x27;</span>)<span class="comment">#每个字符用“-”连接</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.cat(ts2, sep=<span class="string">&#x27;-&#x27;</span>)<span class="comment">#合并两个字符Series，连接符为“-”</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.contains(<span class="string">&#x27;[a-z]u&#x27;</span>)<span class="comment">#查看包含正则模式的序列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.startswith(<span class="string">&#x27;A&#x27;</span>)<span class="comment">#查看以A为开始的序列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.endswith(<span class="string">&#x27;e&#x27;</span>)<span class="comment">#查看以e为结尾的序列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.match(<span class="string">&#x27;A|s&#x27;</span>)<span class="comment">#查看以A为开头,s为结尾的序列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.count(<span class="string">&#x27;[A|s]&#x27;</span>)<span class="comment">#查看以A为开头,s为结尾的序列数量</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.find(<span class="string">&#x27;de&#x27;</span>)<span class="comment">#从左往右寻找&#x27;de&#x27;,匹配返回位置索引,未找到返回&#x27;-1&#x27;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.rfind(<span class="string">&#x27;de&#x27;</span>)<span class="comment">#从右往左寻找</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.replace(<span class="string">&#x27;\s?&#x27;</span>, <span class="string">&#x27;LA&#x27;</span>, regex=<span class="literal">True</span>)<span class="comment">#使用正则进行字符串替换</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pat = <span class="string">&#x27;(\w+o)(\w+s)(\w+u)(\w+n)&#x27;</span></span><br><span class="line">ts.<span class="built_in">str</span>.extract(pat)<span class="comment">#拆分&#x27;o&#x27;,&#x27;s&#x27;,&#x27;u&#x27;,&#x27;n&#x27;为4列</span></span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.to_numeric(ts, errors=<span class="string">&#x27;ignore&#x27;</span>)<span class="comment">#将可以转为数值的字符转为数值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.pad(<span class="number">6</span>,<span class="string">&#x27;left&#x27;</span>,<span class="string">&#x27;*&#x27;</span>)<span class="comment">#选定字符串长度为6的填充为&quot;*&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.lstrip()<span class="comment">#去掉字符串左侧空格</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.zfill(<span class="number">8</span>)<span class="comment">#用0补足8位</span></span><br></pre></td></tr></table></figure>
<h4 id="3-分类数据"><a href="#3-分类数据" class="headerlink" title="3.分类数据"></a>3.分类数据</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = df.psavert.astype(<span class="string">&#x27;category&#x27;</span>)<span class="comment">#Dataframe转为category对象</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s.cat.categories<span class="comment">#查看分类对象属性</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s.cat.add_categories(<span class="string">&#x27;C1&#x27;</span>)<span class="comment">#增加一个类别</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s.cat.remove_categories(<span class="number">11.7</span>)<span class="comment">#删除一个类别</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = s.cat.rename_categories(&#123;<span class="string">&#x27;S1&#x27;</span>:<span class="string">&#x27;xxx&#x27;</span>&#125;)<span class="comment">#重命名类别及其值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s.cat.reorder_categories([<span class="string">&#x27;S1&#x27;</span>, <span class="string">&#x27;S2&#x27;</span>, <span class="string">&#x27;S3&#x27;</span>, <span class="string">&#x27;S4&#x27;</span>], ordered=<span class="literal">True</span>)<span class="comment">#设置排序</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.sort_values(<span class="string">&#x27;psavert&#x27;</span>)<span class="comment">#值排序</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.set_index(<span class="string">&#x27;psavert&#x27;</span>).sort_index()<span class="comment">#作为索引排序</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">res = df.psavert &lt;= <span class="string">&#x27;S4&#x27;</span><span class="comment">#比较顺序</span></span><br><span class="line">res</span><br></pre></td></tr></table></figure>
<h4 id="4-时序数据"><a href="#4-时序数据" class="headerlink" title="4.时序数据"></a>4.时序数据</h4><p>————-图——————–</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = pd.to_datetime(df.date)<span class="comment">#生成时间序列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s2 = pd.to_datetime([<span class="string">&#x27;2020\\1\\1&#x27;</span>,<span class="string">&#x27;2020\\1\\3&#x27;</span>],<span class="built_in">format</span>=<span class="string">&#x27;%Y\\%m\\%d&#x27;</span>)<span class="comment">#强制格式转换,生成时间序列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.date_range(<span class="string">&#x27;1967-07-01&#x27;</span>,<span class="string">&#x27;2015-04-01&#x27;</span>, freq=<span class="string">&#x27;D&#x27;</span>)<span class="comment">#查看两个时间之间的时间,间隔1天</span></span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#dt对象</span></span><br><span class="line">s.dt.daysinmonth<span class="comment">#每个月几天</span></span><br><span class="line">s.dt.dayofweek<span class="comment">#每周几天</span></span><br><span class="line">s.dt.dayofweek.isin([<span class="number">5</span>,<span class="number">6</span>])<span class="comment">#是否包含双休日</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.Timestamp(<span class="string">&#x27;20210223 22:00:00&#x27;</span>)-pd.Timestamp(<span class="string">&#x27;20210222 18:35:00&#x27;</span>)<span class="comment">#计算两个时间之差</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>DataFrame</tag>
      </tags>
  </entry>
  <entry>
    <title>特征选择_1</title>
    <url>/2021/03/20/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9_1/</url>
    <content><![CDATA[<p>在数据预处理过程中，特征选择是一个重要的过程，选择出重要的特征可以加快模型训练速度。通常可以从以下两方面来选择特征：</p>
<blockquote>
<p>1.特征是否发散（对于样本区分作用的大小）<br>2.特征与标签的相关性</p>
</blockquote>
<span id="more"></span>
<p>特征选择的方法主要有3种：</p>
<blockquote>
<p>1.Filter Method：先根据统计量设置阈值选择特征，之后再训练模型。<br>2.Wrapper Method：把最终将要使用的模型的性能作为特征子集的评价标准，多次训练模型选择有利于模型性能的特征子集。<br>3.Embedding Method：将特征选择过程与模型训练过程融为一体，在模型训练的过程中自动进行特征选择。</p>
</blockquote>
<p>常用sklearn中的feature_selection库来进行特征选择。</p>
<h4 id="1-Fliter-过滤法"><a href="#1-Fliter-过滤法" class="headerlink" title="1. Fliter 过滤法:"></a>1. Fliter 过滤法:</h4><blockquote>
<p>Fliter的优点在于只训练一次模型，速度快。但是选择与标签相关性最强的特征子集不一定是最佳特征，甚至可能对结果负优化。</p>
</blockquote>
<h5 id="1-1-方差选择法"><a href="#1-1-方差选择法" class="headerlink" title="1.1 方差选择法"></a>1.1 方差选择法</h5><p>计算各个特征的方差，设置阈值，选择方差大于阈值的特征。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line"><span class="comment">#参数threshold为方差的阈值</span></span><br><span class="line">VarianceThreshold(threshold=<span class="number">3</span>).fit_transform(iris.data)</span><br></pre></td></tr></table></figure>

<h5 id="1-2-Pearson相关系数法"><a href="#1-2-Pearson相关系数法" class="headerlink" title="1.2 Pearson相关系数法"></a>1.2 Pearson相关系数法</h5><p>计算各个特征对于标签的Pearson相关系数和p值，选择前k名的特征。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"><span class="comment">#参数k为选择的特征个数</span></span><br><span class="line">SelectKBest(<span class="keyword">lambda</span> X, Y: array(<span class="built_in">map</span>(<span class="keyword">lambda</span> x:pearsonr(x, Y), X.T)).T, k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Pearson法的缺陷在于只对线性相关敏感，对非线性关系不敏感。</p>
</blockquote>
<h5 id="1-3-卡方检验-互信息法等方法"><a href="#1-3-卡方检验-互信息法等方法" class="headerlink" title="1.3 卡方检验\互信息法等方法"></a>1.3 卡方检验\互信息法等方法</h5><p>也是用来评价X与y的相关性，先构建评价函数，再选择前K名的特征。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#chi2</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line">SelectKBest(chi2, k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br><span class="line"></span><br><span class="line"><span class="comment">#MIC</span></span><br><span class="line"><span class="keyword">from</span> minepy <span class="keyword">import</span> MINE</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mic</span>(<span class="params">x, y</span>):</span></span><br><span class="line">     m = MINE()</span><br><span class="line">     m.compute_score(x, y)</span><br><span class="line">     <span class="keyword">return</span> (m.mic(), <span class="number">0.5</span>)</span><br><span class="line">SelectKBest(<span class="keyword">lambda</span> X, Y: array(<span class="built_in">map</span>(<span class="keyword">lambda</span> x:mic(x, Y), X.T)).T, k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>

<h4 id="2-Wrapper-包装法"><a href="#2-Wrapper-包装法" class="headerlink" title="2. Wrapper 包装法:"></a>2. Wrapper 包装法:</h4><blockquote>
<p>Wrapper的优点在于能够识别模型最适宜的特征子集，缺点在于训练多次模型，算法复杂性高，且特征子集不一定是<u>大多数解释变量</u>。</p>
</blockquote>
<p>Wrapper最具代表性的方法就是RFE递归消除特征法，即使用一个基模型来进行多轮训练，每轮训练都遍历所有特征，之后消除重要性(feature_importances_)低的特征，再基于新的特征集进行下一轮训练。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#RFE</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">rfe = RFE(estimator=LogisticRegression(), n_features_to_select=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br><span class="line"></span><br><span class="line"><span class="comment">#RFECV</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFECV</span><br><span class="line">rfecv = RFECV(estimator=svc, step=<span class="number">1</span>, cv=StratifiedKFold(<span class="number">2</span>), scoring=<span class="string">&#x27;roc_auc&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="3-Embedded-嵌入法"><a href="#3-Embedded-嵌入法" class="headerlink" title="3. Embedded 嵌入法"></a>3. Embedded 嵌入法</h4><h5 id="3-1-基于惩罚项的特征选择法"><a href="#3-1-基于惩罚项的特征选择法" class="headerlink" title="3.1 基于惩罚项的特征选择法"></a>3.1 基于惩罚项的特征选择法</h5><p>使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。由于L1正则化会产生稀疏权值矩阵，所以其自带特征选择的特性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#L1正则</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">SelectFromModel(LogisticRegression(penalty=<span class="string">&quot;l1&quot;</span>, C=<span class="number">0.1</span>)).fit_transform(iris.data, iris.target)</span><br><span class="line"></span><br><span class="line"><span class="comment">#L2正则</span></span><br><span class="line">SelectFromModel(LogisticRegression(penalty=<span class="string">&quot;l2&quot;</span>,threshold=<span class="number">0.5</span>, C=<span class="number">0.1</span>)).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>

<h5 id="3-2-基于树模型的特征选择法"><a href="#3-2-基于树模型的特征选择法" class="headerlink" title="3.2 基于树模型的特征选择法"></a>3.2 基于树模型的特征选择法</h5><p>树模型中GBDT也可用来作为基模型进行特征选择。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line">SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>

<h4 id="4-基于SHAP值的特征筛选"><a href="#4-基于SHAP值的特征筛选" class="headerlink" title="4. 基于SHAP值的特征筛选"></a>4. 基于SHAP值的特征筛选</h4><p>SHAP是由Shapley value启发的可加性解释模型。对于每条样本，每个特征都会对应一个SHAP value值体现其对结果的贡献。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> shap</span><br><span class="line"><span class="comment"># 创建模型解释器</span></span><br><span class="line">explainer_xgb = shap.TreeExplainer(model1)</span><br><span class="line">explainer_lgb = shap.TreeExplainer(model2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取训练集每个样本每个特征特征的SHAP值，并对特征进行整体的可视化</span></span><br><span class="line">shape_values = explainer_lgb.shap_values(data[cols])</span><br><span class="line">shap.summary_plot(shape_values, data[cols], plot_type=<span class="string">&#x27;bar&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="5-基于对抗验证-Adversarial-Validation-的特征筛选"><a href="#5-基于对抗验证-Adversarial-Validation-的特征筛选" class="headerlink" title="5. 基于对抗验证(Adversarial Validation)的特征筛选"></a>5. 基于对抗验证(Adversarial Validation)的特征筛选</h4><p>常用于训练集与测试集相差非常大的情况。实现步骤：<br>1.将训练集和测试集合并，分别打上0和1的标签。<br>2.构建模型进行训练，逐个将特征输入模型，记录AUC。<br>3.最后将AUC高的特征删除(将测试和训练样本差别很大的特征删除)，通过删掉这些特征实现模型效果提升。</p>
]]></content>
      <categories>
        <category>Feature engineering</category>
      </categories>
      <tags>
        <tag>Embedding</tag>
        <tag>Wapper</tag>
        <tag>Filter</tag>
      </tags>
  </entry>
</search>
