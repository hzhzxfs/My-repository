<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>地理空间数据EDA数据探索性分析</title>
    <url>/2021/04/17/EDA%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2%E6%80%A7%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>EDA——数据探索性分析，是通过了解数据集的基本情况、变量间的相互关系以及变量与预测值之间的关系，为后期特征工程和建立模型做铺垫。本文以<em>智慧海洋建设竞赛</em>为例进行演示。</p>
<h4 id="1-总体了解数据"><a href="#1-总体了解数据" class="headerlink" title="1. 总体了解数据"></a>1. 总体了解数据</h4><h5 id="1-1-查看样本个数和原始特征维度"><a href="#1-1-查看样本个数和原始特征维度" class="headerlink" title="1.1 查看样本个数和原始特征维度"></a>1.1 查看样本个数和原始特征维度</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_train.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_test.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_train.columns	<span class="comment">#查看列名</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.set_option(<span class="string">&#x27;display.max_info_rows&#x27;</span>,<span class="number">2699639</span>)	<span class="comment">#提高非缺失值检查的行数上线</span></span><br><span class="line"><span class="comment">#pd.options.display.max_info_rows = 2699639</span></span><br><span class="line">data_train.info()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#查看count 非空值数、std 标准差、（25%、50%、75%）分位数等基本情况</span></span><br><span class="line">data_train.describe([<span class="number">0.01</span>,<span class="number">0.025</span>,<span class="number">0.05</span>,<span class="number">0.5</span>,<span class="number">0.75</span>,<span class="number">0.9</span>,<span class="number">0.99</span>])	</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h5 id="1-2-查看缺失值和唯一值等"><a href="#1-2-查看缺失值和唯一值等" class="headerlink" title="1.2 查看缺失值和唯一值等"></a>1.2 查看缺失值和唯一值等</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_train.isnull().<span class="built_in">any</span>()	<span class="comment">#查看缺失值</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#查看含有缺失值的列名</span></span><br><span class="line">data_train.columns[data_train.isnull().<span class="built_in">any</span>()].tolist()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#查看仅有唯一值的特征</span></span><br><span class="line">one_value_fea_train = [col <span class="keyword">for</span> col <span class="keyword">in</span> data_train.columns <span class="keyword">if</span> data_train[col].nunique() &lt;= <span class="number">1</span>]</span><br><span class="line">one_value_fea_test = [col <span class="keyword">for</span> col <span class="keyword">in</span> data_test.columns <span class="keyword">if</span> data_test[col].nunique() &lt;= <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<h4 id="2-查看数据特性和特征分布"><a href="#2-查看数据特性和特征分布" class="headerlink" title="2. 查看数据特性和特征分布"></a>2. 查看数据特性和特征分布</h4><h5 id="2-1-渔船轨迹可视化"><a href="#2-1-渔船轨迹可视化" class="headerlink" title="2.1 渔船轨迹可视化"></a>2.1 渔船轨迹可视化</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 从每个类别中随机抽取三个渔船的轨迹进行可视化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_three_traj</span>():</span></span><br><span class="line">    fig,axes = plt.subplots(nrows=<span class="number">3</span>,ncols=<span class="number">3</span>,figsize=(<span class="number">20</span>,<span class="number">15</span>))</span><br><span class="line">    plt.subplots_adjust(wspace=<span class="number">0.2</span>,hspace=<span class="number">0.2</span>)</span><br><span class="line">    <span class="comment"># 对于每一个类别，随机选出刺网的三条轨迹进行可视化</span></span><br><span class="line">    lables = [<span class="string">&quot;ciwang&quot;</span>,<span class="string">&quot;weiwang&quot;</span>,<span class="string">&quot;tuowang&quot;</span>]</span><br><span class="line">    <span class="keyword">for</span> i,file_type <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>([<span class="string">&quot;ciwang_data&quot;</span>,<span class="string">&quot;weiwang_data&quot;</span>,<span class="string">&quot;tuowang_data&quot;</span>])):</span><br><span class="line">        data1, data2, data3 = get_random_three_traj(<span class="built_in">type</span>=file_type)</span><br><span class="line">        <span class="keyword">for</span> j, datax <span class="keyword">in</span> <span class="built_in">enumerate</span>([data1, data2, data3]):</span><br><span class="line">            x_data = datax[<span class="string">&quot;x&quot;</span>].loc[-<span class="number">1</span>:].values</span><br><span class="line">            y_data = datax[<span class="string">&quot;y&quot;</span>].loc[-<span class="number">1</span>:].values</span><br><span class="line">            axes[i][j - <span class="number">1</span>].scatter(x_data[<span class="number">0</span>], y_data[<span class="number">0</span>], label=<span class="string">&quot;start&quot;</span>, c=<span class="string">&quot;red&quot;</span>, s=<span class="number">20</span>, marker=<span class="string">&quot;o&quot;</span>)</span><br><span class="line">            axes[i][j - <span class="number">1</span>].plot(x_data, y_data, label=lables[i])</span><br><span class="line">            axes[i][j - <span class="number">1</span>].scatter(x_data[<span class="built_in">len</span>(x_data) - <span class="number">1</span>], y_data[<span class="built_in">len</span>(y_data) - <span class="number">1</span>], label=<span class="string">&quot;end&quot;</span>, c=<span class="string">&quot;green&quot;</span>, s=<span class="number">20</span>,</span><br><span class="line">                                   marker=<span class="string">&quot;D&quot;</span>)</span><br><span class="line">            axes[i][j - <span class="number">1</span>].grid(alpha=<span class="number">2</span>)</span><br><span class="line">            axes[i][j - <span class="number">1</span>].legend(loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">visualize_three_traj()</span><br></pre></td></tr></table></figure>
<p>—————-图1——————</p>
<p>从图中可以发现,不同类别的轨迹有一定区分性.</p>
<blockquote>
<p>刺网为规则多边形.</p>
<p>围网为包围的形状.</p>
<p>拖网为点到点,转弯次数少.</p>
</blockquote>
<p>从轨迹数据可以猜测其特征可能为转弯的角度大小\转弯次数\起始点之间的距离和时间\经度和维度变化范围等.</p>
<p>此外,存在一些异常轨迹需要剔除.</p>
<h4 id="3-坐标序列可视化"><a href="#3-坐标序列可视化" class="headerlink" title="3. 坐标序列可视化"></a>3. 坐标序列可视化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 随机选取某条数据，观察x坐标序列和y坐标序列的变化情况</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_one_traj_x_y</span>():</span></span><br><span class="line">    fig,axes = plt.subplots(nrows=<span class="number">2</span>,ncols=<span class="number">1</span>,figsize=(<span class="number">10</span>,<span class="number">8</span>))</span><br><span class="line">    plt.subplots_adjust(wspace=<span class="number">0.2</span>,hspace=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">    data1 = get_random_one_traj(<span class="built_in">type</span>=<span class="string">&quot;weiwang_data&quot;</span>)</span><br><span class="line">    x = data1[<span class="string">&quot;x&quot;</span>].loc[-<span class="number">1</span>:]</span><br><span class="line">    x = x / <span class="number">10000</span></span><br><span class="line">    </span><br><span class="line">    y = data1[<span class="string">&quot;y&quot;</span>].loc[-<span class="number">1</span>:]</span><br><span class="line">    y = y / <span class="number">10000</span></span><br><span class="line"></span><br><span class="line">    arr1 = np.arange(<span class="built_in">len</span>(x))</span><br><span class="line">    arr2 = np.arange(<span class="built_in">len</span>(y))</span><br><span class="line"></span><br><span class="line">    axes[<span class="number">0</span>].plot(arr1,x,label=<span class="string">&quot;x&quot;</span>)</span><br><span class="line">    axes[<span class="number">1</span>].plot(arr2,y,label=<span class="string">&quot;y&quot;</span>)</span><br><span class="line">    axes[<span class="number">0</span>].grid(alpha=<span class="number">3</span>)</span><br><span class="line">    axes[<span class="number">0</span>].legend(loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line">    axes[<span class="number">1</span>].grid(alpha=<span class="number">3</span>)</span><br><span class="line">    axes[<span class="number">1</span>].legend(loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">visualize_one_traj_x_y()</span><br></pre></td></tr></table></figure>
<p>———-图2———–</p>
<p>由上图可知,存在同一时间段内x\y坐标均未变化,说明可能该时段内渔船正停留在某处.</p>
<h4 id="4-三类渔船速度和方向可视化"><a href="#4-三类渔船速度和方向可视化" class="headerlink" title="4.三类渔船速度和方向可视化"></a>4.三类渔船速度和方向可视化</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 每类轨迹，随机选取某个渔船，可视化速度序列和方向序列</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_three_traj_speed_direction</span>():</span></span><br><span class="line">    fig,axes = plt.subplots(nrows=<span class="number">3</span>,ncols=<span class="number">2</span>,figsize=(<span class="number">20</span>,<span class="number">15</span>))</span><br><span class="line">    plt.subplots_adjust(wspace=<span class="number">0.1</span>,hspace=<span class="number">0.1</span>)</span><br><span class="line">    <span class="comment"># 随机选出刺网的三条轨迹进行可视化</span></span><br><span class="line">    file_types = [<span class="string">&quot;ciwang_data&quot;</span>,<span class="string">&quot;weiwang_data&quot;</span>,<span class="string">&quot;tuowang_data&quot;</span>]</span><br><span class="line">    speed_types = [<span class="string">&quot;ciwang_speed&quot;</span>,<span class="string">&quot;weiwang_speed&quot;</span>,<span class="string">&quot;tuowang_speed&quot;</span>]</span><br><span class="line">    doirections = [<span class="string">&quot;ciwang_direction&quot;</span>,<span class="string">&quot;weiwang_direction&quot;</span>,<span class="string">&quot;tuowang_direction&quot;</span>]</span><br><span class="line">    colors = [<span class="string">&#x27;blue&#x27;</span>, <span class="string">&#x27;red&#x27;</span>, <span class="string">&#x27;brown&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> i,file_name <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(file_types)):</span><br><span class="line">        datax = get_random_one_traj(<span class="built_in">type</span>=file_name)</span><br><span class="line">        x_data = datax[<span class="string">&quot;速度&quot;</span>].loc[-<span class="number">1</span>:].values</span><br><span class="line">        y_data = datax[<span class="string">&quot;方向&quot;</span>].loc[-<span class="number">1</span>:].values</span><br><span class="line">        axes[i][<span class="number">0</span>].plot(<span class="built_in">range</span>(<span class="built_in">len</span>(x_data)), x_data, label=speed_types[i], color=colors[i])</span><br><span class="line">        axes[i][<span class="number">0</span>].grid(alpha=<span class="number">2</span>)</span><br><span class="line">        axes[i][<span class="number">0</span>].legend(loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line">        axes[i][<span class="number">1</span>].plot(<span class="built_in">range</span>(<span class="built_in">len</span>(y_data)), y_data, label=doirections[i], color=colors[i])</span><br><span class="line">        axes[i][<span class="number">1</span>].grid(alpha=<span class="number">2</span>)</span><br><span class="line">        axes[i][<span class="number">1</span>].legend(loc=<span class="string">&quot;best&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">visualize_three_traj_speed_direction()</span><br></pre></td></tr></table></figure>
<p>————图3—————-<br>由上图可知,不同分类渔船的轨迹速度某些时段均存在连续的低值情况,说明可能存在某些海上停留点;不同类别渔船的方向变化都很大,可能是海上漂泊导致,作为特征对于类别的区分度低,但也存在方向变化不大的时段,强化了对停留点存在的判断.</p>
<h4 id="5-三类渔船速度和方向的数据分布"><a href="#5-三类渔船速度和方向的数据分布" class="headerlink" title="5.三类渔船速度和方向的数据分布"></a>5.三类渔船速度和方向的数据分布</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 对某一特征进行数据统计</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_cummulation</span>(<span class="params"><span class="built_in">type</span>,path,kind,columns</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    type:&quot;ciwang&quot;,&quot;weiwang&quot; or &quot;tuowang&quot;</span></span><br><span class="line"><span class="string">    path:数据路径</span></span><br><span class="line"><span class="string">    kind:&quot;速度&quot;or&quot;方向&quot;</span></span><br><span class="line"><span class="string">    columns:与kind对应，&quot;speed&quot;or&quot;direction&quot;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    data_dict = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(path + <span class="built_in">type</span>+<span class="string">&quot;.pkl&quot;</span>,<span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> file:</span><br><span class="line">        data_list = pickle.load(file)</span><br><span class="line">    <span class="keyword">for</span> datax <span class="keyword">in</span> tqdm(data_list):</span><br><span class="line">        data = datax[kind].values</span><br><span class="line">        <span class="keyword">for</span> speed <span class="keyword">in</span> data:</span><br><span class="line">            data_dict.setdefault(speed,<span class="number">0</span>)</span><br><span class="line">            data_dict[speed] += <span class="number">1</span></span><br><span class="line">    data_dict = <span class="built_in">dict</span>(<span class="built_in">sorted</span>(data_dict.items(),key=<span class="keyword">lambda</span> x:x[<span class="number">0</span>],reverse=<span class="literal">False</span>))</span><br><span class="line">    data_df = pd.DataFrame.from_dict(data_dict,columns=[columns],orient=<span class="string">&quot;index&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> data_df</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 分别得到速度和方向的分布数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_speed_and_direction_distribution_data</span>(<span class="params"><span class="built_in">type</span></span>):</span></span><br><span class="line">    path = <span class="string">&quot;./data/&quot;</span></span><br><span class="line">    data_speed_df = get_data_cummulation(<span class="built_in">type</span>=<span class="built_in">type</span>, path=path,kind=<span class="string">&quot;速度&quot;</span>,columns=<span class="string">&quot;speed&quot;</span>)</span><br><span class="line">    data_direction_df = get_data_cummulation(<span class="built_in">type</span>=<span class="built_in">type</span>,path=path,kind=<span class="string">&quot;方向&quot;</span>,columns=<span class="string">&quot;direction&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> data_speed_df,data_direction_df</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 可视化速度和方向的数据分布</span></span><br><span class="line">df_speeds = []</span><br><span class="line">df_directions = []</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_speed_direction1_distribution</span>():</span></span><br><span class="line">    plt.subplots(nrows=<span class="number">1</span>, ncols=<span class="number">2</span>, figsize=(<span class="number">15</span>, <span class="number">6</span>))</span><br><span class="line">    plt.subplots_adjust(wspace=<span class="number">0.2</span>, hspace=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">    file_types = [<span class="string">&quot;ciwang_data&quot;</span>, <span class="string">&quot;weiwang_data&quot;</span>, <span class="string">&quot;tuowang_data&quot;</span>]</span><br><span class="line">    lables = [<span class="string">&quot;ciwang&quot;</span>, <span class="string">&quot;weiwang&quot;</span>, <span class="string">&quot;tuowang&quot;</span>]</span><br><span class="line">    colors = [<span class="string">&quot;red&quot;</span>, <span class="string">&quot;blue&quot;</span>, <span class="string">&quot;green&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, filenames <span class="keyword">in</span> <span class="built_in">enumerate</span>(file_types):</span><br><span class="line">        df11, df21 = get_speed_and_direction_distribution_data(file_types[i])</span><br><span class="line">        plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">        ax1 = sns.kdeplot(df11[<span class="string">&quot;speed&quot;</span>].values / <span class="number">1000000</span>, color=colors[i],shade=<span class="literal">True</span>)</span><br><span class="line">        plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">        ax3 = sns.kdeplot(df21[<span class="string">&quot;direction&quot;</span>].values / <span class="number">1000000</span>, color=colors[i],shade=<span class="literal">True</span>)</span><br><span class="line">        df_speeds.append(df11)</span><br><span class="line">        df_directions.append(df21)</span><br><span class="line">    ax1.legend(lables)</span><br><span class="line">    ax1.set_xlabel(<span class="string">&quot;Speed&quot;</span>)</span><br><span class="line">    ax3.set_xlabel(<span class="string">&quot;Direction&quot;</span>)</span><br><span class="line">    ax3.legend(lables)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plot_speed_direction1_distribution()</span><br></pre></td></tr></table></figure>
<p>———–图4———————</p>
<p>由上图可知,三种类别渔船的速度分布差异较大,而刺网和围网方向分布差异不明显,拖网方向分布有差异.</p>
]]></content>
      <categories>
        <category>Geospatial Data Analysis</category>
      </categories>
      <tags>
        <tag>DataFrame</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title>LightGBM调参_1</title>
    <url>/2021/03/26/lightGBM%E8%B0%83%E5%8F%82/</url>
    <content><![CDATA[<p>#1简单列举一下日常调参过程中常用的几种方法，具体的原理下次补上。</p>
<h4 id="1-经验法"><a href="#1-经验法" class="headerlink" title="1. 经验法:"></a>1. 经验法:</h4><blockquote>
<p>往两个方向调：</p>
<p>1.提高准确率：max_depth, num_leaves, learning_rate</p>
<p>2.降低过拟合：max_bin, min_data_in_leaf；L1, L2正则化；数据抽样, 列采样</p>
</blockquote>
<p>1.使用较小的num_leaves，max_depth和max_bin，降低复杂度。</p>
<p>2.使用min_data_in_leaf和min_sum_hessian_in_leaf，该值越大，模型的学习越保守。</p>
<span id="more"></span>

<p>3.设置bagging_freq和bagging_fraction使用bagging。</p>
<p>4.设置feature_fraction进行特征采样。</p>
<p>5.使用lambda_l1,lambda_l2和min_gain_to_split正则化。</p>
<h4 id="2-贪心调参"><a href="#2-贪心调参" class="headerlink" title="2. 贪心调参:"></a>2. 贪心调参:</h4><p>先调整对模型影响最大的参数，再调整对模型影响次大的参数，缺点是容易调成局部最优，需要多次调试。日常调参顺序如下:</p>
<p>① num_leaves, max_depth</p>
<p>② min_data_in_leaf, min_child_weight</p>
<p>③ bagging_freq, bagging_fraction,  feature_fraction,</p>
<p>④ reg_lambda, reg_alpha</p>
<p>⑤ min_split_gain</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="comment"># 调objective</span></span><br><span class="line">best_obj = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> obj <span class="keyword">in</span> objective:</span><br><span class="line">    model = LGBMRegressor(objective=obj)</span><br><span class="line">    score = cross_val_score(model, X_train, y_train, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;f1&#x27;</span>).mean()</span><br><span class="line">    best_obj[obj] = score</span><br><span class="line"></span><br><span class="line"><span class="comment"># num_leaves</span></span><br><span class="line">best_leaves = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> leaves <span class="keyword">in</span> num_leaves:</span><br><span class="line">    model = LGBMRegressor(objective=<span class="built_in">min</span>(best_obj.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])[<span class="number">0</span>], num_leaves=leaves)</span><br><span class="line">    score = cross_val_score(model, X_train, y_train, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;f1&#x27;</span>).mean()</span><br><span class="line">    best_leaves[leaves] = score</span><br><span class="line"></span><br><span class="line"><span class="comment"># max_depth</span></span><br><span class="line">best_depth = <span class="built_in">dict</span>()</span><br><span class="line"><span class="keyword">for</span> depth <span class="keyword">in</span> max_depth:</span><br><span class="line">    model = LGBMRegressor(objective=<span class="built_in">min</span>(best_obj.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])[<span class="number">0</span>],</span><br><span class="line">                          num_leaves=<span class="built_in">min</span>(best_leaves.items(), key=<span class="keyword">lambda</span> x:x[<span class="number">1</span>])[<span class="number">0</span>],</span><br><span class="line">                          max_depth=depth)</span><br><span class="line">    score = cross_val_score(model, X_train, y_train, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;f1&#x27;</span>).mean()</span><br><span class="line">    best_depth[depth] = score</span><br></pre></td></tr></table></figure>

<p>以此类推，按调参顺序依次调整优化，并且可以对每一个最优参数下模型的得分进行可视化。</p>
<h4 id="3-网格搜索"><a href="#3-网格搜索" class="headerlink" title="3. 网格搜索"></a>3. 网格搜索</h4><p>即穷举搜索，在参数数组里循环遍历，一般大数据集不会用到，因为速度太慢。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_best_cv_params</span>(<span class="params">learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">581</span>, num_leaves=<span class="number">31</span>, max_depth=-<span class="number">1</span>, bagging_fraction=<span class="number">1.0</span>, feature_fraction=<span class="number">1.0</span>, bagging_freq=<span class="number">0</span>, min_data_in_leaf=<span class="number">20</span>, min_child_weight=<span class="number">0.001</span>, min_split_gain=<span class="number">0</span>, reg_lambda=<span class="number">0</span>, reg_alpha=<span class="number">0</span>, param_grid=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    cv_fold = KFold(n_splits=<span class="number">5</span>, shuffle=<span class="literal">True</span>, random_state=<span class="number">2021</span>)</span><br><span class="line"></span><br><span class="line">    model_lgb = lgb.LGBMClassifier(learning_rate=learning_rate,</span><br><span class="line">                                   n_estimators=n_estimators,</span><br><span class="line">                                   num_leaves=num_leaves,</span><br><span class="line">                                   max_depth=max_depth,</span><br><span class="line">                                   bagging_fraction=bagging_fraction,</span><br><span class="line">                                   feature_fraction=feature_fraction,</span><br><span class="line">                                   bagging_freq=bagging_freq,</span><br><span class="line">                                   min_data_in_leaf=min_data_in_leaf,</span><br><span class="line">                                   min_child_weight=min_child_weight,</span><br><span class="line">                                   min_split_gain=min_split_gain,</span><br><span class="line">                                   reg_lambda=reg_lambda,</span><br><span class="line">                                   reg_alpha=reg_alpha,</span><br><span class="line">                                   n_jobs= <span class="number">8</span></span><br><span class="line">                                  )</span><br><span class="line"></span><br><span class="line">    f1 = make_scorer(f1_score, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">    grid_search = GridSearchCV(estimator=model_lgb, </span><br><span class="line">                               cv=cv_fold,</span><br><span class="line">                               param_grid=param_grid,</span><br><span class="line">                               scoring=f1</span><br><span class="line"></span><br><span class="line">                              )</span><br><span class="line">    grid_search.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;模型当前最优参数为:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(grid_search.best_params_))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;模型当前最优得分为:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(grid_search.best_score_))</span><br></pre></td></tr></table></figure>
<p>总体思路是先粗调再细调。在一开始调整时，可设置较大的学习率如0.1，先确定树的个数，再依次调整参数，最后设置较小的学习率如0.05，确定最终参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lgb_params = &#123;<span class="string">&#x27;num_leaves&#x27;</span>: <span class="built_in">range</span>(<span class="number">10</span>, <span class="number">80</span>, <span class="number">5</span>), <span class="string">&#x27;max_depth&#x27;</span>: <span class="built_in">range</span>(<span class="number">3</span>,<span class="number">10</span>,<span class="number">2</span>)&#125;</span><br><span class="line">get_best_cv_params()</span><br><span class="line"><span class="comment">#----------------------------------------</span></span><br><span class="line">lgb_params = &#123;<span class="string">&#x27;num_leaves&#x27;</span>: <span class="built_in">range</span>(<span class="number">25</span>, <span class="number">35</span>, <span class="number">1</span>), <span class="string">&#x27;max_depth&#x27;</span>: <span class="built_in">range</span>(<span class="number">5</span>,<span class="number">9</span>,<span class="number">1</span>)&#125;</span><br><span class="line">get_best_cv_params(n_estimators=<span class="number">85</span>)</span><br><span class="line"><span class="comment">#----------------------------------------</span></span><br><span class="line">lgb_params = &#123;<span class="string">&#x27;bagging_fraction&#x27;</span>: [i/<span class="number">10</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>,<span class="number">10</span>,<span class="number">1</span>)], </span><br><span class="line">              <span class="string">&#x27;feature_fraction&#x27;</span>: [i/<span class="number">10</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>,<span class="number">10</span>,<span class="number">1</span>)],</span><br><span class="line">              <span class="string">&#x27;bagging_freq&#x27;</span>: <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">81</span>,<span class="number">10</span>)&#125;</span><br><span class="line">get_best_cv_params(n_estimators=<span class="number">85</span>, num_leaves=<span class="number">29</span>, max_depth=<span class="number">7</span>, min_data_in_leaf=<span class="number">45</span>）</span><br><span class="line"><span class="comment">#----------------------------------------</span></span><br><span class="line">lgb_params = &#123;<span class="string">&#x27;reg_lambda&#x27;</span>: [<span class="number">0</span>,<span class="number">0.001</span>,<span class="number">0.01</span>,<span class="number">0.03</span>,<span class="number">0.08</span>,<span class="number">0.3</span>,<span class="number">0.5</span>], <span class="string">&#x27;reg_alpha&#x27;</span>: [<span class="number">0</span>,<span class="number">0.001</span>,<span class="number">0.01</span>,<span class="number">0.03</span>,<span class="number">0.08</span>,<span class="number">0.3</span>,<span class="number">0.5</span>]&#125;</span><br><span class="line">get_best_cv_params(n_estimators=<span class="number">85</span>, num_leaves=<span class="number">29</span>, max_depth=<span class="number">7</span>, min_data_in_leaf=<span class="number">45</span>, bagging_fraction=<span class="number">0.9</span>, feature_fraction=<span class="number">0.9</span>, bagging_freq=<span class="number">40</span>)</span><br><span class="line"><span class="comment">#----------------------------------------</span></span><br><span class="line">lgb_params = &#123;<span class="string">&#x27;min_split_gain&#x27;</span>: [i/<span class="number">10</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">11</span>,<span class="number">1</span>)]&#125;</span><br><span class="line">get_best_cv_params(n_estimators=<span class="number">85</span>, num_leaves=<span class="number">29</span>, max_depth=<span class="number">7</span>, min_data_in_leaf=<span class="number">45</span>, bagging_fraction=<span class="number">0.9</span>, feature_fraction=<span class="number">0.9</span>, bagging_freq=<span class="number">40</span>, min_split_gain=<span class="literal">None</span>)</span><br><span class="line"><span class="comment">#----------------------------------------</span></span><br><span class="line">final_params = &#123;</span><br><span class="line">                <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.01</span>,</span><br><span class="line">                <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">29</span>,</span><br><span class="line">                <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">7</span>,</span><br><span class="line">                <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;multiclass&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;num_class&#x27;</span>: <span class="number">4</span>,</span><br><span class="line">                <span class="string">&#x27;min_data_in_leaf&#x27;</span>:<span class="number">45</span>,</span><br><span class="line">                <span class="string">&#x27;min_child_weight&#x27;</span>:<span class="number">0.001</span>,</span><br><span class="line">                <span class="string">&#x27;bagging_fraction&#x27;</span>: <span class="number">0.9</span>,</span><br><span class="line">                <span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.9</span>,</span><br><span class="line">                <span class="string">&#x27;bagging_freq&#x27;</span>: <span class="number">40</span>,</span><br><span class="line">                <span class="string">&#x27;min_split_gain&#x27;</span>: <span class="number">0</span>,</span><br><span class="line">                <span class="string">&#x27;reg_lambda&#x27;</span>:<span class="number">0</span>,</span><br><span class="line">                <span class="string">&#x27;reg_alpha&#x27;</span>:<span class="number">0</span>,</span><br><span class="line">                <span class="string">&#x27;nthread&#x27;</span>: <span class="number">6</span></span><br><span class="line">               &#125;</span><br><span class="line"></span><br><span class="line">cv_result = lgb.cv(train_set=lgb_train,</span><br><span class="line">                   early_stopping_rounds=<span class="number">20</span>,</span><br><span class="line">                   num_boost_round=<span class="number">5000</span>,</span><br><span class="line">                   nfold=<span class="number">5</span>,</span><br><span class="line">                   stratified=<span class="literal">True</span>,</span><br><span class="line">                   shuffle=<span class="literal">True</span>,</span><br><span class="line">                   params=final_params,</span><br><span class="line">                   feval=f1_score_vali,</span><br><span class="line">                   seed=<span class="number">0</span>,</span><br><span class="line">                  )</span><br></pre></td></tr></table></figure>
<h4 id="4-贝叶斯调参"><a href="#4-贝叶斯调参" class="headerlink" title="4. 贝叶斯调参"></a>4. 贝叶斯调参</h4><p>是一种用模型找到目标函数最小值的方法，比网格和随机搜索省时。步骤如下：</p>
<p>① 定义优化函数(rf_cv）</p>
<p>② 建立模型</p>
<p>③ 定义待优化的参数</p>
<p>④ 得到优化结果，并返回要优化的分数指标</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="comment">#定义优化函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rf_cv_lgb</span>(<span class="params">num_leaves, max_depth, bagging_fraction, feature_fraction, bagging_freq, min_data_in_leaf, </span></span></span><br><span class="line"><span class="function"><span class="params">              min_child_weight, min_split_gain, reg_lambda, reg_alpha</span>):</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立模型</span></span><br><span class="line">    model_lgb = lgb.LGBMClassifier(boosting_type=<span class="string">&#x27;gbdt&#x27;</span>, objective=<span class="string">&#x27;multiclass&#x27;</span>, num_class=<span class="number">4</span>,learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">5000</span>,num_leaves=<span class="built_in">int</span>(num_leaves), max_depth=<span class="built_in">int</span>(max_depth), bagging_fraction=<span class="built_in">round</span>(bagging_fraction, <span class="number">2</span>), feature_fraction=<span class="built_in">round</span>(feature_fraction, <span class="number">2</span>),bagging_freq=<span class="built_in">int</span>(bagging_freq), min_data_in_leaf=<span class="built_in">int</span>(min_data_in_leaf),min_child_weight=min_child_weight)</span><br><span class="line">    f1 = make_scorer(f1_score, average=<span class="string">&#x27;micro&#x27;</span>)</span><br><span class="line">    val = cross_val_score(model_lgb, X_train_split, y_train_split, cv=<span class="number">5</span>, scoring=f1).mean()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> val</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bayes_opt <span class="keyword">import</span> BayesianOptimization</span><br><span class="line"><span class="comment">#定义优化参数</span></span><br><span class="line">bayes_lgb = BayesianOptimization(</span><br><span class="line">    rf_cv_lgb, </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&#x27;num_leaves&#x27;</span>:(<span class="number">10</span>, <span class="number">200</span>),</span><br><span class="line">        <span class="string">&#x27;max_depth&#x27;</span>:(<span class="number">3</span>, <span class="number">20</span>),</span><br><span class="line">        <span class="string">&#x27;bagging_fraction&#x27;</span>:(<span class="number">0.5</span>, <span class="number">1.0</span>),</span><br><span class="line">        <span class="string">&#x27;feature_fraction&#x27;</span>:(<span class="number">0.5</span>, <span class="number">1.0</span>),</span><br><span class="line">        <span class="string">&#x27;bagging_freq&#x27;</span>:(<span class="number">0</span>, <span class="number">100</span>),</span><br><span class="line">        <span class="string">&#x27;min_data_in_leaf&#x27;</span>:(<span class="number">10</span>,<span class="number">100</span>),</span><br><span class="line">        <span class="string">&#x27;min_child_weight&#x27;</span>:(<span class="number">0</span>, <span class="number">10</span>),</span><br><span class="line">        <span class="string">&#x27;min_split_gain&#x27;</span>:(<span class="number">0.0</span>, <span class="number">1.0</span>),</span><br><span class="line">        <span class="string">&#x27;reg_alpha&#x27;</span>:(<span class="number">0.0</span>, <span class="number">10</span>),</span><br><span class="line">        <span class="string">&#x27;reg_lambda&#x27;</span>:(<span class="number">0.0</span>, <span class="number">10</span>),</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#开始优化</span></span><br><span class="line">bayes_lgb.maximize(n_iter=<span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#显示优化结果</span></span><br><span class="line">bayes_lgb.<span class="built_in">max</span></span><br></pre></td></tr></table></figure>

<p>参数优化完成后，可根据优化后的参数建立新的模型，降低学习率并寻找最优模型迭代次数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#设置较小的学习率，并通过cv函数确定当前最优的迭代次数</span></span><br><span class="line">base_params_lgb = &#123;</span><br><span class="line">                    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">                    <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;multiclass&#x27;</span>,</span><br><span class="line">                    <span class="string">&#x27;num_class&#x27;</span>: <span class="number">4</span>,</span><br><span class="line">                    <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.01</span>,</span><br><span class="line">                    <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">138</span>,</span><br><span class="line">                    <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">11</span>,</span><br><span class="line">                    <span class="string">&#x27;min_data_in_leaf&#x27;</span>: <span class="number">43</span>,</span><br><span class="line">                    <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">6.5</span>,</span><br><span class="line">                    <span class="string">&#x27;bagging_fraction&#x27;</span>: <span class="number">0.64</span>,</span><br><span class="line">                    <span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.93</span>,</span><br><span class="line">                    <span class="string">&#x27;bagging_freq&#x27;</span>: <span class="number">49</span>,</span><br><span class="line">                    <span class="string">&#x27;reg_lambda&#x27;</span>: <span class="number">7</span>,</span><br><span class="line">                    <span class="string">&#x27;reg_alpha&#x27;</span>: <span class="number">0.21</span>,</span><br><span class="line">                    <span class="string">&#x27;min_split_gain&#x27;</span>: <span class="number">0.288</span>,</span><br><span class="line">                    <span class="string">&#x27;nthread&#x27;</span>: <span class="number">10</span>,</span><br><span class="line">                    <span class="string">&#x27;verbose&#x27;</span>: -<span class="number">1</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">cv_result_lgb = lgb.cv(</span><br><span class="line">    train_set=train_matrix,</span><br><span class="line">    early_stopping_rounds=<span class="number">1000</span>, </span><br><span class="line">    num_boost_round=<span class="number">20000</span>,</span><br><span class="line">    nfold=<span class="number">5</span>,</span><br><span class="line">    stratified=<span class="literal">True</span>,</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    params=base_params_lgb,</span><br><span class="line">    feval=f1_score_vali,</span><br><span class="line">    seed=<span class="number">0</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;迭代次数&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(cv_result_lgb[<span class="string">&#x27;f1_score-mean&#x27;</span>])))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;最终模型的f1为&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">max</span>(cv_result_lgb[<span class="string">&#x27;f1_score-mean&#x27;</span>])))</span><br></pre></td></tr></table></figure>

<p>模型参数确定之后，建立最终模型并对验证集进行验证。</p>
]]></content>
      <categories>
        <category>Hyperparameter</category>
      </categories>
      <tags>
        <tag>Grid search</tag>
        <tag>Bayesian optimization</tag>
        <tag>Cross validation</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas的一些常用操作_2</title>
    <url>/2021/02/21/pandas_2/</url>
    <content><![CDATA[<blockquote>
<p>今天继续介绍几个常用的Pandas操作。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;./economics.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="1-DataFrame的apply方法"><a href="#1-DataFrame的apply方法" class="headerlink" title="1.DataFrame的apply方法"></a>1.DataFrame的apply方法</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[[<span class="string">&#x27;psavert&#x27;</span>,<span class="string">&#x27;uempmed&#x27;</span>]].apply(<span class="keyword">lambda</span> x:x.<span class="built_in">max</span>()-x.<span class="built_in">min</span>(), axis=<span class="number">1</span>)<span class="comment">#axis=1 将函数应用到列</span></span><br></pre></td></tr></table></figure>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.applymap(<span class="keyword">lambda</span> x:x*<span class="number">10</span>)<span class="comment">#applymap 将函数应用到每个元素</span></span><br></pre></td></tr></table></figure>
<h4 id="2-DataFrame的分组"><a href="#2-DataFrame的分组" class="headerlink" title="2.DataFrame的分组"></a>2.DataFrame的分组</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupby(<span class="string">&#x27;unemploy&#x27;</span>)[<span class="string">&#x27;psavert&#x27;</span>].median()<span class="comment">#样例:df.groupby(分组依据)[数据来源].使用操作</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">condition = df.unemploy &gt; df.unemploy.mean()<span class="comment">#使用condition定义分组依据</span></span><br><span class="line">df.groupby(condition)[<span class="string">&#x27;psavert&#x27;</span>].mean()<span class="comment">#分为True和False两组</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupby([condition, df[<span class="string">&#x27;unemploy&#x27;</span>]])[<span class="string">&#x27;psavert&#x27;</span>].mean()<span class="comment">#True组和False两组分别细分</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.groupby([df[<span class="string">&#x27;unemploy&#x27;</span>], df[<span class="string">&#x27;uempmed&#x27;</span>]])[<span class="string">&#x27;psavert&#x27;</span>].mean()<span class="comment">#两级分组</span></span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gb = df.groupby([<span class="string">&#x27;unemploy&#x27;</span>])<span class="comment">#Groupby对象</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gb.size()<span class="comment">#每组的元素个数</span></span><br><span class="line"><span class="comment">#和DataFrame一样，Groupby对象也有max\idxmin\all\\nunique\quantile\prod等函数，这里不一一列举。</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gb.agg([<span class="string">&#x27;skew&#x27;</span>, <span class="string">&#x27;sum&#x27;</span>, <span class="string">&#x27;idxmax&#x27;</span>])<span class="comment">#agg聚合函数，查看每个分组的三个统计量</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gb.agg(<span class="keyword">lambda</span> x: x.mean()-x.<span class="built_in">min</span>())<span class="comment">#在agg中自定义函数</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gb.<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x.shape[<span class="number">0</span>] &gt; <span class="number">100</span>)<span class="comment">#组过滤</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gb.apply(<span class="keyword">lambda</span> x: x**<span class="number">2</span>)<span class="comment">#组的apply方法</span></span><br></pre></td></tr></table></figure>
<h4 id="3-DataFrame的连接"><a href="#3-DataFrame的连接" class="headerlink" title="3.DataFrame的连接"></a>3.DataFrame的连接</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df1 = df[<span class="number">0</span>:<span class="number">10</span>]</span><br><span class="line">df2 = df[<span class="number">10</span>:<span class="number">20</span>]</span><br><span class="line">df1.merge(df2, on=<span class="string">&#x27;date&#x27;</span>, how=<span class="string">&#x27;outer&#x27;</span>)<span class="comment">#merge表示关系型连接，包括左连接、右连接、内连接和外(全)连接</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.concat([df1, df2], axis=<span class="number">0</span>)<span class="comment">#concat是方向性连接，axis=0表示纵向连接</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_min</span>(<span class="params">x1, x2</span>):</span></span><br><span class="line">    <span class="built_in">min</span> = x1.where(x1&lt;x2, x1)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">min</span></span><br><span class="line">df1.combine(df2, choose_min)<span class="comment">#使用combine函数自定义连接规则</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>下一期是Pandas的常见数据处理，包括缺失数据、文本数据、分类数据和时序数据。</p>
</blockquote>
]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>DataFrame</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas的一些常用操作_1</title>
    <url>/2021/02/21/pandas_1/</url>
    <content><![CDATA[<blockquote>
<p>今天介绍几个常用的Pandas操作。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;./economics.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="1-DataFrame-to-markdown-latex"><a href="#1-DataFrame-to-markdown-latex" class="headerlink" title="1.DataFrame to markdown/latex"></a>1.DataFrame to markdown/latex</h4><p>dataframe可以转换为许多常用格式，如csv,excel,sql,json,html,latex等等，这里以markdown和latex为例。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(df.to_markdown())</span><br><span class="line"><span class="built_in">print</span>(df.to_latex())</span><br></pre></td></tr></table></figure>
<p>or</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.to_markdown(<span class="string">&#x27;table.md&#x27;</span>)</span><br><span class="line">df.to_latex(<span class="string">&#x27;table.tex&#x27;</span>)</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<p>也可以自定义输出latex格式，如表格宽度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.to_latex(<span class="string">&#x27;tb.tex&#x27;</span>,column_format=<span class="string">&#x27;lp&#123;1.8cm&#125;p&#123;1.8cm&#125;p&#123;1.8cm&#125;p&#123;1.8cm&#125;p&#123;1.8cm&#125;p&#123;1.8cm&#125;&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>除此以外，dataframe还可以保存为图片。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> dataframe_image <span class="keyword">as</span> dfi</span><br><span class="line">dfi.export(obj = df, filename = <span class="string">&#x27;table.jpg&#x27;</span>, fontsize=<span class="number">15</span>)</span><br></pre></td></tr></table></figure>
<h4 id="2-DataFrame常用属性查询"><a href="#2-DataFrame常用属性查询" class="headerlink" title="2.DataFrame常用属性查询"></a>2.DataFrame常用属性查询</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.values	<span class="comment">#值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.index	<span class="comment">#索引号</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.columns	<span class="comment">#列标签</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.dtypes	<span class="comment">#数据类型</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.shape	<span class="comment">#形状(几行几列)</span></span><br></pre></td></tr></table></figure>
<h4 id="3-DataFrame常用基本函数"><a href="#3-DataFrame常用基本函数" class="headerlink" title="3.DataFrame常用基本函数"></a>3.DataFrame常用基本函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.head(<span class="number">5</span>)	<span class="comment">#前5行</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.tail(<span class="number">5</span>)	<span class="comment">#后5行</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.info()	<span class="comment">#信息概况</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.describe()	<span class="comment">#主要统计量(count、mean、std、max、min、quartile)</span></span><br></pre></td></tr></table></figure>
<h4 id="4-DataFrame唯一值函数"><a href="#4-DataFrame唯一值函数" class="headerlink" title="4.DataFrame唯一值函数"></a>4.DataFrame唯一值函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].unique()	<span class="comment">#唯一值组成的数组</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].nunique()    <span class="comment">#唯一值个数</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].value_counts()    <span class="comment">#唯一值及其频数</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.describe()    <span class="comment">#主要统计量(count、mean、std、max、min、quartile)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].duplicated()    <span class="comment">#重复行的布尔值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[df[<span class="string">&#x27;psavert&#x27;</span>].duplicated()]    <span class="comment">#单列去重(删除重复行)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.drop_duplicates(subset=[<span class="string">&#x27;psavert&#x27;</span>, <span class="string">&#x27;pop&#x27;</span>], keep=<span class="string">&#x27;first&#x27;</span>)    <span class="comment">#多列去重(保留first唯一值)</span></span><br></pre></td></tr></table></figure>
<h4 id="5-DataFrame替换函数"><a href="#5-DataFrame替换函数" class="headerlink" title="5.DataFrame替换函数"></a>5.DataFrame替换函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].replace(<span class="number">12.5</span>, <span class="string">&#x27;A&#x27;</span>, inplace = <span class="literal">True</span>)    <span class="comment">#替换某列的单个值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].replace(&#123;<span class="number">12.5</span>:<span class="string">&#x27;A&#x27;</span>, <span class="number">11.7</span>:<span class="string">&#x27;B&#x27;</span>&#125;, inplace = <span class="literal">True</span>)    <span class="comment">#替换某列的多个值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;date&#x27;</span>].replace(&#123;<span class="string">r&#x27;2\d+&#x27;</span>: <span class="string">&#x27;The 21st century&#x27;</span>&#125;, regex=<span class="literal">True</span>, inplace = <span class="literal">True</span>)    <span class="comment">#正则替换</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].mask(df[<span class="string">&#x27;psavert&#x27;</span>]&gt;<span class="number">12.0</span> ,<span class="string">&#x27;A&#x27;</span>, inplace = <span class="literal">True</span>)    <span class="comment">#条件符合，进行替换</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;psavert&#x27;</span>].where(df[<span class="string">&#x27;psavert&#x27;</span>]&lt;<span class="number">12.0</span> ,<span class="string">&#x27;A&#x27;</span>, inplace = <span class="literal">True</span>)    <span class="comment">#条件不符合，进行替换</span></span><br></pre></td></tr></table></figure>
<h4 id="5-DataFrame排序函数"><a href="#5-DataFrame排序函数" class="headerlink" title="5.DataFrame排序函数"></a>5.DataFrame排序函数</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.sort_values(<span class="string">&#x27;psavert&#x27;</span>,ascending = <span class="literal">False</span>)    <span class="comment">#单列降序排序</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.sort_values([<span class="string">&#x27;psavert&#x27;</span>,<span class="string">&#x27;uempmed&#x27;</span>],ascending=[<span class="literal">True</span>,<span class="literal">False</span>])    <span class="comment">#前者升序情况下，后降序</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>今天先写到这，下一期接着写DataFrame的apply方法。</p>
</blockquote>
]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>DataFrame</tag>
      </tags>
  </entry>
  <entry>
    <title>Pandas的一些常用操作_3</title>
    <url>/2021/02/21/pandas_3/</url>
    <content><![CDATA[<blockquote>
<p>今天介绍Pandas对一些常见数据的处理方法。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;./economics.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="1-缺失数据处理"><a href="#1-缺失数据处理" class="headerlink" title="1.缺失数据处理"></a>1.缺失数据处理</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.isna()<span class="comment">#是否有缺失值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.isna().mean()<span class="comment">#缺失的比例 </span></span><br></pre></td></tr></table></figure>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[df.psavert.isna()]<span class="comment">#查看某列是否有缺失值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[df[[<span class="string">&#x27;psavert&#x27;</span>,<span class="string">&#x27;pop&#x27;</span>,<span class="string">&#x27;uempmed&#x27;</span>]].isna().<span class="built_in">any</span>(<span class="number">1</span>)]<span class="comment">#查看所有列至少有一个缺失值的行“any()至少有一个为空,all()都为空”</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[df[[<span class="string">&#x27;psavert&#x27;</span>,<span class="string">&#x27;pop&#x27;</span>,<span class="string">&#x27;uempmed&#x27;</span>]].notna().<span class="built_in">all</span>(<span class="number">1</span>)]<span class="comment">#查看所有没有缺失值的行</span></span><br><span class="line">df.loc[df[[<span class="string">&#x27;psavert&#x27;</span>,<span class="string">&#x27;pop&#x27;</span>,<span class="string">&#x27;uempmed&#x27;</span>]].notna().<span class="built_in">all</span>(<span class="number">1</span>)]<span class="comment">#查看所有没有缺失值的区域</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.dropna(axis = <span class="number">0</span>, how = <span class="string">&#x27;any&#x27;</span>, subset = [<span class="string">&#x27;psavert&#x27;</span>, <span class="string">&#x27;pop&#x27;</span>])<span class="comment">#axis=0(删除)行,how=&#x27;any&#x27;至少有一个缺失的行</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.dropna(axis = <span class="number">1</span>, thresh = df.shape[<span class="number">0</span>]-<span class="number">5</span>)<span class="comment">#删除超过5个缺失值的列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.fillna(method = <span class="string">&#x27;ffill&#x27;</span>, limit = <span class="number">1</span>)<span class="comment">#method=&#x27;ffill&#x27;用前面的元素填充/method=&#x27;bfill&#x27;用后面的元素填充,limit=1连续缺失值的最大填充次数为1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.fillna(df.mean())<span class="comment">#用每列的均值填充</span></span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.interpolate(limit_direction=<span class="string">&#x27;both&#x27;</span>, limit=<span class="number">1</span>)<span class="comment">#用线性插值填充(both为双向限制插值)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.interpolate(<span class="string">&#x27;nearest&#x27;</span>).values<span class="comment">#用最近邻插值填充</span></span><br></pre></td></tr></table></figure>
<p>此外，也可以<a href="https://blog.csdn.net/wj1298250240/article/details/103600075">使用KNN来填充缺失值</a>。</p>
<h4 id="2-文本数据处理"><a href="#2-文本数据处理" class="headerlink" title="2.文本数据处理"></a>2.文本数据处理</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts = pd.Series(df[<span class="string">&#x27;Area&#x27;</span>].values, index=df[<span class="string">&#x27;pct_2014&#x27;</span>])<span class="comment">#DataFrame转换为Series</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>[<span class="number">0</span>]<span class="comment">#查看第一个字符</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.<span class="built_in">len</span>()<span class="comment">#查看字符长度</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.split(<span class="string">&#x27;[aon]&#x27;</span>, n=<span class="number">3</span>, expand=<span class="literal">True</span>)<span class="comment">#从左到右拆分字符串，最大拆分次数3次，生成多列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.join(<span class="string">&#x27;-&#x27;</span>)<span class="comment">#每个字符用“-”连接</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.cat(ts2, sep=<span class="string">&#x27;-&#x27;</span>)<span class="comment">#合并两个字符Series，连接符为“-”</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.contains(<span class="string">&#x27;[a-z]u&#x27;</span>)<span class="comment">#查看包含正则模式的序列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.startswith(<span class="string">&#x27;A&#x27;</span>)<span class="comment">#查看以A为开始的序列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.endswith(<span class="string">&#x27;e&#x27;</span>)<span class="comment">#查看以e为结尾的序列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.match(<span class="string">&#x27;A|s&#x27;</span>)<span class="comment">#查看以A为开头,s为结尾的序列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.count(<span class="string">&#x27;[A|s]&#x27;</span>)<span class="comment">#查看以A为开头,s为结尾的序列数量</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.find(<span class="string">&#x27;de&#x27;</span>)<span class="comment">#从左往右寻找&#x27;de&#x27;,匹配返回位置索引,未找到返回&#x27;-1&#x27;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.rfind(<span class="string">&#x27;de&#x27;</span>)<span class="comment">#从右往左寻找</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.replace(<span class="string">&#x27;\s?&#x27;</span>, <span class="string">&#x27;LA&#x27;</span>, regex=<span class="literal">True</span>)<span class="comment">#使用正则进行字符串替换</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pat = <span class="string">&#x27;(\w+o)(\w+s)(\w+u)(\w+n)&#x27;</span></span><br><span class="line">ts.<span class="built_in">str</span>.extract(pat)<span class="comment">#拆分&#x27;o&#x27;,&#x27;s&#x27;,&#x27;u&#x27;,&#x27;n&#x27;为4列</span></span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.to_numeric(ts, errors=<span class="string">&#x27;ignore&#x27;</span>)<span class="comment">#将可以转为数值的字符转为数值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.pad(<span class="number">6</span>,<span class="string">&#x27;left&#x27;</span>,<span class="string">&#x27;*&#x27;</span>)<span class="comment">#选定字符串长度为6的填充为&quot;*&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.lstrip()<span class="comment">#去掉字符串左侧空格</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ts.<span class="built_in">str</span>.zfill(<span class="number">8</span>)<span class="comment">#用0补足8位</span></span><br></pre></td></tr></table></figure>
<h4 id="3-分类数据"><a href="#3-分类数据" class="headerlink" title="3.分类数据"></a>3.分类数据</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = df.psavert.astype(<span class="string">&#x27;category&#x27;</span>)<span class="comment">#Dataframe转为category对象</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s.cat.categories<span class="comment">#查看分类对象属性</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s.cat.add_categories(<span class="string">&#x27;C1&#x27;</span>)<span class="comment">#增加一个类别</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s.cat.remove_categories(<span class="number">11.7</span>)<span class="comment">#删除一个类别</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = s.cat.rename_categories(&#123;<span class="string">&#x27;S1&#x27;</span>:<span class="string">&#x27;xxx&#x27;</span>&#125;)<span class="comment">#重命名类别及其值</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s.cat.reorder_categories([<span class="string">&#x27;S1&#x27;</span>, <span class="string">&#x27;S2&#x27;</span>, <span class="string">&#x27;S3&#x27;</span>, <span class="string">&#x27;S4&#x27;</span>], ordered=<span class="literal">True</span>)<span class="comment">#设置排序</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.sort_values(<span class="string">&#x27;psavert&#x27;</span>)<span class="comment">#值排序</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.set_index(<span class="string">&#x27;psavert&#x27;</span>).sort_index()<span class="comment">#作为索引排序</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">res = df.psavert &lt;= <span class="string">&#x27;S4&#x27;</span><span class="comment">#比较顺序</span></span><br><span class="line">res</span><br></pre></td></tr></table></figure>
<h4 id="4-时序数据"><a href="#4-时序数据" class="headerlink" title="4.时序数据"></a>4.时序数据</h4><p>————-图——————–</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = pd.to_datetime(df.date)<span class="comment">#生成时间序列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s2 = pd.to_datetime([<span class="string">&#x27;2020\\1\\1&#x27;</span>,<span class="string">&#x27;2020\\1\\3&#x27;</span>],<span class="built_in">format</span>=<span class="string">&#x27;%Y\\%m\\%d&#x27;</span>)<span class="comment">#强制格式转换,生成时间序列</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.date_range(<span class="string">&#x27;1967-07-01&#x27;</span>,<span class="string">&#x27;2015-04-01&#x27;</span>, freq=<span class="string">&#x27;D&#x27;</span>)<span class="comment">#查看两个时间之间的时间,间隔1天</span></span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#dt对象</span></span><br><span class="line">s.dt.daysinmonth<span class="comment">#每个月几天</span></span><br><span class="line">s.dt.dayofweek<span class="comment">#每周几天</span></span><br><span class="line">s.dt.dayofweek.isin([<span class="number">5</span>,<span class="number">6</span>])<span class="comment">#是否包含双休日</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.Timestamp(<span class="string">&#x27;20210223 22:00:00&#x27;</span>)-pd.Timestamp(<span class="string">&#x27;20210222 18:35:00&#x27;</span>)<span class="comment">#计算两个时间之差</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>DataFrame</tag>
      </tags>
  </entry>
  <entry>
    <title>Tsfresh——自动化特征工程工具</title>
    <url>/2021/03/25/tsfresh/</url>
    <content><![CDATA[<blockquote>
<p>改进模型的潜在途径之一是：生成更多的潜在特征，输入更多的样本。</p>
</blockquote>
<p>Tsfresh是处理时间序列数据的特征工程工具，能够自动计算大量时间序列特征，如平均值、最大值、峰度等。之后，可以使用这些特征集构建机器学习模型。</p>
<p>本文以<em>天池-心跳信号分类预测</em>为例，演示tsfresh工具的用法。</p>
<h2 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h2><h5 id="1-合并train和test数据"><a href="#1-合并train和test数据" class="headerlink" title="1. 合并train和test数据"></a>1. 合并train和test数据</h5><p>合并数据集，对整体数据做统一的特征工程。(注意需要为test数据添加label列，值为-1，方便后续操作)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_test[<span class="string">&#x27;label&#x27;</span>] = -<span class="number">1</span></span><br><span class="line">all_data = pd.concat((data_train, data_test)).reset_index(drop = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h5 id="2-对原特征一列拆成多列，并为每条数据添加时间特征time"><a href="#2-对原特征一列拆成多列，并为每条数据添加时间特征time" class="headerlink" title="2. 对原特征一列拆成多列，并为每条数据添加时间特征time"></a>2. 对原特征一列拆成多列，并为每条数据添加时间特征time</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_heatbeat_df = all_data[<span class="string">&#x27;heartbeat_signals&#x27;</span>].<span class="built_in">str</span>.split(<span class="string">&#x27;,&#x27;</span>, expand = <span class="literal">True</span>).stack()</span><br></pre></td></tr></table></figure>
<h5 id="3-Index处理"><a href="#3-Index处理" class="headerlink" title="3. Index处理"></a>3. Index处理</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_heatbeat_df = all_heatbeat_df.reset_inex()</span><br><span class="line">all_heatbeat_df = all_heatbeat_df.set_inex(<span class="string">&#x27;level_0&#x27;</span>)</span><br><span class="line">all_heatbeat_df.index.name = <span class="literal">None</span></span><br><span class="line">all_heatbeat_df.rename(columns=&#123;<span class="string">&#x27;level_1&#x27;</span>:<span class="string">&#x27;time&#x27;</span>, <span class="number">0</span>:<span class="string">&#x27;heartbeat_signals&#x27;</span>, inpalce = <span class="literal">True</span>&#125;)</span><br><span class="line">all_heatbeat_df[<span class="string">&#x27;heartbeat_signals&#x27;</span>].astype(<span class="built_in">float</span>)</span><br></pre></td></tr></table></figure>

<h5 id="4-label列单独存储，不进入tsfresh"><a href="#4-label列单独存储，不进入tsfresh" class="headerlink" title="4. label列单独存储，不进入tsfresh"></a>4. label列单独存储，不进入tsfresh</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">all_data_label = all_data[<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">all_data = all_data.drop[<span class="string">&#x27;label&#x27;</span>, axis=<span class="number">1</span>].drop[<span class="string">&#x27;heartbeat_signals&#x27;</span>, axis=<span class="number">1</span>]</span><br><span class="line">all_data = all_data.join(all_heatbeat_df)</span><br></pre></td></tr></table></figure>

<h5 id="5-tsfresh特征抽取"><a href="#5-tsfresh特征抽取" class="headerlink" title="5. tsfresh特征抽取"></a>5. tsfresh特征抽取</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tsfresh <span class="keyword">import</span> extract_features</span><br><span class="line">all_features = extract_features(all_data, column_id=<span class="string">&#x27;id&#x27;</span>, column_sort=<span class="string">&#x27;time&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h5 id="6-特征选择"><a href="#6-特征选择" class="headerlink" title="6.特征选择"></a>6.特征选择</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#删除nan值</span></span><br><span class="line"><span class="keyword">from</span> tsfresh.utilities.dataframe_functions <span class="keyword">import</span> impute</span><br><span class="line">impute(all_features)</span><br></pre></td></tr></table></figure>

<h5 id="7-相关性特征提取"><a href="#7-相关性特征提取" class="headerlink" title="7.相关性特征提取"></a>7.相关性特征提取</h5><p>衍生众多特征之后，许多特征之间可能有很多相关性，需进一步筛选。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tsfresh <span class="keyword">import</span> select_features</span><br><span class="line">all_features_filtered = select_features(all_features, all_data_label)</span><br></pre></td></tr></table></figure>
<h5 id="8-特征重命名，重新添加label"><a href="#8-特征重命名，重新添加label" class="headerlink" title="8.特征重命名，重新添加label"></a>8.特征重命名，重新添加label</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num = all_features_filtered.columns.size</span><br><span class="line">all_features_filtered.columns = [<span class="string">&#x27;f_&#x27;</span> + <span class="built_in">str</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span> (<span class="number">0</span>, num)]</span><br><span class="line">all_features_filtered[<span class="string">&#x27;label&#x27;</span>] = all_data_label</span><br></pre></td></tr></table></figure>

<h4 id="tsfresh包深入探究"><a href="#tsfresh包深入探究" class="headerlink" title="tsfresh包深入探究"></a>tsfresh包深入探究</h4><h5 id="1-筛选特征的方法"><a href="#1-筛选特征的方法" class="headerlink" title="1. 筛选特征的方法"></a>1. 筛选特征的方法</h5><p>上文采用了手工打标签的方式划分训练集和测试集，略显麻烦，可以通过tsfresh的内置方法来提取训练数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">kind_to_fc_parameters = tsfresh.feature_extration.settings.from_columns(directed_features)</span><br></pre></td></tr></table></figure>
<p>生成的params为训练集的特征字典，可以当作参数传入后续计算中。</p>
<h5 id="2-自定义特征衍生规则"><a href="#2-自定义特征衍生规则" class="headerlink" title="2. 自定义特征衍生规则"></a>2. 自定义特征衍生规则</h5><p>tsfresh自带的衍生规则以字典的形式存放，可以直接调用，也可以自定义。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#单个特征计算</span></span><br><span class="line">tsfresh.feature_extraction.feature_calculators.abs_energy(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">#设定衍生规则</span></span><br><span class="line"><span class="keyword">from</span> tsfresh.featureextraction <span class="keyword">import</span> extractfeatures</span><br><span class="line">params = &#123;<span class="string">&#x27;fft_coefficient:[&#123;&#x27;</span>coe<span class="string">f&#x27;:0, &#x27;</span>att<span class="string">r&#x27;:&#x27;</span><span class="built_in">abs</span><span class="string">&#x27;&#125;], &#x27;</span>kurtosis<span class="string">&#x27;: None, &#x27;</span>skewness<span class="string">&#x27;: None&#125;</span></span><br></pre></td></tr></table></figure>

<p>后续可以在extractfeatures中设置defaultparameters = params，具体用法见<a href="https://tsfresh.readthedocs.io/en/latest/text/feature_extraction_settings.html">操作文档</a>。</p>
<h5 id="3-减小内存使用"><a href="#3-减小内存使用" class="headerlink" title="3. 减小内存使用"></a>3. 减小内存使用</h5><p>tsfresh默认参数太吃内存，且耗时长，笔者i7的16G笔记本最多只能跑到60%的进度就会卡住，天池notebook和谷歌colab都是没跑出结果就断线了，硬要跑的话只能租个高配服务器，或者调整衍生特征的数量，通过<strong>chunksize</strong>设置，具体用法见<a href="https://tsfresh.readthedocs.io/en/latest/text/feature_extraction_settings.html">操作文档</a>。</p>
]]></content>
      <categories>
        <category>Feature engineering</category>
      </categories>
      <tags>
        <tag>tsfresh</tag>
      </tags>
  </entry>
  <entry>
    <title>图神经网络——基本图论与PyG库</title>
    <url>/2021/06/15/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E5%9B%BE%E8%AE%BA%E4%B8%8EPyG%E5%BA%93/</url>
    <content><![CDATA[<p>在以往的深度学习中，主要的数据形式包括矩阵、张量、序列(sequence)和时间序列(time series)，然而还有许多数据是图的结构，如社交网络、知识图谱等。图结构数据具有以下特点：</p>
<blockquote>
<p>任意的大小和复杂的拓扑结构；<br>没有固定的节点排序或参考点；<br>通常是动态的，并具有多模态的特征；<br>图的信息包括节点信息、边信息和拓扑结构信息。</p>
</blockquote>
<h4 id="1-图结构数据"><a href="#1-图结构数据" class="headerlink" title="1. 图结构数据"></a>1. 图结构数据</h4><h5 id="1-1-图-Graphs"><a href="#1-1-图-Graphs" class="headerlink" title="1.1 图(Graphs)"></a>1.1 图(Graphs)</h5><p><strong>·</strong> 一个图记作$\mathcal{G}={\mathcal{V}, \mathcal{E}}$，其中 $\mathcal{V}=\left{v_{1}, \ldots, v_{N}\right}$是数量为$N=|\mathcal{V}|$ 的节点的集合， $\mathcal{E}=\left{e_{1}, \ldots, e_{M}\right}$ 是数量为 $M$ 的边的集合。<br><strong>·</strong> 节点表示实体(entities)，边表示实体间的关系(relations)；节点和边的信息可以是类别型或数值型的。<br><strong>·</strong> 只有一种类型的节点和一种类型的边的图称为同质图(Homogeneous Graph)。<br><strong>·</strong> 存在多种类型的节点和多种类型的边的图称为异质图(Heterogeneous Graph)</p>
<span id="more"></span>

<h5 id="1-2-图的邻接矩阵-Adjacency-Matrix"><a href="#1-2-图的邻接矩阵-Adjacency-Matrix" class="headerlink" title="1.2 图的邻接矩阵(Adjacency Matrix)"></a>1.2 图的邻接矩阵(Adjacency Matrix)</h5><p><strong>·</strong> 图$\mathcal{G}={\mathcal{V}, \mathcal{E}}$，其对应的邻接矩阵记为$\mathbf{A} \in{0,1}^{N \times N}$。$\mathbf{A}<em>{i, j}=1$表示存在从节点$v_i$到$v_j$的边，反之表示不存在从节点$v_i$到$v_j$的边。<br><strong>·</strong> 在无向图中，节点$v_i$到$v_j$的边存在，则节点$v_j$到$v_i$的边也存在，所以无向图的邻接矩阵的对称的。<br><img src="https://z3.ax1x.com/2021/06/28/RtabM8.png" alt="图1"><br>其邻接矩阵为：<br><img src="https://z3.ax1x.com/2021/06/28/RtaoGt.png" alt="图2"><br><strong>·</strong> 在无权图中，各条边的权重是等价的，都为1。<br><strong>·</strong> 在有权图中，其对应的邻接矩阵通常被记为$\mathbf{W} \in{0,1}^{N \times N}$，其中$\mathbf{W}</em>{i, j}=w_{ij}$表示从节点$v_i$到$v_j$的边的权重。若边不存在时，边的权重为$0$。</p>
<h5 id="1-3-节点的度-degree"><a href="#1-3-节点的度-degree" class="headerlink" title="1.3 节点的度(degree)"></a>1.3 节点的度(degree)</h5><p><strong>·</strong> 在有向有权图中，节点$v_i$的出度(out degree)等于从$v_i$出发的边的权重之和，节点$v_i$的入度(in degree)等于从连向$v_i$的边的权重之和；节点$v_i$的度记为$d(v_i)$，入度记为$d_{in}(v_i)$，出度记为$d_{out}(v_i)$。<br><strong>·</strong> 在无向图中，出度=入度。<br><strong>·</strong> 在无权图中，入读等于连向$v_i$的边的数量，出度等于从$v_i$出发的边的数量。</p>
<h5 id="1-4-邻接节点-neighbors"><a href="#1-4-邻接节点-neighbors" class="headerlink" title="1.4 邻接节点(neighbors)"></a>1.4 邻接节点(neighbors)</h5><p><strong>·</strong> 与节点$v_i$直接相连的节点称为邻接节点，记为**$\mathcal{N(v_i)}$**。<br><strong>·</strong> 与节点$v_i$距离$k$步的称为$k$跳远的邻接节点(neighbors with $k$-hop) [一个节点的$2$跳远的邻接节点包含自身]</p>
<h5 id="1-5-行走-walk"><a href="#1-5-行走-walk" class="headerlink" title="1.5 行走(walk)"></a>1.5 行走(walk)</h5><p><strong>·</strong> $walk(v_1, v_2) = (v_1, e_6,e_5,e_4,e_1,v_2)$，这是一次“行走”，从节点$v_1$出发，依次经过边$e_6,e_5,e_4,e_1$，最终到达节点$v_2$的“行走”。<br><strong>·</strong> “行走”中，节点允许重复。<br><img src="https://z3.ax1x.com/2021/06/28/Rta7xf.png" alt="图3"></p>
<h5 id="1-6-路径-path"><a href="#1-6-路径-path" class="headerlink" title="1.6 路径(path)"></a>1.6 路径(path)</h5><p>“路径”是节点不可重复的“行走”。</p>
<h5 id="1-7-子图-subgraph"><a href="#1-7-子图-subgraph" class="headerlink" title="1.7 子图(subgraph)"></a>1.7 子图(subgraph)</h5><p>指节点集和边集分别是整图的节点集的子集和边集的子集的图。</p>
<h5 id="1-8-连通分量-connected-component"><a href="#1-8-连通分量-connected-component" class="headerlink" title="1.8 连通分量(connected component)"></a>1.8 连通分量(connected component)</h5><p>图$\mathcal{G}={\mathcal{V}, \mathcal{E}}$的子图为$\mathcal{G}^{\prime}$，记属于图$\mathcal{G}$但不属于$\mathcal{G}^{\prime}$图的节点集合记为$\mathcal{V}/\mathcal{V}^{\prime}$，属于$\mathcal{V}^{\prime}$的任意节点对之间存在至少一条路径，但不存在一条边连接属于$\mathcal{V}^{\prime}$的节点与属于$\mathcal{V}/\mathcal{V}^{\prime}$的节点，则图$\mathcal{G}^{\prime}$是图$\mathcal{G}$的连通分量。<br><img src="https://z3.ax1x.com/2021/06/28/RtaTRP.png" alt="图4"><br>左右两个子图都是整图的连通分量。</p>
<h5 id="1-8-连通图-connected-graph"><a href="#1-8-连通图-connected-graph" class="headerlink" title="1.8 连通图(connected graph)"></a>1.8 连通图(connected graph)</h5><p>只包含一个连通分量(即其自身)的图是一个连通图。</p>
<h5 id="1-9-最短路径-shortest-path"><a href="#1-9-最短路径-shortest-path" class="headerlink" title="1.9 最短路径(shortest path)"></a>1.9 最短路径(shortest path)</h5><p>$v_{s}, v_{t} \in \mathcal{V}$ 是图$\mathcal{G}={\mathcal{V}, \mathcal{E}}$上的一对节点，节点对$v_{s}, v_{t} \in \mathcal{V}$之间所有路径集合$\mathcal{P}<em>{\mathrm{st}}$。节点对$v</em>{s}, v_{t}$之间的最短路径$p_{\mathrm{s} t}^{\mathrm{sp}}$为$\mathcal{P}<em>{\mathrm{st}}$中长度最短的一条路径，即$p</em>{\mathrm{s} t}^{\mathrm{sp}}=\arg \min <em>{p \in \mathcal{P}</em>{\mathrm{st}}}|p|$，$p$表示$\mathcal{P}_{\mathrm{st}}$中的一条路径，$|p|$是路径$p$的长度(边数量×权重)。</p>
<h5 id="1-10-直径-diameter"><a href="#1-10-直径-diameter" class="headerlink" title="1.10 直径(diameter)"></a>1.10 直径(diameter)</h5><p>一个连通图$\mathcal{G}={\mathcal{V}, \mathcal{E}}$中，其直径为其所有节点对之间的最短路径的最大值，即$\operatorname{diameter}(\mathcal{G})=\max <em>{v</em>{s}, v_{t} \in \mathcal{V}} \min <em>{p \in \mathcal{P}</em>{s t}}|p|$</p>
<h5 id="1-11-拉普拉斯矩阵-Laplacian-Matrix"><a href="#1-11-拉普拉斯矩阵-Laplacian-Matrix" class="headerlink" title="1.11 拉普拉斯矩阵(Laplacian Matrix)"></a>1.11 拉普拉斯矩阵(Laplacian Matrix)</h5><p>图$\mathcal{G}={\mathcal{V}, \mathcal{E}}$，其邻接矩阵为$A$，其拉普拉斯矩阵定义为$\mathbf{L=D-A}$，其中$\mathbf{D=diag(d(v_1), \cdots, d(v_N))}$。</p>
<h5 id="1-12-对称归一化的拉普拉斯矩阵-Symmetric-normalized-Laplacian"><a href="#1-12-对称归一化的拉普拉斯矩阵-Symmetric-normalized-Laplacian" class="headerlink" title="1.12 对称归一化的拉普拉斯矩阵(Symmetric normalized Laplacian)"></a>1.12 对称归一化的拉普拉斯矩阵(Symmetric normalized Laplacian)</h5><p>图$\mathcal{G}={\mathcal{V}, \mathcal{E}}$，其邻接矩阵为$A$，其规范化的拉普拉斯矩阵定义为$\mathbf{L=D^{-\frac{1}{2}}(D-A)D^{-\frac{1}{2}}=I-D^{-\frac{1}{2}}AD^{-\frac{1}{2}}}$。</p>
<h4 id="2-图结构数据上的机器学习"><a href="#2-图结构数据上的机器学习" class="headerlink" title="2. 图结构数据上的机器学习"></a>2. 图结构数据上的机器学习</h4><blockquote>
<p><strong>节点预测</strong>：预测节点的类别或某类属性的取值。<br><strong>边预测</strong>：预测两个节点间是否存在链接。<br><strong>图预测</strong>：对不同的图进行分类或预测图的属性。<br><strong>节点聚类</strong>：检测节点是否形成一个类。</p>
</blockquote>
<h4 id="3-环境配置"><a href="#3-环境配置" class="headerlink" title="3. 环境配置"></a>3. 环境配置</h4><p><a href="https://pytorch-geometric.readthedocs.io/en/latest/">PyTorch Geometric</a> (PyG)是面向几何深度学习的PyTorch的扩展库，基于PyG库，我们可以轻松地根据数据生成一个图对象。</p>
<h5 id="3-1-安装的pytorch和cudatoolkit"><a href="#3-1-安装的pytorch和cudatoolkit" class="headerlink" title="3.1 安装的pytorch和cudatoolkit"></a>3.1 安装的pytorch和cudatoolkit</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda install pytorch torchvision torchaudio cudatoolkit&#x3D;11.1 -c pytorch -c nvidia</span><br></pre></td></tr></table></figure>
<p>安装成功后可以验证一下。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)</span><br><span class="line"><span class="built_in">print</span>(torch.version.cuda)</span><br></pre></td></tr></table></figure>
<h5 id="3-2-安装PyG"><a href="#3-2-安装PyG" class="headerlink" title="3.2 安装PyG"></a>3.2 安装PyG</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-<span class="number">1.8</span><span class="number">.0</span>+cu111.html</span><br><span class="line">pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-<span class="number">1.8</span><span class="number">.0</span>+cu111.html</span><br><span class="line">pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-<span class="number">1.8</span><span class="number">.0</span>+cu111.html</span><br><span class="line">pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-<span class="number">1.8</span><span class="number">.0</span>+cu111.html</span><br><span class="line">pip install torch-geometric</span><br></pre></td></tr></table></figure>

<h4 id="4-Data类——PyG中图的表示"><a href="#4-Data类——PyG中图的表示" class="headerlink" title="4. Data类——PyG中图的表示"></a>4. Data类——PyG中图的表示</h4><h5 id="4-1-构造函数"><a href="#4-1-构造函数" class="headerlink" title="4.1 构造函数"></a>4.1 构造函数</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Data</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, x=<span class="literal">None</span>, edge_index=<span class="literal">None</span>, edge_attr=<span class="literal">None</span>, y=<span class="literal">None</span>, **kwargs</span>):</span></span><br><span class="line">    self.x = x</span><br><span class="line">    self.edge_index = edge_index</span><br><span class="line">    self.edge_attr = edge_attr</span><br><span class="line">    self.y = y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> key, item <span class="keyword">in</span> kwargs.items():</span><br><span class="line">        <span class="keyword">if</span> key == <span class="string">&#x27;num_nodes&#x27;</span>:</span><br><span class="line">            self.__num_nodes__ = item</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self[key] = item</span><br></pre></td></tr></table></figure>
<p>Args:<br><strong>·</strong> x (Tensor, optional): 节点属性矩阵，大小为<code>[num_nodes, num_node_features]</code><br><strong>·</strong> edge_index (LongTensor, optional): 边索引矩阵，大小为<code>[2, num_edges]</code>，第0行为尾节点，第1行为头节点，头指向尾。<br><strong>·</strong> edge_attr (Tensor, optional): 边属性矩阵，大小为<code>[num_edges, num_edge_features]</code><br><strong>·</strong> y (Tensor, optional): 节点或图的标签，任意大小(其实也可以是边的标签)</p>
<h5 id="4-2-参数"><a href="#4-2-参数" class="headerlink" title="4.2 参数"></a>4.2 参数</h5><p>一个图至少包含<code>x, edge_index, edge_attr, y, num_nodes</code>5个属性，也可以指定额外参数使<code>Data</code>对象包含其他的属性：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">graph = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y, num_nodes=num_nodes, other_attr=other_attr)</span><br></pre></td></tr></table></figure>

<h5 id="4-3-Data对象与其他数据互转"><a href="#4-3-Data对象与其他数据互转" class="headerlink" title="4.3 Data对象与其他数据互转"></a>4.3 Data对象与其他数据互转</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#dict to Data</span></span><br><span class="line">graph_dict = &#123;</span><br><span class="line">    <span class="string">&#x27;x&#x27;</span>: x,</span><br><span class="line">    <span class="string">&#x27;edge_index&#x27;</span>: edge_index,</span><br><span class="line">    <span class="string">&#x27;edge_attr&#x27;</span>: edge_attr,</span><br><span class="line">    <span class="string">&#x27;y&#x27;</span>: y,</span><br><span class="line">    <span class="string">&#x27;num_nodes&#x27;</span>: num_nodes,</span><br><span class="line">    <span class="string">&#x27;other_attr&#x27;</span>: other_attr</span><br><span class="line">&#125;</span><br><span class="line">graph_data = Data.from_dict(graph_dict)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Data to dict</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Data_to_dict</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">return</span> &#123;key: item <span class="keyword">for</span> key, item <span class="keyword">in</span> self&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Data to namedtuple</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Data_to_namedtuple</span>(<span class="params">self</span>):</span></span><br><span class="line">    keys = self.keys</span><br><span class="line">    DataTuple = collections.namedtuple(<span class="string">&#x27;DataTuple&#x27;</span>, keys)</span><br><span class="line">    <span class="keyword">return</span> DataTuple(*[self[key] <span class="keyword">for</span> key <span class="keyword">in</span> keys])</span><br></pre></td></tr></table></figure>

<h5 id="4-4-Data对象属性"><a href="#4-4-Data对象属性" class="headerlink" title="4.4 Data对象属性"></a>4.4 Data对象属性</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#获取Data对象属性</span></span><br><span class="line">x = graph_data[<span class="string">&#x27;x&#x27;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#获取Data对象属性关键字</span></span><br><span class="line">graph_data.keys()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#对边排序并移除重复的边</span></span><br><span class="line">graph_data.coalesce()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> KarateClub</span><br><span class="line">dataset = KarateClub()</span><br><span class="line">data = dataset[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(data.num_nodes) <span class="comment">#节点数量</span></span><br><span class="line"><span class="built_in">print</span>(data.num_edges) <span class="comment">#边数量</span></span><br><span class="line"><span class="built_in">print</span>(data.num_node_features) <span class="comment">#节点属性维度</span></span><br><span class="line"><span class="built_in">print</span>(data.num_edge_features) <span class="comment">#边属性维度</span></span><br><span class="line"><span class="built_in">print</span>(data.num_edges/data.num_nodes) <span class="comment">#平均节点度</span></span><br><span class="line"><span class="built_in">print</span>(data.is_coalesced()) <span class="comment">#是否边是有序的同时不含有重复的边</span></span><br><span class="line"><span class="built_in">print</span>(data.train_mask.<span class="built_in">sum</span>()) <span class="comment">#用作训练集的节点</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">int</span>(data.train_mask.<span class="built_in">sum</span>())/data.num_nodes) <span class="comment">#用作训练集的节点数占比</span></span><br><span class="line"><span class="built_in">print</span>(data.contains_isolated_nodes()) <span class="comment">#此图是否包含孤立的节点</span></span><br><span class="line"><span class="built_in">print</span>(data.contains_self_loops()) <span class="comment">#此图是否包含自环的边</span></span><br><span class="line"><span class="built_in">print</span>(data.is_undirected()) <span class="comment">#此图是否是无向图</span></span><br></pre></td></tr></table></figure>

<h4 id="5-Dataset类——PyG中图数据集的表示"><a href="#5-Dataset类——PyG中图数据集的表示" class="headerlink" title="5.Dataset类——PyG中图数据集的表示"></a>5.Dataset类——PyG中图数据集的表示</h4><h5 id="5-1-数据集的下载"><a href="#5-1-数据集的下载" class="headerlink" title="5.1 数据集的下载"></a>5.1 数据集的下载</h5><p>首先下载PyG内置的Planetoid数据集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> Planetoid</span><br><span class="line">dataset = Planetoid(root=<span class="string">&#x27;/dataset/Cora&#x27;</span>, name=<span class="string">&#x27;Cora&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(dataset))    <span class="comment">#数据集中图的数量</span></span><br><span class="line"><span class="built_in">print</span>(dataset.num_classes)   <span class="comment">#分类任务的数量</span></span><br><span class="line"><span class="built_in">print</span>(dataset.num_node_features)   <span class="comment">##节点属性维度</span></span><br></pre></td></tr></table></figure>
<h5 id="5-1-数据集的使用"><a href="#5-1-数据集的使用" class="headerlink" title="5.1 数据集的使用"></a>5.1 数据集的使用</h5><p>定义一个名为<code>Net</code>的图神经网络模型，将节点分类图数据集加入训练。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Net().to(device)</span><br><span class="line">data = dataset[<span class="number">0</span>].to(device)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.01</span>, weight_decay=<span class="number">5e-4</span>)</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    out = model(data)</span><br><span class="line">    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>

<p>1.<a href="https://github.com/datawhalechina/team-learning-nlp/tree/master/GNN">datawhale-GNN开源学习资料</a></p>
]]></content>
      <categories>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>PyG</tag>
        <tag>Graph</tag>
      </tags>
  </entry>
  <entry>
    <title>图神经网络——基于图神经网络的节点表征学习</title>
    <url>/2021/06/23/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%8A%82%E7%82%B9%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>Graph的特征表示非常复杂：</p>
<blockquote>
<p>1.复杂的拓扑结构，较难从图像中的感受野提取有效信息；<br>2.无特定的节点顺序；<br>3.通常graph会是动态变化的， 且使用多模态特征。</p>
</blockquote>
<p>高质量的节点表征能够用于衡量节点的相似性，同时高质量的节点表征也是准确分类节点的前提。</p>
<p>本文以Cora论文引用网络数据集为例，对MLP、GCN、GAT三种神经网络的分类性能进行对比。首先载入数据集并定义可视化函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#载入数据集</span></span><br><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> Planetoid</span><br><span class="line"><span class="keyword">from</span> torch_geometric.transforms <span class="keyword">import</span> NormalizeFeatures</span><br><span class="line"></span><br><span class="line">dataset = Planetoid(root=<span class="string">&#x27;dataset&#x27;</span>, name=<span class="string">&#x27;Cora&#x27;</span>, transform=NormalizeFeatures())</span><br></pre></td></tr></table></figure>
<span id="more"></span>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#定义可视化函数，并观察整体数据分布</span></span><br><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize</span>(<span class="params">h, color</span>):</span></span><br><span class="line">    z = TSNE(n_components=<span class="number">2</span>).fit_transform(out.detach().cpu().numpy())</span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br><span class="line"></span><br><span class="line">    plt.scatter(z[:, <span class="number">0</span>], z[:, <span class="number">1</span>], s=<span class="number">70</span>, c=color, cmap=<span class="string">&quot;Set2&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">out = data.x</span><br><span class="line">visualize(out,data.y)</span><br></pre></td></tr></table></figure>
<p><img src="https://z3.ax1x.com/2021/06/28/RtavIs.png" alt="整张图"></p>
<h4 id="1-MLP-Multi-layer-Perceptron-在图节点分类中的应用"><a href="#1-MLP-Multi-layer-Perceptron-在图节点分类中的应用" class="headerlink" title="1. MLP(Multi-layer Perceptron)在图节点分类中的应用"></a>1. MLP(Multi-layer Perceptron)在图节点分类中的应用</h4><p>多层感知机（MLP，Multilayer Perceptron）也叫人工神经网络（ANN，Artificial Neural Network）.</p>
<h5 id="1-1-MLP代码"><a href="#1-1-MLP代码" class="headerlink" title="1.1 MLP代码"></a>1.1 MLP代码</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#构造MLP</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Linear</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MLP, self).__init__()</span><br><span class="line">        torch.manual_seed(<span class="number">2021</span>)</span><br><span class="line">        self.lin1 = Linear(dataset.num_features, hidden_channels)</span><br><span class="line">        self.lin2 = Linear(hidden_channels, dataset.num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.lin1(x)</span><br><span class="line">        x = x.relu()</span><br><span class="line">        x = F.dropout(x, p=<span class="number">0.5</span>, training=self.training)</span><br><span class="line">        x = self.lin2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">model = MLP(hidden_channels=<span class="number">16</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果：<br>MLP(<br>       (lin1): Linear(in_features=1433, out_features=16, bias=True)<br>       (lin2): Linear(in_features=16, out_features=7, bias=True)<br>)</p>
</blockquote>
<p>该MLP由两个线性层、一个ReLU非线性层和一个dropout组成。第一个线程层将1433维的节点表征嵌入(embedding)到低维空间中(hidden_channels=16)，第二个线性层将节点表征嵌入到类别空间中(num_classes=7)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#训练MLP</span></span><br><span class="line">model = MLP(hidden_channels=<span class="number">16</span>)</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()  <span class="comment"># Define loss criterion.</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.01</span>, weight_decay=<span class="number">5e-4</span>)  <span class="comment"># Define optimizer.</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>():</span></span><br><span class="line">    model.train()</span><br><span class="line">    optimizer.zero_grad()  <span class="comment"># Clear gradients.</span></span><br><span class="line">    out = model(data.x)  <span class="comment"># Perform a single forward pass.</span></span><br><span class="line">    loss = criterion(out[data.train_mask], data.y[data.train_mask])  <span class="comment"># Compute the loss solely based on the training nodes.</span></span><br><span class="line">    loss.backward()  <span class="comment"># Derive gradients.</span></span><br><span class="line">    optimizer.step()  <span class="comment"># Update parameters based on gradients.</span></span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">201</span>):</span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        loss = train()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch: <span class="subst">&#123;epoch:03d&#125;</span>, Loss: <span class="subst">&#123;loss:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果:<br>Epoch: 050, Loss: 1.1777<br>Epoch: 100, Loss: 0.5491<br>Epoch: 150, Loss: 0.4577<br>Epoch: 200, Loss: 0.2876</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#测试训练后的MLP</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    out = model(data.x)</span><br><span class="line">    pred = out.argmax(dim=<span class="number">1</span>)  <span class="comment"># Use the class with highest probability.</span></span><br><span class="line">    test_correct = pred[data.test_mask] == data.y[data.test_mask]  <span class="comment"># Check against ground-truth labels.</span></span><br><span class="line">    test_acc = <span class="built_in">int</span>(test_correct.<span class="built_in">sum</span>()) / <span class="built_in">int</span>(data.test_mask.<span class="built_in">sum</span>())  <span class="comment"># Derive ratio of correct predictions.</span></span><br><span class="line">    <span class="keyword">return</span> test_acc</span><br><span class="line"></span><br><span class="line">test_acc = test()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Test Accuracy: <span class="subst">&#123;test_acc:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果：<br>Test Accuracy: 0.5850</p>
</blockquote>
<p>MLP的结果较差，是因为用于训练此神经网络的有标签节点数量过少，它对未见过的节点泛化能力很差。</p>
<h4 id="2-GCN-Graph-Convolutional-Network-在图节点分类中的应用"><a href="#2-GCN-Graph-Convolutional-Network-在图节点分类中的应用" class="headerlink" title="2 GCN(Graph Convolutional Network)在图节点分类中的应用"></a>2 GCN(Graph Convolutional Network)在图节点分类中的应用</h4><p>GCN，图卷积神经网络，本质上和CNN的作用一样，就是一个特征提取器，只不过它的对象是图数据。关键在于如何定义局部感受域:</p>
<ul>
<li>Spatial approach: 指定节点的边的方向;</li>
<li>Spectral approach: 通过图的拉普拉斯矩阵的特征值和特征向量对图结构进行处理.</li>
</ul>
<h5 id="2-1-GCN公式"><a href="#2-1-GCN公式" class="headerlink" title="2.1 GCN公式"></a>2.1 GCN公式</h5><p>$$<br>\mathbf{X}^{\prime} = \mathbf{\hat{D}}^{-1/2} \mathbf{\hat{A}}\mathbf{\hat{D}}^{-1/2} \mathbf{X} \mathbf{\Theta}<br>$$<br>其中$\mathbf{\hat{A}} = \mathbf{A} + \mathbf{I}$表示插入自环的邻接矩阵,$\mathbf{I}$是单位矩阵，$\hat{D}<em>{ii} = \sum</em>{j=0} \hat{A}<em>{ij}$表示$\mathbf{\hat{A}}$的对角线度矩阵。$\mathbf{\hat{D}}^{-1/2} \mathbf{\hat{A}}<br>\mathbf{\hat{D}}^{-1/2}$是对称归一化矩阵，它的节点式公式为：<br>$$<br>\mathbf{x}^{\prime}<em>i = \mathbf{\Theta} \sum</em>{j \in \mathcal{N}(v) \cup{ i }} \frac{e</em>{j,i}}{\sqrt{\hat{d}_j \hat{d}<em>i}} \mathbf{x}<em>j<br>$$<br>其中，$\hat{d}<em>i = 1 + \sum</em>{j \in \mathcal{N}(i)} e</em>{j,i}$，$e</em>{j,i}$表示从源节点$j$到目标节点$i$的边的对称归一化系数（默认值为1.0）。</p>
<h5 id="2-2-GCN代码"><a href="#2-2-GCN代码" class="headerlink" title="2.2 GCN代码"></a>2.2 GCN代码</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#构造GCN</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> GCNConv</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GCN</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(GCN, self).__init__()</span><br><span class="line">        torch.manual_seed(<span class="number">2021</span>)</span><br><span class="line">        self.conv1 = GCNConv(dataset.num_features, hidden_channels)</span><br><span class="line">        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, edge_index</span>):</span></span><br><span class="line">        <span class="comment"># x:输入节点特征,可以是节点特征矩阵或一维节点索引张量</span></span><br><span class="line">        <span class="comment"># edge_type:每条边的一维关系类型/索引</span></span><br><span class="line">        x = self.conv1(x, edge_index)</span><br><span class="line">        x = x.relu()</span><br><span class="line">        x = F.dropout(x, p=<span class="number">0.5</span>, training=self.training)</span><br><span class="line">        x = self.conv2(x, edge_index)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">model = GCN(hidden_channels=<span class="number">16</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果：<br>GCN(<br>      (conv1): GCNConv(1433, 16)<br>      (conv2): GCNConv(16, 7)<br>)</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#可视化未训练的GCN</span></span><br><span class="line">model = GCN(hidden_channels=<span class="number">16</span>)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">out = model(data.x, data.edge_index)</span><br><span class="line">visualize(out, color=data.y)</span><br></pre></td></tr></table></figure>
<p><img src="https://z3.ax1x.com/2021/06/28/RtaLqg.png" alt="未训练GCN"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#训练GCN</span></span><br><span class="line">model = GCN(hidden_channels=<span class="number">16</span>)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.01</span>, weight_decay=<span class="number">5e-4</span>)</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>():</span></span><br><span class="line">      model.train()</span><br><span class="line">      optimizer.zero_grad()  <span class="comment"># Clear gradients.</span></span><br><span class="line">      out = model(data.x, data.edge_index)  <span class="comment"># Perform a single forward pass.</span></span><br><span class="line">      loss = criterion(out[data.train_mask], data.y[data.train_mask])  <span class="comment"># Compute the loss solely based on the training nodes.</span></span><br><span class="line">      loss.backward()  <span class="comment"># Derive gradients.</span></span><br><span class="line">      optimizer.step()  <span class="comment"># Update parameters based on gradients.</span></span><br><span class="line">      <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">201</span>):</span><br><span class="line">    loss = train()</span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch: <span class="subst">&#123;epoch:03d&#125;</span>, Loss: <span class="subst">&#123;loss:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果：<br>Epoch: 050, Loss: 1.1346<br>Epoch: 100, Loss: 0.5471<br>Epoch: 150, Loss: 0.4021<br>Epoch: 200, Loss: 0.3391</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#测试</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">      model.<span class="built_in">eval</span>()</span><br><span class="line">      out = model(data.x, data.edge_index)</span><br><span class="line">      pred = out.argmax(dim=<span class="number">1</span>)  <span class="comment"># Use the class with highest probability.</span></span><br><span class="line">      test_correct = pred[data.test_mask] == data.y[data.test_mask]  <span class="comment"># Check against ground-truth labels.</span></span><br><span class="line">      test_acc = <span class="built_in">int</span>(test_correct.<span class="built_in">sum</span>()) / <span class="built_in">int</span>(data.test_mask.<span class="built_in">sum</span>())  <span class="comment"># Derive ratio of correct predictions.</span></span><br><span class="line">      <span class="keyword">return</span> test_acc</span><br><span class="line"></span><br><span class="line">test_acc = test()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Test Accuracy: <span class="subst">&#123;test_acc:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果:Test Accuracy: 0.8090</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#可视化训练后的GCN</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">out = model(data.x, data.edge_index)</span><br><span class="line">visualize(out, color=data.y)</span><br></pre></td></tr></table></figure>
<p><img src="https://z3.ax1x.com/2021/06/28/Rtajaj.png" alt="训练后GCN"></p>
<h4 id="3-GAT-Graph-Attention-Network-在图节点分类中的应用"><a href="#3-GAT-Graph-Attention-Network-在图节点分类中的应用" class="headerlink" title="3.GAT(Graph Attention Network)在图节点分类中的应用"></a>3.GAT(Graph Attention Network)在图节点分类中的应用</h4><p>GAT的提出解决了GCN存在的问题:</p>
<ul>
<li>GCN 假设图是无向的,因为利用了对称的拉普拉斯矩阵 (只有邻接矩阵 A 是对称的，拉普拉斯矩阵才可以正交分解)，不能直接用于有向图。</li>
<li>GCN 不能处理动态图,GCN 在训练时依赖于具体的图结构，测试的时候也要在相同的图上进行。因此只能处理 transductive 任务，不能处理 inductive 任务。</li>
<li>GCN 不能为每个邻居分配不同的权重,GCN 在卷积时对所有邻居节点均一视同仁，不能根据节点重要性分配不同的权重。<h5 id="3-1-GAT公式"><a href="#3-1-GAT公式" class="headerlink" title="3.1 GAT公式"></a>3.1 GAT公式</h5>图注意力算子:<br>$$<br>\mathbf{x}^{\prime}<em>i = \alpha</em>{i,i}\mathbf{\Theta}\mathbf{x}<em>{i} +<br>\sum</em>{j \in \mathcal{N}(i)} \alpha_{i,j}\mathbf{\Theta}\mathbf{x}<em>{j}<br>$$<br>注意力系数$\alpha</em>{i,j}$为:<br>$$<br>\alpha_{i,j} =<br>\frac{<br>\exp\left(\mathrm{LeakyReLU}\left(\mathbf{a}^{\top}<br>[\mathbf{\Theta}\mathbf{x}_i , \Vert , \mathbf{\Theta}\mathbf{x}<em>j]<br>\right)\right)}<br>{\sum</em>{k \in \mathcal{N}(i) \cup { i }}<br>\exp\left(\mathrm{LeakyReLU}\left(\mathbf{a}^{\top}<br>[\mathbf{\Theta}\mathbf{x}_i , \Vert , \mathbf{\Theta}\mathbf{x}_k]<br>\right)\right)}<br>$$<h5 id="3-2-GAT代码"><a href="#3-2-GAT代码" class="headerlink" title="3.2 GAT代码"></a>3.2 GAT代码</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#构造GAT</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Linear</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> GATConv</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GAT</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(GAT, self).__init__()</span><br><span class="line">        torch.manual_seed(<span class="number">2021</span>)</span><br><span class="line">        self.conv1 = GATConv(dataset.num_features, hidden_channels)</span><br><span class="line">        self.conv2 = GATConv(hidden_channels, dataset.num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, edge_index</span>):</span></span><br><span class="line">        x = self.conv1(x, edge_index)</span><br><span class="line">        x = x.relu()</span><br><span class="line">        x = F.dropout(x, p=<span class="number">0.5</span>, training=self.training)</span><br><span class="line">        x = self.conv2(x, edge_index)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">model = GAT(hidden_channels=<span class="number">16</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果:<br>GAT(</p>
<pre><code>(conv1): GATConv(1433, 16, heads=1)
(conv2): GATConv(16, 7, heads=1)
</code></pre>
<p>)</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#可视化未训练的GAT</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">out = model(data.x, data.edge_index)</span><br><span class="line">visualize(out, color=data.y)</span><br></pre></td></tr></table></figure>
<img src="https://z3.ax1x.com/2021/06/28/RtaqsS.png" alt="未训练GAT"></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#训练GAT</span></span><br><span class="line">model = GAT(hidden_channels=<span class="number">16</span>)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.01</span>, weight_decay=<span class="number">5e-4</span>)</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>():</span></span><br><span class="line">      model.train()</span><br><span class="line">      optimizer.zero_grad()  <span class="comment"># Clear gradients.</span></span><br><span class="line">      out = model(data.x, data.edge_index)  <span class="comment"># Perform a single forward pass.</span></span><br><span class="line">      loss = criterion(out[data.train_mask], data.y[data.train_mask])  <span class="comment"># 只根据训练节点计算损失</span></span><br><span class="line">      loss.backward()  <span class="comment"># Derive gradients.</span></span><br><span class="line">      optimizer.step()  <span class="comment"># 根据梯度更新参数</span></span><br><span class="line">      <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">201</span>):</span><br><span class="line">    loss = train()</span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch: <span class="subst">&#123;epoch:03d&#125;</span>, Loss: <span class="subst">&#123;loss:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果:<br>Epoch: 050, Loss: 0.8583<br>Epoch: 100, Loss: 0.3209<br>Epoch: 150, Loss: 0.2267<br>Epoch: 200, Loss: 0.1939</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#测试GAT</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">      model.<span class="built_in">eval</span>()</span><br><span class="line">      out = model(data.x, data.edge_index)</span><br><span class="line">      pred = out.argmax(dim=<span class="number">1</span>)  <span class="comment"># 选取概率最高的一类</span></span><br><span class="line">      test_correct = pred[data.test_mask] == data.y[data.test_mask]		<span class="comment"># 预测与真实对比</span></span><br><span class="line">      test_acc = <span class="built_in">int</span>(test_correct.<span class="built_in">sum</span>()) / <span class="built_in">int</span>(data.test_mask.<span class="built_in">sum</span>())	<span class="comment"># 准确率</span></span><br><span class="line">      <span class="keyword">return</span> test_acc</span><br><span class="line"></span><br><span class="line">test_acc = test()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Test Accuracy: <span class="subst">&#123;test_acc:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果:Test Accuracy: 0.7310</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#可视化训练后的GAT</span></span><br><span class="line">model = GAT(hidden_channels=<span class="number">16</span>)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">out = model(data.x, data.edge_index)</span><br><span class="line">visualize(out, color=data.y)</span><br></pre></td></tr></table></figure>
<p><img src="https://z3.ax1x.com/2021/06/28/RtaXZQ.png" alt="训练后GAT"></p>
<h4 id="4-总结"><a href="#4-总结" class="headerlink" title="4.总结"></a>4.总结</h4><p>GCN和GAT的结果都优于MLP,原因是他们同时考虑了节点自身信息与周围邻接节点的信息.</p>
<p>GCN和GAT的共同点:</p>
<ul>
<li>都遵循消息传递范式；</li>
<li>在邻接节点信息变换阶段，它们都对邻接节点做归一化和线性变换；</li>
<li>在邻接节点信息聚合阶段，它们都将变换后的邻接节点信息做求和聚合；</li>
<li>在中心节点信息变换阶段，它们都只是简单返回邻接节点信息聚合阶段的聚合结果。</li>
</ul>
<p>GCN和GAT的不同点在于归一化方法不同():</p>
<ul>
<li>GCN根据中心节点与邻接节点的度计算归一化系数;GAT根据中心节点与邻接节点的相似度计算归一化系数。</li>
<li>GCN的归一化方式依赖于图的拓扑结构：不同的节点会有不同的度，同时不同节点的邻接节点的度也不同，于是在一些应用中GCN图神经网络会表现出较差的泛化能力;GAT的归一化方式依赖于中心节点与邻接节点的相似度，相似度是训练得到的，因此不受图的拓扑结构的影响，在不同的任务中都会有较好的泛化表现。</li>
</ul>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://github.com/datawhalechina/team-learning-nlp/tree/master/GNN">datawhale-GNN开源学习资料</a><br>2.<a href="https://zhuanlan.zhihu.com/p/306261981">知乎-图节点表征学习</a><br>3.<a href="https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.RGCNConv">GCNConv官方文档</a><br>4.<a href="https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GATConv">GATConv官方文档</a><br>5.<a href="https://baijiahao.baidu.com/s?id=1671028964544884749&wfr=spider&for=pc">GAT图注意力网络</a></p>
]]></content>
      <categories>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>PyG</tag>
        <tag>Graph</tag>
        <tag>MLP</tag>
        <tag>GCN</tag>
        <tag>GAT</tag>
      </tags>
  </entry>
  <entry>
    <title>图神经网络——消息传递网络</title>
    <url>/2021/06/19/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E6%B6%88%E6%81%AF%E4%BC%A0%E9%80%92%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<p>消息传递(Message Passing) 指的是目标节点$S1$的邻居$\mathcal{N(S1)}$——B1、B2、B3，这些邻居节点根据一定的规则将信息(特征)，汇总到目标节点上。信息汇总中最简单的规则就是逐个元素相加。<br>在pytorch-geometric的官方文档中，消息传递图神经网络被描述为:<br>$$<br>\mathbf{x}_i^{(k)} = \gamma^{(k)} \left( \mathbf{x}<em>i^{(k-1)}, \square</em>{j \in \mathcal{N}(i)} , \phi^{(k)}\left(\mathbf{x}<em>i^{(k-1)}, \mathbf{x}<em>j^{(k-1)},\mathbf{e}</em>{j,i}\right) \right),<br>$$<br>其中，$\mathbf{e}</em>{j,i} \in \mathbb{R}^D$ 表示从节点$j$到节点$i$的边的属性，$\mathbf{x}^{(k-1)}_i\in\mathbb{R}^F$表示$(k-1)$层中节点$i$的节点表征，$\square$表示聚合策略，$\gamma$和$\phi$表示一些神经网络方法，比如MLPs多层感知器、LSTM等。<br>从公式中可以看出，目标节点$x_i$在k层的特征可以通过$x_i$在上一层(k-1层)的特征与其相邻节点$x_j$在上一层(k-1层)的特征以及相邻节点到目标节点的边的特征，这三个特征在k层通过$\square$的聚合策略(aggregate)，通过一个$\gamma$在k层的分析方法来导出目标节点$x_i$的特征。</p>
<span id="more"></span>

<h4 id="2-MessagePassing基类"><a href="#2-MessagePassing基类" class="headerlink" title="2. MessagePassing基类"></a>2. MessagePassing基类</h4><p>Pytorch Geometric(PyG)提供了<a href="https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.message_passing.MessagePassing">MessagePassing</a>基类，通过继承基类，并定义message()方法、update()方法、aggregate()方法，可以构造消息传递图神经网络。</p>
<h5 id="2-1-MessagePassing-init-aggr-”add”-flow-”source-to-target”-node-dim-2"><a href="#2-1-MessagePassing-init-aggr-”add”-flow-”source-to-target”-node-dim-2" class="headerlink" title="2.1 MessagePassing.__init__(aggr=”add”, flow=”source_to_target”, node_dim=-2)"></a>2.1 MessagePassing.__init__(aggr=”add”, flow=”source_to_target”, node_dim=-2)</h5><blockquote>
<p>aggr：定义聚合方案(“add”、”mean”或 “max”)，默认值”add”；<br>flow：定义消息传递方向(“source_to_target”或 “target_to_source”)，默认值”source_to_target”；<br>node_dim：定义沿哪个维度传播，指的是节点表征张量(Tensor)的哪一个维度是节点维度，默认值-2(第0维)。</p>
</blockquote>
<h5 id="2-2-MessagePassing-propagate-edge-index-size-None-kwargs"><a href="#2-2-MessagePassing-propagate-edge-index-size-None-kwargs" class="headerlink" title="2.2 MessagePassing.propagate(edge_index, size=None, **kwargs)"></a>2.2 MessagePassing.propagate(edge_index, size=None, **kwargs)</h5><blockquote>
<p>这是一个集成方法，调用其会依次调用message、aggregate、update方法。<br>edge_index：边的端点的索引，当flow=”source_to_target”时，节点edge_index[0]的信息将被传递到节点edge_index[1]；当flow=”target_to_source”时，节点edge_index[1]的信息将被传递到节点edge_index[0]。<br>size：邻接节点的数量与中心节点的数量，默认值None(对称矩阵)；<br>**kwaegs：图的其他属性或额外的数据。</p>
</blockquote>
<h5 id="2-3-MessagePassing-message-…"><a href="#2-3-MessagePassing-message-…" class="headerlink" title="2.3 MessagePassing.message(…)"></a>2.3 MessagePassing.message(…)</h5><blockquote>
<p>以函数的方式构造消息；<br>flow=”source_to_target”，此方式下，message方法负责产生source node需要传出的信息。</p>
</blockquote>
<h5 id="2-4-MessagePassing-update-aggr-out-…"><a href="#2-4-MessagePassing-update-aggr-out-…" class="headerlink" title="2.4 MessagePassing.update(aggr_out, …)"></a>2.4 MessagePassing.update(aggr_out, …)</h5><blockquote>
<p>为每个节点$i \in \mathcal{V}$更新节点表征。</p>
</blockquote>
<h5 id="2-5-MessagePassing-aggregate-…"><a href="#2-5-MessagePassing-aggregate-…" class="headerlink" title="2.5 MessagePassing.aggregate(…)"></a>2.5 MessagePassing.aggregate(…)</h5><p>将从源节点传递过来的消息聚合在目标节点上，一般可选的聚合方式有sum, mean和max。</p>
<h4 id="3-MessagePassing子类"><a href="#3-MessagePassing子类" class="headerlink" title="3. MessagePassing子类"></a>3. MessagePassing子类</h4><h5 id="3-1-GCNConv类"><a href="#3-1-GCNConv类" class="headerlink" title="3.1 GCNConv类"></a>3.1 GCNConv类</h5><p>以继承MessagePassing基类的GCNConv类为例，可以实现一个简单的GNN。<br>GCNConv的公式如下:<br>$$<br>\mathbf{x}<em>i^{(k)} = \sum</em>{j \in \mathcal{N}(i) \cup { i }} \frac{1}{\sqrt{\deg(i)} \cdot \sqrt{\deg(j)}} \cdot \left( \mathbf{\Theta} \cdot \mathbf{x}_j^{(k-1)} \right)<br>$$<br>其中，邻接节点的表征$\mathbf{x}_j^{(k-1)}$首先通过与权重矩阵$\mathbf{\Theta}$相乘进行变换，然后按端点的度(degree)$\deg(i), \deg(j)$进行归一化处理，最后进行求和。这个公式可以分为以下几个步骤：</p>
<blockquote>
<p>向邻接矩阵添加自环边。<br>对节点表征做线性转换。<br>计算归一化系数。<br>归一化邻接节点的节点表征。<br>将相邻节点表征相加（”求和 “聚合）。</p>
</blockquote>
<p>GCNConv继承了MessagePassing，并以”求和”作为领域节点信息聚合方式。该层的所有逻辑都在forward()方法中：<br>1.通过torch_geometric.utils.add_self_loops()函数向边索引添加自循环边，目的是改进原始不考虑中心节点自身的信息量的问题；<br>2.通过torch.nn.Linear实例对节点表征进行线性变换；<br>3.归一化系数是由每个节点的节点度得出的，它被转换为每条边的节点度。结果被保存在形状为[num_edges,]的变量norm中。</p>
<h5 id="3-2-实现"><a href="#3-2-实现" class="headerlink" title="3.2 实现"></a>3.2 实现</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> Planetoid</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> MessagePassing</span><br><span class="line"><span class="keyword">from</span> torch_geometric.utils <span class="keyword">import</span> add_self_loops, degree</span><br><span class="line"><span class="keyword">from</span> torch_sparse <span class="keyword">import</span> SparseTensor</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GCNConv</span>(<span class="params">MessagePassing</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span></span><br><span class="line">        <span class="comment">#in_channels可理解为输入通道数，out_channels可理解为卷积核数量</span></span><br><span class="line">        <span class="built_in">super</span>(GCNConv, self).__init__(aggr=<span class="string">&#x27;add&#x27;</span>, flow=<span class="string">&#x27;source_to_target&#x27;</span>) <span class="comment">#继承，策略为合并</span></span><br><span class="line">        <span class="comment"># &quot;Add&quot; aggregation (Step 5).</span></span><br><span class="line">        <span class="comment"># flow=&#x27;source_to_target&#x27; 表示消息从源节点传播到目标节点</span></span><br><span class="line">        self.lin = torch.nn.Linear(in_channels, out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, edge_index</span>):</span></span><br><span class="line">        <span class="comment"># x has shape [N, in_channels]</span></span><br><span class="line">        <span class="comment"># edge_index has shape [2, E]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 1: 添加自循环到节点特征矩阵(Add self-loops to the adjacency matrix.)</span></span><br><span class="line">        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2: 节点特征矩阵的线性变换(Linearly transform node feature matrix.)</span></span><br><span class="line">        x = self.lin(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 3: Compute normalization.</span></span><br><span class="line">        row, col = edge_index <span class="comment">#用行、列描述特征矩阵</span></span><br><span class="line">        deg = degree(col, x.size(<span class="number">0</span>), dtype=x.dtype)</span><br><span class="line">        deg_inv_sqrt = deg.<span class="built_in">pow</span>(-<span class="number">0.5</span>)</span><br><span class="line">        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 4-5: Start propagating messages.</span></span><br><span class="line">        adjmat = SparseTensor(row=edge_index[<span class="number">0</span>], col=edge_index[<span class="number">1</span>], value=torch.ones(edge_index.shape[<span class="number">1</span>]))</span><br><span class="line">        <span class="comment"># 此处传的不再是edge_index，而是SparseTensor类型的Adjancency Matrix</span></span><br><span class="line">        <span class="keyword">return</span> self.propagate的(adjmat, x=x, norm=norm, deg=deg.view((-<span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line">		<span class="comment"># 此处省略MessagePassing.propagate的代码.</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">message</span>(<span class="params">self, x_j, norm</span>):</span></span><br><span class="line">        <span class="comment"># x_j has shape [E, out_channels]</span></span><br><span class="line">        <span class="comment"># deg_i has shape [E, 1]</span></span><br><span class="line">        <span class="comment"># Step 4: Normalize node features.</span></span><br><span class="line">        <span class="keyword">return</span> norm.view(-<span class="number">1</span>, <span class="number">1</span>) * x_j</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">aggregate</span>(<span class="params">self, inputs, index, ptr, dim_size</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;self.aggr:&#x27;</span>, self.aggr)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;`aggregate` is called&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>().aggregate(inputs, index, ptr=ptr, dim_size=dim_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">message_and_aggregate</span>(<span class="params">self, adj_t, x, norm</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;`message_and_aggregate` is called&#x27;</span>)</span><br><span class="line">        <span class="comment"># 没有实现真实的消息传递与消息聚合的操作</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">self, inputs, deg</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(deg)</span><br><span class="line">        <span class="keyword">return</span> inputs</span><br><span class="line"></span><br><span class="line">dataset = Planetoid(root=<span class="string">&#x27;dataset&#x27;</span>, name=<span class="string">&#x27;Cora&#x27;</span>)</span><br><span class="line">data = dataset[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">net = GCNConv(data.num_features, <span class="number">64</span>)</span><br><span class="line">h_nodes = net(data.x, data.edge_index)</span><br><span class="line"><span class="comment"># print(h_nodes.shape)</span></span><br></pre></td></tr></table></figure>

<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://github.com/datawhalechina/team-learning-nlp/tree/master/GNN">datawhale-GNN开源学习资料</a></p>
]]></content>
      <categories>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>PyG</tag>
        <tag>GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>图神经网络——节点分类与边预测</title>
    <url>/2021/06/27/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E8%8A%82%E7%82%B9%E5%88%86%E7%B1%BB%E4%B8%8E%E8%BE%B9%E9%A2%84%E6%B5%8B/</url>
    <content><![CDATA[<h4 id="1-InMemoryDataset基类"><a href="#1-InMemoryDataset基类" class="headerlink" title="1.InMemoryDataset基类"></a>1.InMemoryDataset基类</h4><p>在PyG中，可以通过继承InMemoryDataset类来自定义一个数据可全部存储到内存的数据集类。(继承Dataset是分次加载到内存，继承InMemoryDataset是一次性加载所有数据到内存)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InMemoryDataset</span>(<span class="params">root: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span>, transform: <span class="type">Optional</span>[<span class="type">Callable</span>] = <span class="literal">None</span>, pre_transform: <span class="type">Optional</span>[<span class="type">Callable</span>] = <span class="literal">None</span>, pre_filter: <span class="type">Optional</span>[<span class="type">Callable</span>] = <span class="literal">None</span></span>)</span></span><br></pre></td></tr></table></figure>
<p>参数说明：</p>
<blockquote>
<p>transform：数据转换函数，用于转换Data对象，每一次数据获取过程中都会被执行。<br>pre_transform：数据转换函数，用于转换Data对象，在Data对象被保存到文件前调用。<br>pre_filter：检查数据是否要保留的函数，接收一个Data对象，返回此Data对象是否应该被包含在最终的数据集中，在Data对象被保存到文件前调用。</p>
</blockquote>
<span id="more"></span>
<h4 id="2-Sequential容器"><a href="#2-Sequential容器" class="headerlink" title="2.Sequential容器"></a>2.Sequential容器</h4><p>nn.Sequential是nn.module的容器，用于按顺序包装一组网络层。参数说明：</p>
<blockquote>
<p>args(str)：模型的全局输入参数；<br>modules ([(str, Callable) or Callable]) ：模块列表。</p>
</blockquote>
<h4 id="3-节点分类"><a href="#3-节点分类" class="headerlink" title="3. 节点分类"></a>3. 节点分类</h4><p>定义一个GAT图神经网络，通过hidden_channels_list参数来设置每一层GATConv的outchannel，所以hidden_channels_list长度即为GATConv的层数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#载入数据集</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> Planetoid</span><br><span class="line"><span class="keyword">from</span> torch_geometric.transforms <span class="keyword">import</span> NormalizeFeatures</span><br><span class="line">dataset = Planetoid(root=<span class="string">&#x27;dataset&#x27;</span>, ame=<span class="string">&#x27;Cora&#x27;</span>,transform=NormalizeFeatures())</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Linear,ReLU</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> GATConv, Sequential</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用Sequential容器定义一个GAT网络</span></span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> GATConv</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GAT</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_features, hidden_channels_list, num_classes</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(GAT, self).__init__()</span><br><span class="line">        torch.manual_seed(<span class="number">2021</span>)</span><br><span class="line">        hns = [num_features] + hidden_channels_list</span><br><span class="line">        conv_list = []</span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(hidden_channels_list)):</span><br><span class="line">            conv_list.append((GATConv(hns[idx], hns[idx+<span class="number">1</span>]), <span class="string">&#x27;x, edge_index -&gt; x&#x27;</span>))</span><br><span class="line">            conv_list.append(ReLU(inplace=<span class="literal">True</span>),)</span><br><span class="line"></span><br><span class="line">        self.convseq = Sequential(<span class="string">&#x27;x, edge_index&#x27;</span>, conv_list)</span><br><span class="line">        self.linear = Linear(hidden_channels_list[-<span class="number">1</span>], num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, edge_index</span>):</span></span><br><span class="line">        x = self.convseq(x, edge_index)</span><br><span class="line">        x = F.dropout(x, p=<span class="number">0.5</span>, training=self.training)</span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#训练和测试</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model = GAT(num_features=dataset.num_features, hidden_channels_list=[<span class="number">200</span>, <span class="number">100</span>], num_classes=dataset.num_classes).to(device)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.01</span>, weight_decay=<span class="number">5e-4</span>)</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment">#train()、test()省略，与上章一致</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">201</span>):</span><br><span class="line">    loss = train()</span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch: <span class="subst">&#123;epoch:03d&#125;</span>, Loss: <span class="subst">&#123;loss:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">test_acc = test()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Test Accuracy: <span class="subst">&#123;test_acc:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>构建2层GAT，Accuracy为0.7640；构建2层GCN，Accuracy为0.6490。<br>构造3层GAT(将hidden_channels_list的值改为[200,100,50])，Accuracy为0.7680；构建3层GCN，Accuracy为0.5190。</p>
</blockquote>
<h4 id="4-边预测"><a href="#4-边预测" class="headerlink" title="4 边预测"></a>4 边预测</h4><p>边预测任务的目标是预测两个节点间是否有边。做边预测任务首先需要获取正负样本数量平衡的数据集(edge_index存储的是正样本，需要采样一些不存在边的节点对作为负样本边)，PyG中可以通过train_test_split_edges(data, val_ratio=0.05, test_ratio=0.1)采样负样本边。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#构造边预测神经网络</span></span><br><span class="line"><span class="keyword">import</span> os.path <span class="keyword">as</span> osp</span><br><span class="line"><span class="keyword">from</span> torch_geometric.utils <span class="keyword">import</span> negative_sampling</span><br><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> Planetoid</span><br><span class="line"><span class="keyword">import</span> torch_geometric.transforms <span class="keyword">as</span> T</span><br><span class="line"><span class="keyword">from</span> torch_geometric.utils <span class="keyword">import</span> train_test_split_edges</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> GCNConv</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = GCNConv(in_channels, <span class="number">128</span>)</span><br><span class="line">        self.conv2 = GCNConv(<span class="number">128</span>, out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, x, edge_index</span>):</span></span><br><span class="line">        x = self.conv1(x, edge_index)</span><br><span class="line">        x = x.relu()</span><br><span class="line">        <span class="keyword">return</span> self.conv2(x, edge_index)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, z, pos_edge_index, neg_edge_index</span>):</span></span><br><span class="line">        edge_index =  torch.cat([pos_edge_index, neg_edge_index], dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> (z[edge_index[<span class="number">0</span>]] * z[edge_index[<span class="number">1</span>]]).<span class="built_in">sum</span>(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode_all</span>(<span class="params">self, z</span>):</span></span><br><span class="line">        <span class="comment">#对所有的节点对预测存在边的几率</span></span><br><span class="line">        prob_adj = z @ z.t()    <span class="comment"># @ 表示矩阵乘法</span></span><br><span class="line">        <span class="keyword">return</span> (prob_adj &gt; <span class="number">0</span>).nonzero(as_tuple=<span class="literal">False</span>).t()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义单个epoch的训练过程</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_link_labels</span>(<span class="params">pos_edge_index, neg_edge_index</span>):</span></span><br><span class="line">    num_links = pos_edge_index.size(<span class="number">1</span>) + neg_edge_index.size(<span class="number">1</span>)</span><br><span class="line">    link_labels = torch.zeros(num_links, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">    link_labels[:pos_edge_index.size(<span class="number">1</span>)] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> link_labels</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">data, model, optimizer</span>):</span></span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    neg_edge_index = negative_sampling(</span><br><span class="line">        edge_index = data.train_pos_edge_index,</span><br><span class="line">        num_nodes = data.num_nodes,</span><br><span class="line">        num_neg_samples = data.train_pos_edge_index.size(<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    train_neg_edge_set = <span class="built_in">set</span>(<span class="built_in">map</span>(<span class="built_in">tuple</span>, neg_edge_index.T.tolist()))</span><br><span class="line">    val_pos_edge_set = <span class="built_in">set</span>(<span class="built_in">map</span>(<span class="built_in">tuple</span>, data.val_pos_edge_index.T.tolist()))</span><br><span class="line">    test_pos_edge_set = <span class="built_in">set</span>(<span class="built_in">map</span>(<span class="built_in">tuple</span>, data.test_pos_edge_index.T.tolist()))</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">len</span>(train_neg_edge_set &amp; val_pos_edge_set) &gt; <span class="number">0</span>) <span class="keyword">or</span> (<span class="built_in">len</span>(train_neg_edge_set &amp; test_pos_edge_set) &gt; <span class="number">0</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;wrong!&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    z = model.encode(data.x, data.train_pos_edge_index)</span><br><span class="line">    link_logits = model.decode(z, data.train_pos_edge_index, neg_edge_index)</span><br><span class="line">    link_labels = get_link_labels(data.train_pos_edge_index, neg_edge_index).to(data.x.device)</span><br><span class="line">    loss = F.binary_cross_entropy_with_logits(link_logits, link_labels)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#定义单个epoch验证与测试过程</span></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">data, model</span>):</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    z = model.encode(data.x, data.train_pos_edge_index)</span><br><span class="line"></span><br><span class="line">    results = []</span><br><span class="line">    <span class="keyword">for</span> prefix <span class="keyword">in</span> [<span class="string">&#x27;val&#x27;</span>, <span class="string">&#x27;test&#x27;</span>]:</span><br><span class="line">        pos_edge_index = data[<span class="string">f&#x27;<span class="subst">&#123;prefix&#125;</span>_pos_edge_index&#x27;</span>]</span><br><span class="line">        neg_edge_index = data[<span class="string">f&#x27;<span class="subst">&#123;prefix&#125;</span>_neg_edge_index&#x27;</span>]</span><br><span class="line">        link_logits = model.decode(z, pos_edge_index, neg_edge_index)</span><br><span class="line">        link_probs = link_logits.sigmoid()</span><br><span class="line">        link_labels = get_link_labels(pos_edge_index, neg_edge_index)</span><br><span class="line">        results.append(roc_auc_score(link_labels.cpu(), link_probs.cpu()))</span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#完整的训练、验证与测试</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">	dataset = Planetoid(<span class="string">&#x27;/Dataset/Planetoid/Cora&#x27;</span>, <span class="string">&#x27;Cora&#x27;</span>, transform=T.NormalizeFeatures())</span><br><span class="line">    data = dataset[<span class="number">0</span>]</span><br><span class="line">    ground_truth_edge_index = data.edge_index.to(device)</span><br><span class="line">    data.train_mask = data.val_mask = data.test_mask = data.y = <span class="literal">None</span></span><br><span class="line">    data = train_test_split_edges(data)</span><br><span class="line">    data = data.to(device)</span><br><span class="line"></span><br><span class="line">    model = Net(dataset.num_features, <span class="number">64</span>).to(device)</span><br><span class="line">    optimizer = torch.optim.Adam(params=model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">    best_val_auc = test_auc = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">101</span>):</span><br><span class="line">        loss = train(data, model, optimizer)</span><br><span class="line">        val_auc, tmp_test_auc = test(data, model)</span><br><span class="line">        <span class="keyword">if</span> val_auc &gt; best_val_auc:</span><br><span class="line">            best_val_auc = val_auc</span><br><span class="line">            test_auc = tmp_test_auc</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch: <span class="subst">&#123;epoch:03d&#125;</span>, Loss: <span class="subst">&#123;loss:<span class="number">.4</span>f&#125;</span>, Val: <span class="subst">&#123;val_auc:<span class="number">.4</span>f&#125;</span>, &#x27;</span></span><br><span class="line">              <span class="string">f&#x27;Test: <span class="subst">&#123;test_auc:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    z = model.encode(data.x, data.train_pos_edge_index)</span><br><span class="line">    final_edge_index = model.decode_all(z)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>结果：Epoch: 100, Loss: 0.4414, Val: 0.9330, Test: 0.8943</p>
</blockquote>
<p>将Sequential容器用于边预测，需要在Net类定义中将__init_函数和main()做部分修改：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#替换上面的对应代码</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, hidden_channels_list, out_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        torch.manual_seed(<span class="number">2021</span>)</span><br><span class="line">        hns = [in_channels] + hidden_channels_list</span><br><span class="line">        conv_list = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(hidden_channels_list)-<span class="number">1</span>):</span><br><span class="line">            conv_list.append((GCNConv(hns[idx], hns[idx+<span class="number">1</span>]), <span class="string">&#x27;x, edge_index -&gt; x&#x27;</span>))</span><br><span class="line">            conv_list.append(ReLU(inplace=<span class="literal">True</span>), )</span><br><span class="line">        conv_list.append((GCNConv(hns[-<span class="number">2</span>], hns[-<span class="number">1</span>]), <span class="string">&#x27;x, edge_index -&gt; x&#x27;</span>))</span><br><span class="line"></span><br><span class="line">        self.convseq = Sequential(<span class="string">&#x27;x, edge_index&#x27;</span>, conv_list)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">        model = Net(dataset.num_features,[<span class="number">200</span>,<span class="number">100</span>],dataset.num_classes).to(device)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>在边预测任务用Sequential容器的结果：<br>Epoch: 100, Loss: 0.4226, Val: 0.9123, Test: 0.8958</p>
</blockquote>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://github.com/datawhalechina/team-learning-nlp/tree/master/GNN">datawhale-GNN开源学习资料</a><br>2.<a href="https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.sequential.Sequential">GNN官方文档</a><br>3.<a href="https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.sequential.Sequential">Sequential官网文档</a></p>
]]></content>
      <categories>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>PyG</tag>
        <tag>GCN</tag>
        <tag>GAT</tag>
        <tag>Sequential</tag>
      </tags>
  </entry>
  <entry>
    <title>地理数据分析常用工具</title>
    <url>/2021/04/15/%E5%9C%B0%E7%90%86%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7/</url>
    <content><![CDATA[<p>在地理空间数据分析中，常用一些模块进行地理数据分析、特征提取及可视化，包括shapely、geopandas、folium、kepler.gl、geohash等工具。</p>
<h4 id="1-shapely"><a href="#1-shapely" class="headerlink" title="1. shapely"></a>1. shapely</h4><p>shapely是基于笛卡尔坐标的几何对象操作和分析Python库，底层基于GEOS和JTS拓扑运算库。</p>
<h5 id="1-1-Point对象"><a href="#1-1-Point对象" class="headerlink" title="1.1 Point对象"></a>1.1 Point对象</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> shapely.geometry <span class="keyword">import</span> Point</span><br><span class="line"></span><br><span class="line">point1 = Point(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">point2 = Point(<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">point3 = Point(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#点的可视化</span></span><br><span class="line">geo.GeometryCollection([point1,point2,point3])</span><br><span class="line"></span><br><span class="line"><span class="comment">#Point转为numpy数组</span></span><br><span class="line"><span class="built_in">print</span>(np.array(point))</span><br></pre></td></tr></table></figure>
<span id="more"></span>

<h5 id="1-2-LineString对象"><a href="#1-2-LineString对象" class="headerlink" title="1.2 LineString对象"></a>1.2 LineString对象</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建LineString对象</span></span><br><span class="line">line1 = geo.LineString([(<span class="number">0</span>,<span class="number">0</span>),(<span class="number">2</span>,-<span class="number">2.2</span>),(<span class="number">3</span>,<span class="number">3.3</span>),(<span class="number">4</span>,-<span class="number">4.4</span>),(<span class="number">5</span>,-<span class="number">5.5</span>),(<span class="number">6</span>,<span class="number">6.6</span>)])</span><br><span class="line">line1 </span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#点和线的可视化</span></span><br><span class="line">geo.GeometryCollection([line1,Point(<span class="number">1</span>,<span class="number">1</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算点线或线线的最短距离</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;点线距离:&#x27;</span>+<span class="built_in">str</span>(Point(<span class="number">1</span>,<span class="number">1</span>).distance(line1)))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#几何中心</span></span><br><span class="line">center = line1.centroid </span><br><span class="line">geo.GeometryCollection([line1,center])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#几何对象的最小外接矩形</span></span><br><span class="line">b_rect = line1.envelope </span><br><span class="line">geo.GeometryCollection([line1,b_rect]) </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> <span class="comment">#简化线(Douglas-Pucker算法)</span></span><br><span class="line">line1_simplify = line1.simplify(<span class="number">0.4</span>, preserve_topology=<span class="literal">False</span>) </span><br><span class="line">line1</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#生成线的缓冲区</span></span><br><span class="line">buffer_with_circle = line1.buffer(<span class="number">0.3</span>)  </span><br><span class="line">geo.GeometryCollection([line1,buffer_with_circle])</span><br></pre></td></tr></table></figure>
<h5 id="1-3-LinearRings对象"><a href="#1-3-LinearRings对象" class="headerlink" title="1.3 LinearRings对象"></a>1.3 LinearRings对象</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> shapely.geometry.polygon <span class="keyword">import</span> LinearRing</span><br><span class="line"></span><br><span class="line">ring = geo.polygon.LinearRing([(<span class="number">0</span>, <span class="number">0</span>), (<span class="number">1</span>, -<span class="number">1</span>), (<span class="number">1</span>, <span class="number">0</span>)])</span><br><span class="line">geo.GeometryCollection([ring])</span><br></pre></td></tr></table></figure>

<h5 id="1-4-Polygon对象"><a href="#1-4-Polygon对象" class="headerlink" title="1.4 Polygon对象"></a>1.4 Polygon对象</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> shapely.geometry <span class="keyword">import</span> Polygon</span><br><span class="line"></span><br><span class="line">poly1 = Polygon([(<span class="number">0</span>,<span class="number">0</span>),(<span class="number">2</span>,<span class="number">0</span>),(<span class="number">1</span>,-<span class="number">1</span>),(-<span class="number">3</span>,<span class="number">1</span>),(<span class="number">0</span>,<span class="number">0</span>)]) <span class="comment">#起点和终点相同</span></span><br><span class="line">poly1</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#通过numpy生成多边形</span></span><br><span class="line">coords = np.array([(<span class="number">0</span>,<span class="number">0</span>),(<span class="number">1</span>,<span class="number">0.1</span>),(<span class="number">2</span>,<span class="number">0</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">0</span>,<span class="number">0</span>)])</span><br><span class="line">poly2 = Polygon(coords)</span><br><span class="line">poly2 </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#多个polygon的集合</span></span><br><span class="line">geo.GeometryCollection([poly1,poly2])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#几何中心</span></span><br><span class="line">center = poly1.centroid <span class="comment">#几何中心</span></span><br><span class="line">geo.GeometryCollection([center,poly1]) </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#最小外接矩形</span></span><br><span class="line">rect = poly1.minimum_rotated_rectangle <span class="comment">#最小外接矩形</span></span><br><span class="line">geo.GeometryCollection([rect,poly1])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">poly1.boundary<span class="comment">#边缘</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">poly2.simplify(<span class="number">0.5</span>)<span class="comment">#简化面</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">r1 = poly2.contains(point2) <span class="comment">#面点关系</span></span><br><span class="line"><span class="built_in">print</span>(r1)</span><br><span class="line"></span><br><span class="line">r2 = poly2.intersects(line1) <span class="comment">#面线关系</span></span><br><span class="line"><span class="built_in">print</span>(r2)</span><br><span class="line"></span><br><span class="line">r3 = poly1.intersects(poly2) <span class="comment">#面面关系</span></span><br><span class="line"><span class="built_in">print</span>(r3)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">poly1.intersection(poly2) <span class="comment">#面面交集</span></span><br><span class="line">poly1.union(poly2) 		  <span class="comment">#面面并集</span></span><br><span class="line">poly2.difference(poly1)   <span class="comment">#面面补集</span></span><br></pre></td></tr></table></figure>
<h4 id="2-geopandas"><a href="#2-geopandas" class="headerlink" title="2. geopandas"></a>2. geopandas</h4><p>geopandas是pandas在地理数据处理领域的扩展包，其核心数据结构是GeoSeries和GeoDataFrame。<br>geopandasd 主要功能为:</p>
<blockquote>
<p>1.文件读写<br>2.空间查询<br>3.坐标转换<br>4.空间join<br>5.地理数据可视化</p>
</blockquote>
<h5 id="2-1-文件读写"><a href="#2-1-文件读写" class="headerlink" title="2.1 文件读写"></a>2.1 文件读写</h5><p>geopandas可读geojson和shp等空间文件，也可读含有geometry字段的csv文件。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> shapely </span><br><span class="line"><span class="keyword">import</span> geopandas <span class="keyword">as</span> gpd </span><br><span class="line"><span class="keyword">from</span> shapely <span class="keyword">import</span> wkt </span><br><span class="line"><span class="keyword">from</span> shapely <span class="keyword">import</span> geometry <span class="keyword">as</span> geo</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#读取geojson</span></span><br><span class="line">countries = gpd.read_file(<span class="string">&quot;dfcountries.geojson&quot;</span>,bbox = [-<span class="number">180</span>,-<span class="number">80</span>,<span class="number">180</span>,<span class="number">80</span>])</span><br><span class="line">countries.plot()<span class="comment">#显示图</span></span><br><span class="line">countries		<span class="comment">#显示表格</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#读取shp</span></span><br><span class="line">cities = gpd.GeoDataFrame.from_file(<span class="string">&#x27;./cities.shp&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">cities.plot()<span class="comment">#显示图</span></span><br><span class="line">cities		 <span class="comment">#显示表格</span></span><br></pre></td></tr></table></figure>
<p><img src="https://ae01.alicdn.com/kf/U1e0d77ed2da2455cb9957e8ed6f59c20a.jpg" alt="f1"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#保存成geojson</span></span><br><span class="line">countries.to_file(<span class="string">&quot;dfcountries.geojson&quot;</span>,driver = <span class="string">&quot;GeoJSON&quot;</span>)</span><br><span class="line"><span class="comment">#保存成csv</span></span><br><span class="line">cities.to_csv(<span class="string">&quot;dfcountries.csv&quot;</span>,index = <span class="literal">False</span>,sep = <span class="string">&quot;\t&quot;</span>)</span><br><span class="line"><span class="comment">#保存成shp</span></span><br><span class="line">cities.to_file(<span class="string">&quot;.a/cities.shp&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#按值的大小填充颜色</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">fig, ax = plt.subplots(figsize=(<span class="number">9</span>,<span class="number">6</span>),dpi = <span class="number">100</span>)</span><br><span class="line">cities.plot(<span class="string">&#x27;area&#x27;</span>,ax = ax,legend = <span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://ae01.alicdn.com/kf/U31c06924a773420bb62cc56d2ee9442ft.jpg" alt="f2"></p>
<h4 id="3-Folium"><a href="#3-Folium" class="headerlink" title="3.Folium"></a>3.Folium</h4><p>folium是一种交互式动态地图接口，可以用来画热力图、填充地图、路径图、散点标记等图。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> folium</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment">#指定坐标中心和缩放尺寸，生成交互式地图</span></span><br><span class="line">m = folium.Map(location=[<span class="number">30.33</span>,<span class="number">120.37</span>],zoom_start=<span class="number">10</span>)</span><br><span class="line">m</span><br></pre></td></tr></table></figure>
<p><img src="https://ae01.alicdn.com/kf/Uc6be8164498d4fda99801e6d30ac676eV.jpg" alt="f3"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> folium.plugins <span class="keyword">import</span> HeatMap</span><br><span class="line"><span class="comment">#生成随机数据，指定坐标中心和缩放尺寸，生成热力图</span></span><br><span class="line">data=(np.random.normal(size=(<span class="number">100</span>,<span class="number">3</span>))*np.array([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]])+np.array([[<span class="number">30.33</span>,<span class="number">120.37</span>,<span class="number">1</span>]])).tolist()</span><br><span class="line">m=folium.Map([<span class="number">30.33</span>,<span class="number">120.37</span>],tiles=<span class="string">&#x27;Stamen Toner&#x27;</span>,zoom_start=<span class="number">6</span>)</span><br><span class="line">HeatMap(data).add_to(m)</span><br><span class="line">m </span><br></pre></td></tr></table></figure>
<p><img src="https://ae01.alicdn.com/kf/U1fe179343a464ab1926cd10582207986X.jpg" alt="f4"></p>
<p>folium的MarkerCluster()聚类函数，可以用来反映一个区域的拥挤程度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> folium.plugins <span class="keyword">import</span> MarkerCluster</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建对象</span></span><br><span class="line">marker_cluster = MarkerCluster().add_to(m)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将经纬度加入聚类</span></span><br><span class="line"><span class="keyword">for</span> element <span class="keyword">in</span> data:</span><br><span class="line">    folium.Marker(location=[element[<span class="number">0</span>], element[<span class="number">1</span>]],icon=<span class="literal">None</span>).add_to(marker_cluster)</span><br><span class="line"></span><br><span class="line"><span class="comment">#添加至地图</span></span><br><span class="line">m.add_child(marker_cluster)</span><br><span class="line">m</span><br></pre></td></tr></table></figure>

<h4 id="4-Kepler-gl"><a href="#4-Kepler-gl" class="headerlink" title="4.Kepler.gl"></a>4.Kepler.gl</h4><p>Kepler.gl是Uber联合Mapbox推出的地理空间可视化工具，支持3种数据格式：CSV、JSON、GeoJSON。接下来将以杭州市OSM路网为例，制作路径流动动画。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keplergl <span class="keyword">import</span> KeplerGl</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#读取geojson</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./hzroad.json&#x27;</span>,encoding=<span class="string">&#x27;gb18030&#x27;</span>, errors=<span class="string">&#x27;ignore&#x27;</span>) <span class="keyword">as</span> g:</span><br><span class="line">    raw_roads = json.load(g)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#生成虚构时间戳信息和高度0</span></span><br><span class="line">start_time = time.mktime(time.strptime(<span class="string">&#x27;2020-05-29 20:00:00&#x27;</span>, <span class="string">&quot;%Y-%m-%d %H:%M:%S&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(raw_roads[<span class="string">&#x27;features&#x27;</span>].__len__()):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(raw_roads[<span class="string">&#x27;features&#x27;</span>][i][<span class="string">&#x27;geometry&#x27;</span>][<span class="string">&#x27;coordinates&#x27;</span>].__len__()):</span><br><span class="line">        <span class="comment"># 更新当前对应的时间戳</span></span><br><span class="line">        shift_time = <span class="built_in">int</span>((j / raw_roads[<span class="string">&#x27;features&#x27;</span>][i][<span class="string">&#x27;geometry&#x27;</span>][<span class="string">&#x27;coordinates&#x27;</span>].__len__())*<span class="number">3600</span>) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 高度设置为0</span></span><br><span class="line">        raw_roads[<span class="string">&#x27;features&#x27;</span>][i][<span class="string">&#x27;geometry&#x27;</span>][<span class="string">&#x27;coordinates&#x27;</span>][j] \</span><br><span class="line">            .extend([<span class="number">0</span>, </span><br><span class="line">                     <span class="built_in">int</span>(start_time) + shift_time])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keplergl <span class="keyword">import</span> KeplerGl</span><br><span class="line"><span class="comment"># 生成KeplerGl对象</span></span><br><span class="line">m = KeplerGl(height=<span class="number">400</span>, </span><br><span class="line">                data=&#123;<span class="string">&#x27;flow&#x27;</span>: raw_roads&#125;) <span class="comment"># data以图层名为键，对应的矢量数据为值</span></span><br><span class="line">m</span><br><span class="line">m.save_to_html(file_name=<span class="string">&#x27;./hangzhou.html&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="5-Geohash"><a href="#5-Geohash" class="headerlink" title="5.Geohash"></a>5.Geohash</h4><p>GeoHash是一种地址编码方法，可以将地理经纬度坐标编码为由字母和数字所构成的字符串。其原理类似哈希表：由于遍历列表查找时间复杂度高，而创建散列函数能够更高效地定位数据。而GeoHash将二维的经纬度坐标编码到一维的字符串中，在做地理位置索引时只需匹配字符串，便于信息的缓存和压缩。</p>
<p>GeoHash采用二分法不断缩小经度和纬度的区间来进行二进制编码，最后将经纬度分别产生的编码奇偶位交叉合并，再用字母数字表示。</p>
<p><img src="https://ae01.alicdn.com/kf/Uf288553e6d2c43038976764613dd1cc1x.jpg" alt="f6"></p>
<blockquote>
<p>p.s. 酷炫动图见公众号</p>
</blockquote>
]]></content>
      <categories>
        <category>Geospatial Data Analysis</category>
      </categories>
      <tags>
        <tag>geopandas</tag>
        <tag>shapely</tag>
        <tag>Kepler.gl</tag>
        <tag>folium</tag>
        <tag>geohash</tag>
      </tags>
  </entry>
  <entry>
    <title>异常检测——基于统计学的方法</title>
    <url>/2021/05/14/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E5%AD%A6%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>异常检测的统计方法包含两种主要类型：参数方法和非参数方法。<br><strong>参数方法：</strong>假定数据服从以θ为参数的参数分布，该参数分布的概率密度函数f(x,θ)给出x属于该分布的概率，该值越小，x越可能是异常点。(仅对数据做的统计假定满足实际约束时可行)<br><strong>非参数方法</strong>：不假定先验统计模型，假定参数的个数较为灵活，并非完全无参。</p>
<span id="more"></span>
<h4 id="1-参数方法"><a href="#1-参数方法" class="headerlink" title="1. 参数方法"></a>1. 参数方法</h4><h5 id="1-1-基于正态分布的一元异常点检测"><a href="#1-1-基于正态分布的一元异常点检测" class="headerlink" title="1.1 基于正态分布的一元异常点检测"></a>1.1 基于正态分布的一元异常点检测</h5><p>仅涉及单变量的数据称为一元数据，假定数据由正态分布产生，由输入数据学习正态分布的参数，并把低概率的点识别为异常点。<br>输入数据集<img src="https://ae01.alicdn.com/kf/U0f09d41be5f54bc4892bab4863e0cc3b6.jpg" alt="公式1"> ，根据公式可以求出参数均值μ和方差σ，和概率密度函数p(x)。<br>$$<br>\mu =\frac{1}{n}\sum_{i=1}^n{x_i}<br>$$<br>$$<br>\sigma ^2=\frac{1}{n}\sum_{i=1}^n{\left( x_i-\mu \right)}^2<br>$$<br>$$<br>p\left( x \right) =\frac{1}{\sqrt{2\pi}\sigma}\exp \left( -\frac{\left( x-\mu \right) ^2}{2\sigma ^2} \right)<br>$$<br>在正太分布的假设中，可以为p(x)取值设置阈值来判断异常点。<br>或者简单地用3σ法则来判断，(μ-3σ，μ+3σ)包含99.7%的数据，某数据超出这个范围，可以简单的标记为异常点(Outlier)。</p>
<h5 id="1-2-多元离群点的检测方法"><a href="#1-2-多元离群点的检测方法" class="headerlink" title="1.2 多元离群点的检测方法"></a>1.2 多元离群点的检测方法</h5><p>一元异常点检测的方法可以扩展为多元异常点检测。<br>对于一个n维数据集合<img src="https://ae01.alicdn.com/kf/U22473047196a4af9b6ebca2d9ed966d3A.jpg" alt="公式2">，当各个维度的特征之间相互独立，可以按上述公式计算每个维度的均值和方差。基于正态分布的假设，根据概率<img src="https://ae01.alicdn.com/kf/U98b94844e25b4646a6538fab43facfbc8.jpg" alt="公式3">的大小判断x是否属于异常值。<br>$$<br>p\left(\vec{x} \right) =\prod_{j=1}^n{p\left(x_j;\mu _j;\sigma <em>{j}^{2} \right)}=\prod</em>{j=1}^n{\frac{1}{\sqrt{2\pi}\sigma _j}}\exp \left(-\frac{\left(x_j-\mu <em>j \right) ^2}{2\sigma <em>{j}^{2}} \right)<br>$$<br>当各个维度的特征相关时，需要用到基于多元正太分布来进行异常点检测，首先计算n维的均值向量<br>$$<br>\vec{\mu}=\left( E\left( x_1 \right) ,…,E\left( x_n \right) \right)<br>$$<br>和n×n的协方差矩阵：<br>$$<br>\sum{=\frac{1}{m}}\sum</em>{i=1}^m{\left( \vec{x}-\vec{\mu} \right) \left( \vec{x}-\vec{\mu} \right) ^T}<br>$$<br>如有新数据，可以计算<img src="https://ae01.alicdn.com/kf/U98b94844e25b4646a6538fab43facfbc8.jpg" alt="公式3">,根据概率值大小判断是否属于异常值。<br>$$<br>p\left( \vec{x} \right) =\frac{1}{\left( 2\pi ^{\frac{n}{2}}\left| \sum{} \right|^{</em>{2}^{1}} \right)}\exp \left( -\frac{1}{2}\left( \vec{x}-\vec{\mu} \right) ^T\sum{^{-1}\left( \vec{x}-\vec{\mu} \right)} \right)<br>$$</p>
<h4 id="2-非参数方法"><a href="#2-非参数方法" class="headerlink" title="2. 非参数方法"></a>2. 非参数方法</h4><p>非参数方法是对数据做较少的假定，因而适合多数情况。</p>
<h5 id="2-1-基于角度的方法"><a href="#2-1-基于角度的方法" class="headerlink" title="2.1 基于角度的方法"></a>2.1 基于角度的方法</h5><p>基于角度的方法往往在高维空间里会很有效，其主要思想是：数据边界上的数据很可能将整个数据包围在一个较小的角度内，而内部的数据点则可能以不同的角度围绕着他们。如下图所示，其中点A是一个异常点，点B位于数据内部。<br><img src="https://z3.ax1x.com/2021/05/25/gzJbtA.png" alt="a"><br>如果数据点与其余点离得较远，则潜在角度可能越小。因此，具有较小角度谱的数据点较有可能异常值，而具有较大角度谱的数据点不太可能是异常值。</p>
<h5 id="2-2-基于频数直方图的无监督异常点检测算法-HBOS"><a href="#2-2-基于频数直方图的无监督异常点检测算法-HBOS" class="headerlink" title="2.2 基于频数直方图的无监督异常点检测算法(HBOS)"></a>2.2 基于频数直方图的无监督异常点检测算法(HBOS)</h5><p>HBOS(Histogram-based Outlier Score)是一种单变量方法组合，HBOS在全局异常检测问题上表现良好，但不能检测局部异常值。但是HBOS比标准算法快得多，尤其是在大数据集上。<br>HOBS不能对特征之间的依赖关系进行建模，其基本假设是数据集的每个维度相互独立。然后对每个维度进行区间(bin)划分，对每一个bin进行评分。区间的密度越低，异常评分越高，越可能是异常点。<br>①等宽分桶：标准直方图构建，在值范围内构造k个等宽箱，样本若如每个箱的概率作为密度的估计[时间复杂度O(n)]<br>②动态宽度分桶：对所有值进行排序，将固定数量的N/k个连续值装进一个箱。N是总实例数，k是箱个数，所有箱面积一样，跨度大的箱的高度低，即密度小(例外情况：超过k个数相等，此时允许在同一个箱里超过N/k值)<br>对每个维度都计算一个独立的直方图，其中每个箱子的高度表示密度的估计，对直方图进行归一化处理，最后计算每一个实例的HBOS值。<br>$$<br>HBOS\left( p \right) =\sum_{i=0}^d{\log \left( \frac{1}{hist_i\left( p \right)} \right)}<br>$$<br>详细推导过程：<br>假设样本o的第i个特征概率密度为<img src="https://ae01.alicdn.com/kf/U3264e024f94c45a0bbe5110c82bca661k.jpg" alt="x">，则o的概率密度为：<br>$$<br>P\left( p \right) =P_1\left( p \right) P_2\left( p \right) …P_d\left( p \right)<br>$$<br>两边取对数<br>$$<br>\log \left( P\left( p \right) \right) =\log \left( P_1\left( p \right) P_2\left( p \right) …P_d\left( p \right) \right) =\sum_{i=1}^d{\log \left( P_i\left( p \right) \right)}<br>$$<br>为了达到概率密度越大异常评分越小，两边同乘-1。<br>$$<br>-\log \left( P\left( p \right) \right) =-\sum_{i=1}^d{\log \left( P_i\left( p \right) \right)}=\sum_{i=1}^d{\frac{1}{\log \left( P_i\left( p \right) \right)}}<br>$$<br>最后：<br>$$<br>HOBS\left( p \right) =-\log \left( P\left( p \right) \right) =\sum_{i=1}^d{\frac{1}{\log \left( P_i\left( p \right) \right)}}<br>$$</p>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://github.com/datawhalechina/team-learning-data-mining/tree/master/AnomalyDetection">datawhale-异常检测开源学习资料</a></p>
]]></content>
      <categories>
        <category>Anomaly Detection</category>
      </categories>
      <tags>
        <tag>Normal distribution</tag>
        <tag>HBOS</tag>
      </tags>
  </entry>
  <entry>
    <title>异常检测——基于相似度的方法</title>
    <url>/2021/05/20/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E2%80%94%E2%80%94%E5%9F%BA%E4%BA%8E%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>异常值通常具有更高的离群程度分数值，同时也更具有可解释性。在基于相似度的异常检测方法中，主要思想是异常点的表示与正常点不同。</p>
<h4 id="1-基于距离的度量"><a href="#1-基于距离的度量" class="headerlink" title="1.  基于距离的度量"></a>1.  基于距离的度量</h4><p>基于距离的异常检测有这样一个前提假设，即异常点的k近邻距离要远大于正常点。常见的方法是嵌套循环，第一层循环遍历每个数据，第二层循环进行异常判断计算当前点与其他点的距离，一旦发现多于k个数据点与当前点的距离在D之内，则将该点标记为非异常值。 此法计算的时间复杂度为O(N<sup>2</sup>)，当数据量比较大时需要修剪方法以加快距离计算。</p>
<span id="more"></span>
<h5 id="1-1-基于单元的方法"><a href="#1-1-基于单元的方法" class="headerlink" title="1.1 基于单元的方法"></a>1.1 基于单元的方法</h5><p>在基于单元格的技术中，数据空间被划分为单元格，单元格的宽度是阈值D和数据维数d的函数。<br><img src="https://z3.ax1x.com/2021/05/25/gzuzmn.jpg" alt="xx"><br>L1邻居表示通过最多1个单元间的边界可从该单元到达的单元格的集合，L2邻居表示通过跨越2个或3个边界而获得的那些单元格。<br>以二维数据为例，网格间的距离为D/2√d，其具有以下性质：<br>①单元格中两点之间的距离最多为D/2 。<br>②一个点与L1邻接点之间的距离最大为D。<br>③一个点与它的Lr邻居(其中r&gt;2)中的一个点之间的距离至少为D。</p>
<p>但是L2中并不能直接得出结论，需要另外定义规则，如：<br>①如果一个单元格中包含超过k个数据点及其L1邻居，那么这些数据点都不是异常值。<br>②如果单元A及其相邻L1和L2中包含少于k个数据点，则单元A中的所有点都是异常值。<br>利用第二条规则的修剪能力，对于包含至少一个数据点的每个单元格A，计算其中的点数及其L1和L2邻居的总和。 如果该数字不超过k，则将单元格A中的所有点标记为离群值。</p>
<p>对于此时仍未标记为异常值或非异常值的单元格中的数据点，需要明确计算其k最近邻距离，在L1和L2中不超过k个且距离小于D的数据点，则声明为异常值。</p>
<h5 id="1-2-基于索引的方法"><a href="#1-2-基于索引的方法" class="headerlink" title="1.2 基于索引的方法"></a>1.2 基于索引的方法</h5><p>对于一个给定数据集，基于索引的方法利用多维索引结构(如R树、k-d树)来搜索每个数据对象A在半径范围D内的相邻点。设M是一个异常值在其D-邻域内允许含有对象数的最大值，若发现某个数据对象A的D-邻域内出现M+1甚至更多个相邻点， 则判定对象A不是异常值。该算法时间复杂度在最坏情况下为O(kN<sup>2</sup>)，其中k是数据集维数，N是数据集包含对象的个数。但是构造索引的任务本身也需要密集复杂的计算量。</p>
<h4 id="2-基于密度的度量"><a href="#2-基于密度的度量" class="headerlink" title="2.  基于密度的度量"></a>2.  基于密度的度量</h4><p>基于密度的算法主要有局部离群因子(LocalOutlierFactor，LOF)，以及LOCI、CLOF等基于LOF的改进算法。LOF的关键步骤在于给每个数据点都分配一个离散度，其主要思想是：针对给定的数据集，对其中的任意一个数据点，如果在其局部邻域内的点都很密集，那么认为此数据点为正常数据点；而离群点则是距离正常数据点最近邻的点都比较远的数据点。</p>
<h5 id="2-1-k-距离"><a href="#2-1-k-距离" class="headerlink" title="2.1 k-距离"></a>2.1 k-距离</h5><p>类似k近邻的思路，首先定义一个k-距离，对于数据集D中的给定对象p，对象p与数据集D中任意点o的距离为d(po)。我们把数据集D中与对象p距离最近的k个相邻点的最远距离表示为k-distance(p)，把距离对象p距离第k近的点表示为o<sub>k</sub>，那么给定对象p和点o<sub>k</sub>之间的距离d(p,o<sub>k</sub>)=k-distance(p)，满足：<br>①在集合D中至少有不包括p在内的k个点 o’，其中o’∈D \ {p}，满足d(p,o’)≤d(p,o<sub>k</sub>)<br>②在集合D中最多有不包括p在内的k-1个点o’，其中o’∈D \ {p}，满足d(p,o’)&lt;d(po<sub>k</sub>)<br>直观一些理解，就是以对象p为中心，对数据集D中的所有点到p的距离进行排序，距离对象p第k近的点o<sub>k</sub>与p之间的距离就是k-距离。</p>
<h5 id="2-2-k-邻域"><a href="#2-2-k-邻域" class="headerlink" title="2.2 k-邻域"></a>2.2 k-邻域</h5><p>到对象p的距离小于等于k-距离的所有点的集合就是k-邻域：<br>$$<br>N_{k-dis\tan ce\left(p \right)}\left(p \right) =\left{ q\in D\backslash{q}\left| d\left(p,d \right) \right. \le k-ds\tan ce\left(p \right) \right}<br>$$<br>k-邻域包含对象p的第k距离以内的所有点，包括第k距离点；对象p的第k邻域点的个数丨N<sub>k</sub>(p)≥3丨。<br>在二维平面上展示，对象p的k-邻域是以对象p为圆心、k-距离为半径围成的圆形区域。<br><img src="https://www.hualigs.cn/image/608950de59262%E3%80%82jpg" alt="img"></p>
<h5 id="2-3-可达距离"><a href="#2-3-可达距离" class="headerlink" title="2.3  可达距离"></a>2.3  可达距离</h5><p>按照到对象o的距离远近，将数据集D内的点按照到o 的距离分为两类:<br>①若p<sub>i</sub>在对象o的k-邻域内，则可达距离就是给定点p<sub>i</sub>关于对象o的k-距离；<br>②若p<sub>i</sub>在对象o的k-邻域外，则可达距离就是给定点p<sub>i</sub>关于对象o的实际距离。<br>给定点p<sub>i</sub>关于对象o的可达距离用数学公式可以表示为：<br>$$<br>r e a c h−d i s t_ k (p,o) = max {k−distance(o),d (p,o)}<br>$$<br>这样的分类处理可以简化后续的计算，同时让得到的数值区分度更高。<br><img src="https://www.hualigs.cn/image/608951ddb06ae%E3%80%82jpg" alt="可达距离。jpg"><br>如图所示：<br>①p<sub>1</sub>在对象o的k-邻域内，d(p<sub>1</sub>,o)&lt;k−distance(o)，<br>可达距离reach−dist<sub>k</sub>(p<sub>1</sub>,o)=k−distance(o) ;<br>②p<sub>2</sub>在对象o的k-邻域外，d (p<sub>2</sub>,o)&gt;k−distance(o)，<br>可达距离reach−dist<sub>k</sub>(p<sub>2</sub>,o)= d(p<sub>2</sub>,o) ;</p>
<blockquote>
<p>需要注意的是:为了减少距离的计算开销，对象的k-邻域内的所有对象的k-距离计算量可以被显著降低，相当于使用一个阈值把需要计算的部分“截断”了， 的值越高，无需计算的邻近点越多，计算开销越小。但是另一方面，k的值变高，可能意味着可达距离变远，对集群点和离群点的区分度可能变低。因此，如何选择k值，是LOF算法能否达到效率与效果平衡的重要因素。</p>
</blockquote>
<h5 id="3-4-局部可达密度"><a href="#3-4-局部可达密度" class="headerlink" title="3.4 局部可达密度"></a>3.4 局部可达密度</h5><p>可以将“密度”直观地理解为点的聚集程度，简单来说，点与点之间距离越短，则密度越大。我们使用数据集D中对象p与对象o的k-邻域内所有点的可达距离平均值的倒数来定义局部可达密度。<br>在进行局部可达密度的计算的时候，我们需要避免数据集内所有数据落在同一点上，即所有可达距离之和为0的情况：此时局部密度为∞，后续计算将无法进行。</p>
<p>LOF算法中针对这一问题进行了如下的定义：对于数据集D内的给定对象p，存在至少MinPts(p)≥1个不同于p的点。因此，我们使用对象p到o∈N_{MinPts}(p)的可达距离reach-dist<sub>{MinPts}</sub>(p, o)作为度量对象p邻域的密度的值。点p的局部可达密度计算公式为：<br>$$<br>lrd_{MinPts}(p)=1/(\frac {\sum\limits_{o∈N_{MinPts}(p)} reach-dist_{MinPts}(p,o)} {\left\vert N_{MinPts}(p) \right\vert})<br>$$<br>由公式可看出，这里是对给定点p进行度量，计算其邻域内的所有对象o到给定点p的可达距离平均值。给定点p的局部可达密度越高，越可能与其邻域内的点p属于同一簇；密度越低，越可能是离群点。  </p>
<h5 id="3-5-局部异常因子"><a href="#3-5-局部异常因子" class="headerlink" title="3.5 局部异常因子"></a>3.5 局部异常因子</h5><p>得到lrd(局部可达密度)以后就可以将每个点的lrd将与它们的k个邻点的lrd进行比较，得到局部异常因子LOF。 LOF是对象p的邻居点o(o∈N<sub>MinPts</sub>(p))的lrd平均值与p的lrd的比值。</p>
<p>p的局部可达密度越低，且它的MinPts近邻的平均局部可达密度越高，则p的LOF值越高。如果这个比值越接近1，说明o的邻域点密度差不多，o可能和邻域同属一簇；如果这个比值小于1，说明o的密度高于其邻域点密度，o为密集点；如果这个比值大于1，说明o的密度小于其邻域点密度，o可能是异常点。<br><img src="https://www.hualigs.cn/image/6089531d9e0f7.jpg" alt="局部异常因子公式.png"><br>由上述公式计算出的LOF数值，就是我们所需要的离群点分数。</p>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://github.com/datawhalechina/team-learning-data-mining/tree/master/AnomalyDetection">datawhale-异常检测开源学习资料</a></p>
]]></content>
      <categories>
        <category>Anomaly Detection</category>
      </categories>
      <tags>
        <tag>LOF</tag>
      </tags>
  </entry>
  <entry>
    <title>异常检测——线性相关方法</title>
    <url>/2021/05/17/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>真实数据集中不同维度的特征可能具有高度相关性，这是因为不同的特征往往是由相同的基础过程以密切相关的方式产生的。这被称为——回归建模，是一种参数化的相关性分析。<br>一类相关性分析通过其他变量预测单独的属性值，另一类方法用一些潜在变量来代表整个数<br>据。前者的代表是<strong>线性回归</strong>，后者一个典型的例子是<strong>主成分分析</strong>。<br>线性相关分析基于的假设是：①近似线性相关假设；②子空间假设。为了确定线性模型是否适合数据集，需要进行探索性和可视化分析。</p>
<span id="more"></span>
<h4 id="0-探索性和可视化分析"><a href="#0-探索性和可视化分析" class="headerlink" title="0. 探索性和可视化分析"></a>0. 探索性和可视化分析</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data.describe()</span><br><span class="line">train_data.info()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#相关性分析</span></span><br><span class="line">numeric_features = [<span class="string">&#x27;f&#x27;</span>+<span class="built_in">str</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>)]</span><br><span class="line">numeric = train_data[numeric_features]</span><br><span class="line">correlation.numeric.corr()</span><br><span class="line"></span><br><span class="line">f,ax = plt.subplots(figsize=(<span class="number">15</span>,<span class="number">15</span>))</span><br><span class="line">sns.heatmap(correlation,square=<span class="literal">True</span>)</span><br><span class="line">plt.tile(<span class="string">&#x27;Correlation of Numeric features&#x27;</span>,y=<span class="number">1</span>,size=<span class="number">16</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://z3.ax1x.com/2021/05/25/gzuhyd.jpg" alt="xx"></p>
<h4 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1. 线性回归"></a>1. 线性回归</h4><p>线性回归：假设不同维度的变量具有一定的相关性，并可以通过一个相关系数矩阵进行衡量。<br>在线性回归中，异常值是根据自变量对因变量的影响来定义的，自变量之间相互关系中的异常则不那么重要。(此处的异常点检测是基于数据点的整体分布)</p>
<h5 id="1-1-最小二乘法"><a href="#1-1-最小二乘法" class="headerlink" title="1.1 最小二乘法"></a>1.1 最小二乘法</h5><p>基于最小二乘拟合的线性回归可以如下定义：带标签数据集<br>$$<br>D=\left{ \left( x_1,y_1 \right) ,…,\left( x_m,y_m \right) \right}<br>$$<br>m个样本的特征组成矩阵X；m个样本的标签组成标签向量<br>$$<br>y=\left( y_1,…,y_m \right) ^T<br>$$<br>目的是求出θ，使得目标函数J(θ)最小。<br>$$<br>J\left( \theta \right) =\lVert \left. X\theta -y \rVert <em>{2}^{2} \right.<br>$$<br>为什么目标函数J(θ)是残差向量的2-范数的平方？可以这样解释：<br>某个样本(x<sub>i</sub>,y<sub>i</sub>)在θ确定情况下满足：<br>$$<br>y_i=\theta ^Tx_i+\epsilon <em>i<br>$$<br>ϵi为误差项，可以看作独立同分布的随机变量，且服从高斯分布。可以以计算yi的概率密度函数：<br>$$<br>L\left( \theta \right) =p\left( y_i\left| x_i;\theta \right. \right) =\frac{1}{\sqrt{2\pi}\sigma}\exp \left( -\frac{\left( y_i-\theta ^Tx_i \right) ^2}{2\sigma ^2} \right)<br>$$<br>要使J(θ)最小，就要使使L(θ)最大，可使用最大似然估计法。<br>$$<br>L\left( \theta \right) =\log \prod</em>{i=1}^m{\frac{1}{\sqrt{2\pi}\sigma}\exp \left( -\frac{\left( y_i-\theta ^Tx_i \right) ^2}{2\sigma ^2} \right)}=m\log \frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{\sigma ^2}·\frac{1}{2}\sum</em>{i-=1}^m{\left( y_i-\theta ^Tx_i \right)}^2<br>$$<br>通常做法是对似然函数求导，使导数为0。上述也证明了要让L(θ)最大就是要让J(θ)最小。<br>$$<br>\sum_{i-=1}^m{\left( y_i-\theta ^Tx_i \right)}^2=\lVert \left. X\theta -y \rVert \right. _{2}^{2}<br>$$<br>说回最小二乘，目的是求出最优点θ使J(θ)最小。<br>设残差向量r=Xθ−y，在二维空间中讨论，Xθ为一条直线，y为一个向量，存在最优点θ<sup>✲</sup>使得J(θ)最小，这个θ<sup>✲</sup>是最小二乘解集合中的一个元素，如图所示：<br><img src="https://z3.ax1x.com/2021/05/25/gzufQH.png" alt="xx"><br>由图看出这个θ<sup>✲</sup>是r垂直于Xθ时取到。由定义，Xθ是矩阵X的列空间C(X)，与C(X)垂直的是矩阵X的左零空间N(X<sup>T</sup>)，由左零空间定义：<br>$$<br>X^Tr=0<br>$$<br>把r=Xθ−y带入，得到：<br>$$<br>X^T\left( X\theta -y \right) =0<br>$$<br>整理得最优参数θ为：<br>$$<br>\theta =\left( X^T·X \right) ^{-1}·\left( X^T·y \right)<br>$$<br>个人觉得J(θ)也可以认为是某种异常得分。</p>
<h5 id="1-2-梯度下降"><a href="#1-2-梯度下降" class="headerlink" title="1.2 梯度下降"></a>1.2 梯度下降</h5><p>线性回归的优化目标是损失函数，就需要一种优化算法。<br>线性回归中的常用损失函数是均方误差MSE，表达式为：<br>$$<br>L\left( w,b \right) =\frac{1}{2}\left( \hat{y}^{\left( i \right)}-y^{\left( i \right)} \right) ^2 = \frac{1}{n}\sum_{i=1}^n{l^{\left( i \right)}}\left( w,b \right) =\frac{1}{n}\sum_{i=1}{\frac{1}{2}\left( W^Tx^{\left( i \right)}+b-y^{\left( i \right)} \right)}^2<br>$$<br>当模型和损失函数形式较为简单时，面的误差最小化问题的解可以直接用公式表达出来。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。<br>在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）被广泛使用。他的算法是：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch），然后求小批量中数据样本的平均损失和有关模型参数的导数（梯度），最后用此结果与预先设定的学习率的乘积作为模型参数在本次迭代的减小量。如下式所示：<br>$$<br>\left( w,b \right) \gets \left( w,b \right) -\frac{\eta}{\left| \left. \mathbb{B} \right| \right.}\sum_{i\in \mathbb{B}}{\partial _{\left( w,b \right)}}l^{\left( i \right)}\left( w,b \right)<br>$$<br>B表示批量大小batch size；η表示学习率，代表每次学习的步长大小。</p>
<h4 id="2-主成分分析"><a href="#2-主成分分析" class="headerlink" title="2. 主成分分析"></a>2. 主成分分析</h4><p>最小二乘法试图找到一个与数据具有最佳匹配(d-1)维超平面；主成分分析方法可用于解决这一问题的广义版本，它可以找到任意k(k&lt;d)维的最优表示超平面，从而使平方投<br>影误差最小化。</p>
<h5 id="2-1-原理"><a href="#2-1-原理" class="headerlink" title="2.1 原理"></a>2.1 原理</h5><p>对于d维，包含N个样本的数据，用R<sub>i</sub>表示其中第i行[x<sub>i1</sub>,…x<sub>id</sub>]，可得到d×d的协方差矩阵：<br>$$<br>\sum{=\left( R-\bar{R} \right)}^T·\left( R-\bar{R} \right)<br>$$<br>∑是对称并且半正定的，因此可以进行相似对角化：<br>$$<br>\sum{=P·D·P^T}<br>$$<br>这里的D为对角矩阵，对角元素为特征值。P为标准正交矩阵，每一行为对应的特征向量；这些标准正交向量提供了数据应该投影的轴线方向。<br>将样本的协方差矩阵特征值分解以后，特征值就是样本投影到这个轴上后对应的方差，特征值越小，说明投影以后在这个轴上样本点分布集中，而异常点在这种情况下更容易偏移，利用这一点可以作为衡量样本异常的一个指标。在PCA做降维的时候，起作用的是大的特征值对应的特征向量，而在异常检测中，起作用的是特征值小的对应的特征向量。根据以上思想，我们可以定义PCA中一个点x的异常评分：<br>$$<br>Score\left( x \right) =\sum_{i=1}^d{\frac{\left| \left. x^Te_i \right| \right.}{\lambda _i}}<br>$$<br>其中，e<sub>i</sub>为第i个特征向量，λ为沿该方向的方差(也是特征值)，可以看出对异常得分的大部分贡献是由λ值较小的主成分的提供的。</p>
<h5 id="2-2-PyOD实例"><a href="#2-2-PyOD实例" class="headerlink" title="2.2 PyOD实例"></a>2.2 PyOD实例</h5><p>测试PCA和HBOS的性能对比。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> pyod.models.pca <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> pyod.models.hbos <span class="keyword">import</span> HBOS</span><br><span class="line"><span class="keyword">from</span> pyod.utils.data <span class="keyword">import</span> evaluate_print</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    data = pd.read_csv(<span class="string">&#x27;dataverse_files/breast-cancer-unsupervised-ad.csv&#x27;</span>, header=<span class="literal">None</span>)</span><br><span class="line">    data_x=data[[i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">30</span>)]].values <span class="comment">#特征</span></span><br><span class="line">    data_y=data[<span class="number">30</span>].<span class="built_in">map</span>(<span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x==<span class="string">&#x27;o&#x27;</span> <span class="keyword">else</span> <span class="number">0</span>).values <span class="comment">#标签</span></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=<span class="number">0.3</span>,stratify=data_y)<span class="comment">#使用分层抽样，构建训练集和测试集</span></span><br><span class="line">    train_number=X_train.shape[<span class="number">0</span>]<span class="comment">#训练集样本的个数</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">#PCA方法</span></span><br><span class="line">    clf1 = PCA(n_components=<span class="number">15</span>)</span><br><span class="line">    clf1.fit(X_train)</span><br><span class="line">    y_train_scores_clf1 = clf1.decision_scores_</span><br><span class="line">    y_test_scores_clf1 = clf1.decision_function(X_test)</span><br><span class="line"></span><br><span class="line">	<span class="comment">#HBOS方法</span></span><br><span class="line">    clf2 = HBOS(n_bins=(<span class="built_in">int</span>)(np.sqrt(train_number)))</span><br><span class="line">    clf2.fit(X_train)</span><br><span class="line">    y_train_scores_clf2 = clf2.decision_scores_</span><br><span class="line">    y_test_scores_clf2 = clf2.decision_function(X_test)</span><br><span class="line"></span><br><span class="line">	<span class="comment">#结果对比</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Training Data:&quot;</span>)</span><br><span class="line">    evaluate_print(<span class="string">&#x27;PCA&#x27;</span>, y_train, y_train_scores_clf1)</span><br><span class="line">    evaluate_print(<span class="string">&#x27;HBOS&#x27;</span>, y_train, y_train_scores_clf2)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Test Data:&quot;</span>)</span><br><span class="line">    evaluate_print(<span class="string">&#x27;PCA&#x27;</span>, y_test, y_test_scores_clf1)</span><br><span class="line">    evaluate_print(<span class="string">&#x27;HBOS&#x27;</span>, y_test, y_test_scores_clf2)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果发现，PCA的性能优于HBOS。</p>
<blockquote>
<p>局限性:<br>1.回归分析作为检测离群值的工具有一些局限性：为了使回归分析技术有效，数据需要高度相关，并沿着低维子空间对齐。当数据不相关，但在某些区域高度聚集时，这种方法可能不会有效。<br>2.数据中的相关性在本质上可能不是全局性的。子空间相关性可能是特定于数<br>据的特定位置的。在这种情况下，由主成分分析发现的全局子空间对于异常检测是次优的。因此，为了创建更一般的局部子空间模型，有时将线性模型与邻近模型结合起来是有用的。</p>
</blockquote>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://github.com/datawhalechina/team-learning-data-mining/tree/master/AnomalyDetection">datawhale-异常检测开源学习资料</a></p>
]]></content>
      <categories>
        <category>Anomaly Detection</category>
      </categories>
      <tags>
        <tag>OLSE</tag>
        <tag>Linear Regression</tag>
        <tag>Gradient descent</tag>
        <tag>PCA</tag>
        <tag>PyOD</tag>
      </tags>
  </entry>
  <entry>
    <title>异常检测(Anomaly Detection)简介</title>
    <url>/2021/05/12/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<p>异常检测顾名思义就是检测出与正常数据不同的数据，检测给定的新数据是否属于这组数据集。</p>
<h4 id="1-异常检测的任务类型"><a href="#1-异常检测的任务类型" class="headerlink" title="1. 异常检测的任务类型"></a>1. 异常检测的任务类型</h4><p><strong>无监督</strong>：训练集无标签<br><strong>有监督</strong>：训练集的正例和反例均有标签<br><strong>半监督</strong>：在训练集中只有正例，异常实例不参与训练</p>
<span id="more"></span>
<h4 id="2-异常检测的难点"><a href="#2-异常检测的难点" class="headerlink" title="2. 异常检测的难点"></a>2. 异常检测的难点</h4><p><strong>数据量少</strong>：异常检测任务通常情况下负样本（abnormal）较少，0、1两类样本严重不均衡，有时候依赖于人工标签。<br><strong>噪音</strong>：异常和噪音有时容易混淆，如两幅图中的A点识别难度就不一样。<br><img src="https://z3.ax1x.com/2021/05/25/gzuSPO.png" alt="xx"></p>
<h4 id="3-异常检测方法简介"><a href="#3-异常检测方法简介" class="headerlink" title="3. 异常检测方法简介"></a>3. 异常检测方法简介</h4><h5 id="3-1-基于统计学的方法"><a href="#3-1-基于统计学的方法" class="headerlink" title="3.1 基于统计学的方法"></a>3.1 基于统计学的方法</h5><p>通常做法是假设样本服从高斯分布，计算p(x)=ϵ作为判定正常和异常的阈值。<br><img src="https://z3.ax1x.com/2021/05/25/gzu9Re.png" alt="xx"></p>
<h5 id="3-2-线性模型"><a href="#3-2-线性模型" class="headerlink" title="3.2 线性模型"></a>3.2 线性模型</h5><p>在线性模型中，有2个假设：<br>①不同维度的变量近似线性相关，可以通过一个相关系数矩阵进行衡量。<br>②数据是镶嵌在低维子空间中的，线性建模是为了找到某个低维子空间使异常点区别于正常点。<br>常用模型的包括：<br><strong>最小二乘法</strong>——最小化目标函数得到直线方程,计算每个变量与回归方程之间的残差,可通过3σ 法则判断异常值.<br><strong>PCA</strong>——将样本的协方差矩阵特征值分解后，特征值是样本投影到轴上对应的方差，特征值越小，投影后样本点分布越集中，异常点容易偏离，以此作为判定异常值的一个指标。</p>
<h5 id="3-3-基于邻近度的方法"><a href="#3-3-基于邻近度的方法" class="headerlink" title="3.3 基于邻近度的方法"></a>3.3 基于邻近度的方法</h5><p>这类算法适用于数据点的聚集程度高、离群点较少的情况，不适用于数据量大、维度高的数据。<br>①基于集群（簇）的检测——DBSCAN，核心点距离ℇ内最小包含点数、距离ℇ与阈值比较。<br>②基于距离的度量——KNN，将K-近邻距离与阈值比较。<br>③基于密度的度量——LOF(局部离群因子)，邻域点密度与阈值比较。<br><img src="https://z3.ax1x.com/2021/05/25/gzupGD.png" alt="xx"></p>
<h5 id="3-4-集成方法"><a href="#3-4-集成方法" class="headerlink" title="3.4 集成方法"></a>3.4 集成方法</h5><p>①<strong>孤立森林（Isolation Forest）</strong>——不停地使用随机超平面分隔每个子空间，直到每个子空间只有一个数据点为止，低密度的点被单独分配到一个子空间，孤立数低于阈值时，定义为异常值。<br><img src="https://z3.ax1x.com/2021/05/25/gznxIK.png" alt="xx"><br>②树模型——通过树不断划分子空间,数据点在多个树上的平均深度越浅,越可能为异常值。</p>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://github.com/datawhalechina/team-learning-data-mining/tree/master/AnomalyDetection">datawhale-异常检测开源学习资料</a></p>
]]></content>
      <categories>
        <category>Anomaly Detection</category>
      </categories>
      <tags>
        <tag>LOF</tag>
        <tag>OLSE</tag>
        <tag>PCV</tag>
        <tag>Isolation Forest</tag>
        <tag>KNN</tag>
        <tag>DBSCAN</tag>
      </tags>
  </entry>
  <entry>
    <title>模型融合_1</title>
    <url>/2021/03/15/%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88_1/</url>
    <content><![CDATA[<p>Kaggle和天池比赛中常用提高成绩的三种方法：</p>
<blockquote>
<p>1.特征工程<br>2.模型调参<br>3.模型融合<br>模型融合主要有以下几种方式：</p>
</blockquote>
<h4 id="简单加权融合"><a href="#简单加权融合" class="headerlink" title="简单加权融合:"></a>简单加权融合:</h4><blockquote>
<p>①回归（分类概率）：算术平均融合（Arithmetic mean），几何平均融合（Geometric mean）；<br>②分类：投票（Voting)<br>③综合：排序融合(Rank averaging)，log融合</p>
</blockquote>
<h4 id="stacking-blending"><a href="#stacking-blending" class="headerlink" title="stacking/blending:"></a>stacking/blending:</h4><blockquote>
<p>构建多层模型，把初级学习器的输出当作下一层的输入。</p>
</blockquote>
<span id="more"></span>
<h4 id="boosting-bagging（在xgboost-Adaboost-GBDT中已经用到）"><a href="#boosting-bagging（在xgboost-Adaboost-GBDT中已经用到）" class="headerlink" title="boosting/bagging（在xgboost,Adaboost,GBDT中已经用到）:"></a>boosting/bagging（在xgboost,Adaboost,GBDT中已经用到）:</h4><blockquote>
<p>多个分类器的整合</p>
</blockquote>
<h4 id="部分代码案例"><a href="#部分代码案例" class="headerlink" title="部分代码案例:"></a>部分代码案例:</h4><h5 id="1-简单加权平均"><a href="#1-简单加权平均" class="headerlink" title="1.简单加权平均"></a>1.简单加权平均</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Weighted_method</span>(<span class="params">test_pre1,test_pre2,test_pre3,w=[<span class="number">1</span>/<span class="number">3</span>,<span class="number">1</span>/<span class="number">3</span>,<span class="number">1</span>/<span class="number">3</span>]</span>):</span></span><br><span class="line">    Weighted_result = w[<span class="number">0</span>]*pd.Series(test_pre1)+w[<span class="number">1</span>]*pd.Series(test_pre2)+w[<span class="number">2</span>]*pd.Series(test_pre3)</span><br><span class="line">    <span class="keyword">return</span> Weighted_result</span><br><span class="line">    </span><br><span class="line">Weighted_pre = Weighted_method(test_pre1,test_pre2,test_pre3,w)</span><br></pre></td></tr></table></figure>
<h5 id="2-Stacking融合-回归"><a href="#2-Stacking融合-回归" class="headerlink" title="2.Stacking融合(回归)"></a>2.Stacking融合(回归)</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Stacking_method</span>(<span class="params">train_reg1,train_reg2,train_reg3,y_train_true,test_pre1,test_pre2,test_pre3,model_L2= linear_model.LinearRegression(<span class="params"></span>)</span>):</span>    model_L2.fit(pd.concat([pd.Series(train_reg1),pd.Series(train_reg2),pd.Series(train_reg3)],axis=<span class="number">1</span>).values,y_train_true)</span><br><span class="line">    Stacking_result = model_L2.predict(pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=<span class="number">1</span>).values)</span><br><span class="line">    <span class="keyword">return</span> Stacking_result</span><br><span class="line"></span><br><span class="line">Stacking_pre = Stacking_method(train_reg1,train_reg2,train_reg3,y_train_true,</span><br><span class="line">                               test_pre1,test_pre2,test_pre3,model_L2)</span><br></pre></td></tr></table></figure>
<h5 id="3-Voting投票机制"><a href="#3-Voting投票机制" class="headerlink" title="3.Voting投票机制"></a>3.Voting投票机制</h5><blockquote>
<p>硬投票：对多个模型直接进行投票，不区分模型结果的相对重要度，最终投票数最多的类为最终被预测的类。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eclf = VotingClassifier(estimators=[(<span class="string">&#x27;lgb&#x27;</span>, clf1), (<span class="string">&#x27;rf&#x27;</span>, clf2), (<span class="string">&#x27;svc&#x27;</span>, clf3)], voting=<span class="string">&#x27;hard&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> clf, label <span class="keyword">in</span> <span class="built_in">zip</span>([clf1, clf2, clf3, eclf], [<span class="string">&#x27;LGB&#x27;</span>, <span class="string">&#x27;Random Forest&#x27;</span>, <span class="string">&#x27;SVM&#x27;</span>, <span class="string">&#x27;Ensemble&#x27;</span>]):</span><br><span class="line">    scores = cross_val_score(clf, x, y, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Accuracy: %0.2f (+/- %0.2f) [%s]&quot;</span> % (scores.mean(), scores.std(), label))</span><br></pre></td></tr></table></figure>
<h5 id="4-分类模型的Stacking融合"><a href="#4-分类模型的Stacking融合" class="headerlink" title="4.分类模型的Stacking融合"></a>4.分类模型的Stacking融合</h5><blockquote>
<p>Stacking与Blending相比存在一定优势:<br>1.充分使用数据<br>2.使用多次的交叉验证会比较稳健<br>3.不容易过拟合</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">clfs = [LogisticRegression(solver=<span class="string">&#x27;lbfgs&#x27;</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;entropy&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> j, clf <span class="keyword">in</span> <span class="built_in">enumerate</span>(clfs):</span><br><span class="line">    <span class="comment">#依次训练各个单模型</span></span><br><span class="line">    clf.fit(X_d1, y_d1)</span><br><span class="line">    y_submission = clf.predict_proba(X_d2)[:, <span class="number">1</span>]</span><br><span class="line">    dataset_d1[:, j] = y_submission</span><br><span class="line">    <span class="comment">#对于测试集，直接用这k个模型的预测值作为新的特征。</span></span><br><span class="line">    dataset_d2[:, j] = clf.predict_proba(X_predict)[:, <span class="number">1</span>]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;val auc Score: %f&quot;</span> % roc_auc_score(y_predict, dataset_d2[:, j]))</span><br><span class="line"></span><br><span class="line"><span class="comment">#融合使用的模型</span></span><br><span class="line">clf = GradientBoostingClassifier(learning_rate=<span class="number">0.02</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">30</span>)</span><br><span class="line">clf.fit(dataset_d1, y_d2)</span><br></pre></td></tr></table></figure>
<h5 id="5-其他Stacking"><a href="#5-其他Stacking" class="headerlink" title="5.其他Stacking"></a>5.其他Stacking</h5><blockquote>
<p>将特征放进模型中预测，并将预测结果变换并作为新的特征加入原有特征中再经过模型预测结果</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Ensemble_add_feature</span>(<span class="params">train,test,target,clfs</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> j,clf <span class="keyword">in</span> <span class="built_in">enumerate</span>(clfs):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;依次训练各个单模型&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># print(j, clf)</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;使用第1个部分作为预测，第2部分来训练模型，获得其预测的输出作为第2部分的新特征。&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]</span></span><br><span class="line"></span><br><span class="line">        clf.fit(train,target)</span><br><span class="line">        y_train = clf.predict(train)</span><br><span class="line">        y_test = clf.predict(test)</span><br><span class="line"></span><br><span class="line">        <span class="comment">## 新特征生成</span></span><br><span class="line">        train_[:,j*<span class="number">2</span>] = y_train**<span class="number">2</span></span><br><span class="line">        test_[:,j*<span class="number">2</span>] = y_test**<span class="number">2</span></span><br><span class="line">        train_[:, j+<span class="number">1</span>] = np.exp(y_train)</span><br><span class="line">        test_[:, j+<span class="number">1</span>] = np.exp(y_test)</span><br><span class="line">        <span class="comment"># print(&quot;val auc Score: %f&quot; % r2_score(y_predict, dataset_d2[:, j]))</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Method &#x27;</span>,j)</span><br><span class="line"></span><br><span class="line">    train_ = pd.DataFrame(train_)</span><br><span class="line">    test_ = pd.DataFrame(test_)</span><br><span class="line">    <span class="keyword">return</span> train_,test_</span><br><span class="line"></span><br><span class="line"><span class="comment">#模型融合中使用到的各个单模型</span></span><br><span class="line">clfs = [LogisticRegression(),</span><br><span class="line">        RandomForestClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;gini&#x27;</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators=<span class="number">5</span>, n_jobs=-<span class="number">1</span>, criterion=<span class="string">&#x27;entropy&#x27;</span>),</span><br><span class="line">        GradientBoostingClassifier(learning_rate=<span class="number">0.05</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">5</span>)]</span><br><span class="line"></span><br><span class="line">New_train,New_test = Ensemble_add_feature(x_train,x_test,y_train,clfs)</span><br><span class="line">clf = GradientBoostingClassifier(learning_rate=<span class="number">0.02</span>, subsample=<span class="number">0.5</span>, max_depth=<span class="number">6</span>, n_estimators=<span class="number">30</span>)</span><br><span class="line">clf.fit(New_train, y_train)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Ensemble Learning</category>
      </categories>
      <tags>
        <tag>voting</tag>
        <tag>Stacking</tag>
        <tag>Blending</tag>
      </tags>
  </entry>
  <entry>
    <title>特征工程_1</title>
    <url>/2021/04/21/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B_1/</url>
    <content><![CDATA[<p>特征工程就是将原始数据空间变换到新的特征空间，在新的特征空间中，模型能够更好地学习数据中的规律。特征的选择和构造，就是人为地帮助模型学习到原本很难学好的东西，从而使模型达到更好的效果。</p>
<h4 id="1-根据现实情况构造特征"><a href="#1-根据现实情况构造特征" class="headerlink" title="1. 根据现实情况构造特征"></a>1. 根据现实情况构造特征</h4><h5 id="1-1-各点与特定点的距离"><a href="#1-1-各点与特定点的距离" class="headerlink" title="1.1 各点与特定点的距离"></a>1.1 各点与特定点的距离</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;x_dis&#x27;</span>] = (df[<span class="string">&#x27;x&#x27;</span>] - <span class="number">6165599</span>).<span class="built_in">abs</span>()</span><br><span class="line">df[<span class="string">&#x27;y_dis&#x27;</span>] = (df[<span class="string">&#x27;y&#x27;</span>] - <span class="number">5202660</span>).<span class="built_in">abs</span>()</span><br><span class="line">df[<span class="string">&#x27;base_dis] = (df[&#x27;</span>y_dis<span class="string">&#x27;]**2))**0.5 + ((df[&#x27;</span>x_dis<span class="string">&#x27;]**2)</span></span><br><span class="line"><span class="string">del df[&#x27;</span>x_dis<span class="string">&#x27;],df[&#x27;</span>y_dis<span class="string">&#x27;] </span></span><br><span class="line"><span class="string">df[&#x27;</span>base_dis_dif<span class="string">f&#x27;].head()</span></span><br></pre></td></tr></table></figure>
<h5 id="1-2-将时间划分为白天与黑夜"><a href="#1-2-将时间划分为白天与黑夜" class="headerlink" title="1.2 将时间划分为白天与黑夜"></a>1.2 将时间划分为白天与黑夜</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;day_night&#x27;</span>] = <span class="number">0</span></span><br><span class="line">df.loc[(df[<span class="string">&#x27;hour&#x27;</span>] &gt; <span class="number">5</span>) &amp; (df[<span class="string">&#x27;hour&#x27;</span>] &lt; <span class="number">20</span>),<span class="string">&#x27;day_night&#x27;</span>] = <span class="number">1</span></span><br><span class="line">df[<span class="string">&#x27;day_night&#x27;</span>].head()</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h5 id="1-3-将月份划分为季度"><a href="#1-3-将月份划分为季度" class="headerlink" title="1.3 将月份划分为季度"></a>1.3 将月份划分为季度</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;quarter&#x27;</span>] = <span class="number">0</span></span><br><span class="line">df.loc[(df[<span class="string">&#x27;month&#x27;</span>].isin([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])), <span class="string">&#x27;quarter&#x27;</span>] = <span class="number">1</span></span><br><span class="line">df.loc[(df[<span class="string">&#x27;month&#x27;</span>].isin([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, ])), <span class="string">&#x27;quarter&#x27;</span>] = <span class="number">2</span></span><br><span class="line">df.loc[(df[<span class="string">&#x27;month&#x27;</span>].isin([<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])), <span class="string">&#x27;quarter&#x27;</span>] = <span class="number">3</span></span><br><span class="line">df.loc[(df[<span class="string">&#x27;month&#x27;</span>].isin([<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>])), <span class="string">&#x27;quarter&#x27;</span>] = <span class="number">4</span></span><br></pre></td></tr></table></figure>
<h5 id="1-4-特征变化量之间的相似性"><a href="#1-4-特征变化量之间的相似性" class="headerlink" title="1.4 特征变化量之间的相似性"></a>1.4 特征变化量之间的相似性</h5><p>①统计每个ship的对应速度等级的个数.<br>②对方位进行16均分.<br>③统计速度为0的个数，以及速度不为0的统计量.<br>④加入x，v，d，y的中位数和各种位数,并删去count\mean\min\max\std等多余统计特征.<br>⑤以shift为主键,求相邻差异值(偏移量).</p>
<h4 id="2-构造分箱特征"><a href="#2-构造分箱特征" class="headerlink" title="2. 构造分箱特征"></a>2. 构造分箱特征</h4><h5 id="2-1-经纬度和速度的分箱特征"><a href="#2-1-经纬度和速度的分箱特征" class="headerlink" title="2.1 经纬度和速度的分箱特征"></a>2.1 经纬度和速度的分箱特征</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[<span class="string">&#x27;v_bin&#x27;</span>] = pd.qcut(df[<span class="string">&#x27;v&#x27;</span>], <span class="number">200</span>, duplicates=<span class="string">&#x27;drop&#x27;</span>) <span class="comment"># 速度进行 200分位数分箱</span></span><br><span class="line">df[<span class="string">&#x27;v_bin&#x27;</span>] = df[<span class="string">&#x27;v_bin&#x27;</span>].<span class="built_in">map</span>(<span class="built_in">dict</span>(<span class="built_in">zip</span>(df[<span class="string">&#x27;v_bin&#x27;</span>].unique().<span class="built_in">range</span>(df[<span class="string">&#x27;v_bin&#x27;</span>].nunique())))) <span class="comment"># 分箱后映射编码</span></span><br></pre></td></tr></table></figure>

<h5 id="2-2-经纬度分箱后并构造区域"><a href="#2-2-经纬度分箱后并构造区域" class="headerlink" title="2.2 经纬度分箱后并构造区域"></a>2.2 经纬度分箱后并构造区域</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">traj.sort_values(by=<span class="string">&#x27;x&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">x_res = np.zeros((<span class="built_in">len</span>(traj), ))</span><br><span class="line">j = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, col_bins + <span class="number">1</span>):</span><br><span class="line">    low, high = x_bins[i-<span class="number">1</span>], x_bins[i]</span><br><span class="line">    <span class="keyword">while</span>( j &lt; <span class="built_in">len</span>(traj)):</span><br><span class="line">        <span class="keyword">if</span> (traj[<span class="string">&quot;x&quot;</span>].iloc[j] &lt;= high) &amp; (traj[<span class="string">&quot;x&quot;</span>].iloc[j] &gt; low - <span class="number">0.001</span>):</span><br><span class="line">            x_res[j] = i</span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<h4 id="3-构造DataFramte特征"><a href="#3-构造DataFramte特征" class="headerlink" title="3. 构造DataFramte特征"></a>3. 构造DataFramte特征</h4><h5 id="3-1-count计数值"><a href="#3-1-count计数值" class="headerlink" title="3.1 count计数值"></a>3.1 count计数值</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_save_unique_visit_count_table</span>(<span class="params">traj_data_df=<span class="literal">None</span>, bin_to_coord_df=<span class="literal">None</span></span>):</span></span><br><span class="line">    unique_boat_count_df = traj_data_df.groupby([<span class="string">&quot;no_bin&quot;</span>])[<span class="string">&quot;id&quot;</span>].nunique().reset_index()</span><br><span class="line">    unique_boat_count_df.rename(&#123;<span class="string">&quot;id&quot;</span>:<span class="string">&quot;visit_boat_count&quot;</span>&#125;, axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    unique_boat_count_df_save = pd.merge(bin_to_coord_df, unique_boat_count_df,</span><br><span class="line">                                         on=<span class="string">&quot;no_bin&quot;</span>, how=<span class="string">&quot;left&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> unique_boat_count_d</span><br></pre></td></tr></table></figure>
<h5 id="3-2-shift偏移量"><a href="#3-2-shift偏移量" class="headerlink" title="3.2 shift偏移量"></a>3.2 shift偏移量</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> [<span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;y&#x27;</span>]:</span><br><span class="line">    <span class="comment">#对x,y坐标进行时间平移 1 -1 2</span></span><br><span class="line">    df[f + <span class="string">&#x27;_prev_diff&#x27;</span>] = df[f] - g[f].shift(<span class="number">1</span>)</span><br><span class="line">    df[f + <span class="string">&#x27;_next_diff&#x27;</span>] = df[f] - g[f].shift(-<span class="number">1</span>)</span><br><span class="line">    df[f + <span class="string">&#x27;_prev_next_diff&#x27;</span>] = g[f].shift(<span class="number">1</span>) - g[f].shift(-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h5 id="3-3-统计特征"><a href="#3-3-统计特征" class="headerlink" title="3.3 统计特征"></a>3.3 统计特征</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">group_feature</span>(<span class="params">df, key, target, aggs,flag</span>):</span>   </span><br><span class="line">    <span class="comment">#通过字典的形式来构建方法和重命名</span></span><br><span class="line">    agg_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> ag <span class="keyword">in</span> aggs:</span><br><span class="line">        agg_dict[<span class="string">&#x27;&#123;&#125;_&#123;&#125;_&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(target,ag,flag)] = ag</span><br><span class="line">        </span><br><span class="line">    t = df.groupby(key)[target].agg(agg_dict).reset_index()</span><br><span class="line">    <span class="keyword">return</span> t</span><br><span class="line"></span><br><span class="line">t = group_feature(df, <span class="string">&#x27;ship&#x27;</span>,<span class="string">&#x27;x&#x27;</span>,[<span class="string">&#x27;max&#x27;</span>,<span class="string">&#x27;min&#x27;</span>,<span class="string">&#x27;mean&#x27;</span>,<span class="string">&#x27;median&#x27;</span>,<span class="string">&#x27;std&#x27;</span>,<span class="string">&#x27;skew&#x27;</span>],flag)</span><br><span class="line">train = pd.merge(train, t, on=<span class="string">&#x27;ship&#x27;</span>, how=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line">t = group_feature(df, <span class="string">&#x27;ship&#x27;</span>,<span class="string">&#x27;y&#x27;</span>,[<span class="string">&#x27;max&#x27;</span>,<span class="string">&#x27;min&#x27;</span>,<span class="string">&#x27;mean&#x27;</span>,<span class="string">&#x27;median&#x27;</span>,<span class="string">&#x27;std&#x27;</span>,<span class="string">&#x27;skew&#x27;</span>],flag)</span><br><span class="line">train = pd.merge(train, t, on=<span class="string">&#x27;ship&#x27;</span>, how=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line">t = group_feature(df, <span class="string">&#x27;ship&#x27;</span>,<span class="string">&#x27;base_dis_diff&#x27;</span>,[<span class="string">&#x27;max&#x27;</span>,<span class="string">&#x27;min&#x27;</span>,<span class="string">&#x27;mean&#x27;</span>,<span class="string">&#x27;std&#x27;</span>,<span class="string">&#x27;skew&#x27;</span>],flag)</span><br><span class="line">train = pd.merge(train, t, on=<span class="string">&#x27;ship&#x27;</span>, how=<span class="string">&#x27;left&#x27;</span>)</span><br></pre></td></tr></table></figure>


<h4 id="4-构造Embedding特征"><a href="#4-构造Embedding特征" class="headerlink" title="4.构造Embedding特征"></a>4.构造Embedding特征</h4><p>word embedding就是将词映射到另外一个空间,相同类型的词在投影之后的向量空间距离更近,倾向于归到一起.</p>
<h5 id="4-1-Word2vec构造词向量"><a href="#4-1-Word2vec构造词向量" class="headerlink" title="4.1 Word2vec构造词向量"></a>4.1 Word2vec构造词向量</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(num_runs)):</span><br><span class="line">    model = Word2Vec(sentences, size=embedding_size,</span><br><span class="line">                              min_count=min_count,</span><br><span class="line">                              workers=mp.cpu_count(),</span><br><span class="line">                              window=window_size,</span><br><span class="line">                              seed=seed, <span class="built_in">iter</span>=iters, sg=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    embedding_vec = []</span><br><span class="line">    <span class="keyword">for</span> ind, seq <span class="keyword">in</span> <span class="built_in">enumerate</span>(sentences):</span><br><span class="line">        seq_vec, word_count = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> seq:</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> model:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                seq_vec += model[word]</span><br><span class="line">                word_count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> word_count == <span class="number">0</span>:</span><br><span class="line">            embedding_vec.append(embedding_size * [<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            embedding_vec.append(seq_vec / word_count)</span><br></pre></td></tr></table></figure>

<h5 id="4-2-NMF提取文本的主题分布"><a href="#4-2-NMF提取文本的主题分布" class="headerlink" title="4.2 NMF提取文本的主题分布"></a>4.2 NMF提取文本的主题分布</h5><p>TF-IDF是衡量字词的出现频率来定义其重要性的加权技术.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用tfidf对元素进行处理</span></span><br><span class="line">tfidf_vectorizer = TfidfVectorizer(ngram_range=(tf_n,tf_n))</span><br><span class="line">tfidf = tfidf_vectorizer.fit_transform(self.data[<span class="string">&#x27;title_feature&#x27;</span>].values)</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用nmf算法，提取文本的主题分布</span></span><br><span class="line">text_nmf = NMF(n_components=self.nmf_n).fit_transform(tfidf)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Feature Engineering</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
        <tag>Embedding</tag>
        <tag>Word2vec</tag>
        <tag>NMF</tag>
        <tag>DataFramte</tag>
      </tags>
  </entry>
  <entry>
    <title>异常检测——集成方法</title>
    <url>/2021/05/23/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E2%80%94%E2%80%94%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<blockquote>
<p>一般情况下，可以把异常检测看成是数据不平衡下的分类问题，如果数据条件允许，优先使用有监督异常检测如XGBOOST；仅有少量标签时，可以用无监督学习作为一种特征抽取方式，最后喂给有监督的分类模型。<br>之前介绍的统计与概率模型、线性模型、基于相似度的模型，和今天要介绍的集成学习都是无监督模型。</p>
</blockquote>
<p>为什么要使用集成学习？——①适合高维度数据(空间稀疏)，②提高模型鲁棒性。<br>值得注意的时，异常检测没有标签，所以feature bagging、bagging比boosting多。而使用boosting进行异常检测，需要生成伪标签。</p>
<span id="more"></span>
<h4 id="1-Feature-Bagging"><a href="#1-Feature-Bagging" class="headerlink" title="1. Feature Bagging"></a>1. Feature Bagging</h4><p>Feature Bagging，基本思想与bagging相似，只是对象是feature。<br>基本检测器可以彼此完全不同，或不同的参数设置，或使用不同采样的子数据集。Feature bagging常用lof算法为基算法。</p>
<h5 id="1-1-算法原理"><a href="#1-1-算法原理" class="headerlink" title="1.1 算法原理"></a>1.1 算法原理</h5><p><img src="https://z3.ax1x.com/2021/05/25/gzMCDA.png" alt="x"></p>
<h5 id="1-2-不同检测器的分数标准化"><a href="#1-2-不同检测器的分数标准化" class="headerlink" title="1.2 不同检测器的分数标准化"></a>1.2 不同检测器的分数标准化</h5><p>不同检测器可能会在不同的尺度上产生分数。例如，平均k近邻检测器会输出原始距离分数，而LOF算法会输出归一化值。另外，检测器输出的异常值分数大小不一，因此需要归一化，常见方法包括平均和最大化组合函数。<br>以累积求和法和广度优先法为例，简单介绍下：<br>①累积求和：针对每组样本，分别累加对应的全部基本检测器的结果；共输出T组求和结果。<br>②广度优先：枚举每一种基本检测器(共m种)，再嵌套枚举每一组异常值输出（共T种），每种异常检测算法分别检测T组样本数据，共有m*T种组合，按照广度优先搜索，先到先占位，重复数据取后来的异常值的最大值、平均值等。</p>
<h5 id="1-3-Feature-Bagging的缺陷"><a href="#1-3-Feature-Bagging的缺陷" class="headerlink" title="1.3 Feature Bagging的缺陷"></a>1.3 Feature Bagging的缺陷</h5><p>①bagging有放回抽样，会损失部分特征的信息，或者特征始终无法被采样也无法被模型训练到。因此使用bagging实际上需要处理偏差和方差之间的一种微妙的均衡。<br>②bagging的比例通常在0.5到0.99之间，如果存在过多的相关性特征，会导致泛化能力下降。所以进行异常检测之前，必须进行严格的相关性分析或者主成分分析。</p>
<h4 id="2-Isolation-Forest"><a href="#2-Isolation-Forest" class="headerlink" title="2. Isolation Forest"></a>2. Isolation Forest</h4><p>周志华老师在2008年提出的算法，在工业界应用广泛，其不需要定义数学模型也不需要训练数据有标签。<br>这种算法在全局和局部都通过采样引入了随机性，往往要比单纯的feature bagging的效果更好。</p>
<h5 id="2-1-关于异常的定义"><a href="#2-1-关于异常的定义" class="headerlink" title="2.1 关于异常的定义"></a>2.1 关于异常的定义</h5><p>使用孤立森林(Isolation Forests)的前提是，将异常点定义为那些 <em>“容易被孤立的离群点”</em> —— 可以理解为分布稀疏，且距离高密度群体较远的点。即默认①异常数据占总样本量的比例很小，②异常点的特征值与正常点的差异很大。</p>
<h5 id="2-2-查找孤立点的策略"><a href="#2-2-查找孤立点的策略" class="headerlink" title="2.2 查找孤立点的策略"></a>2.2 查找孤立点的策略</h5><p>用一个随机超平面来切割数据空间，切一次可以生成两个子空间。继续用随机超平面来切割每个子空间并循环，直到每个子空间只有一个数据点为止。<br>以二维空间作为演示，点B’跟其他数据点比较疏离，只需要很少的几次操作就可以将它细分出来；点A’需要的切分次数会更多一些。<br><img src="https://z3.ax1x.com/2021/05/25/gzK31e.png" alt="xx"></p>
<h5 id="2-3-树的构造方法"><a href="#2-3-树的构造方法" class="headerlink" title="2.3 树的构造方法"></a>2.3 树的构造方法</h5><p><strong>训练：</strong>构建一棵 iTree 时，先从全量数据中抽取一批样本，然后随机选择一个特征作为起始节点，并在该特征的最大值和最小值之间随机选择一个值，将样本中小于该取值的数据划到左分支，大于等于该取值的划到右分支。然后，在左右两个分支数据中，重复上述步骤，直到满足如下条件：①数据不可再分，即：只包含一条数据，或者全部数据相同。②二叉树达到限定的最大深度。</p>
<p><strong>预测：</strong>计算数据 x 的异常分值时，先要估算它在每棵 iTree 中的路径深度。先沿着一棵 iTree，从根节点开始按不同特征的取值从上往下，直到到达某叶子节点。假设 iTree 的训练样本中同样落在 x 所在叶子节点的样本数为 T.size，则数据 x 在这棵 iTree 上的路径深度 h(x)，可以用下面这个公式计算：<br>$$<br>h\left( x \right) =e+C\left( T.size \right)<br>$$<br>式中，e 表示数据 x 从 iTree 的根节点到叶节点过程中经过的边的数目，C(T.size) 可以认为是一个修正值，它表示在一棵用 T.size 条样本数据构建的二叉树的平均路径长度。C(n) 的计算公式如下：<br>$$<br>C\left( n \right) =2H\left( n-1 \right) -\frac{2\left( n-1 \right)}{n}<br>$$<br>其中，H(n-1) 可用 ln(n-1)+0.5772156649(欧拉常数)估算。数据 x 最终的异常分值 Score(x) 综合了多棵 iTree 的结果：<br>$$<br>Score\left( x \right) =2^{-\frac{E\left( h\left( x \right) \right)}{C\left( \varPsi \right)}}<br>$$<br>公式中，E(h(x)) 表示数据 x 在多棵 iTree 的路径长度的均值，ψ表示单棵 iTree 的训练样本的样本数，C(ψ)表示用ψ条数据构建的二叉树的平均路径长度，它在这里主要用来做归一化。</p>
<p>从异常分值的公式看，如果数据 x 在多棵 iTree 中的平均路径长度越短，得分越接近 1，表明数据 x 越异常；如果数据 x 在多棵 iTree 中的平均路径长度越长，得分越接近 0，表示数据 x 越正常；如果数据 x 在多棵 iTree 中的平均路径长度接近整体均值，则打分会在 0.5 附近。</p>
<blockquote>
<p>在实际情况中，不同模型在不同的数据集上表现不一。但是总体而言KNN等基于位置和距离度量的算法原理不太复杂，表现也较为稳定。<br>唯一的缺点是，KNN等基于距离度量模型受到数据维度的影响较大，当维度比较低时表现很好。如果异常特征隐藏在少数维度上时，KNN和LOF类的效果就不会太好，此时该选择Isolation Forest(适合高维空间)。</p>
</blockquote>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>1.<a href="https://github.com/datawhalechina/team-learning-data-mining/tree/master/AnomalyDetection">datawhale-异常检测开源学习资料</a></p>
]]></content>
      <categories>
        <category>Anomaly Detection</category>
      </categories>
      <tags>
        <tag>Isolation Forest</tag>
        <tag>Feature Bagging</tag>
      </tags>
  </entry>
  <entry>
    <title>特征选择_1</title>
    <url>/2021/03/20/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9_1/</url>
    <content><![CDATA[<p>在数据预处理过程中，特征选择是一个重要的过程，选择出重要的特征可以加快模型训练速度。通常可以从以下两方面来选择特征：</p>
<blockquote>
<p>1.特征是否发散（对于样本区分作用的大小）<br>2.特征与标签的相关性</p>
</blockquote>
<span id="more"></span>
<p>特征选择的方法主要有3种：</p>
<blockquote>
<p>1.Filter Method：先根据统计量设置阈值选择特征，之后再训练模型。<br>2.Wrapper Method：把最终将要使用的模型的性能作为特征子集的评价标准，多次训练模型选择有利于模型性能的特征子集。<br>3.Embedding Method：将特征选择过程与模型训练过程融为一体，在模型训练的过程中自动进行特征选择。</p>
</blockquote>
<p>常用sklearn中的feature_selection库来进行特征选择。</p>
<h4 id="1-Fliter-过滤法"><a href="#1-Fliter-过滤法" class="headerlink" title="1. Fliter 过滤法:"></a>1. Fliter 过滤法:</h4><blockquote>
<p>Fliter的优点在于只训练一次模型，速度快。但是选择与标签相关性最强的特征子集不一定是最佳特征，甚至可能对结果负优化。</p>
</blockquote>
<h5 id="1-1-方差选择法"><a href="#1-1-方差选择法" class="headerlink" title="1.1 方差选择法"></a>1.1 方差选择法</h5><p>计算各个特征的方差，设置阈值，选择方差大于阈值的特征。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line"><span class="comment">#参数threshold为方差的阈值</span></span><br><span class="line">VarianceThreshold(threshold=<span class="number">3</span>).fit_transform(iris.data)</span><br></pre></td></tr></table></figure>

<h5 id="1-2-Pearson相关系数法"><a href="#1-2-Pearson相关系数法" class="headerlink" title="1.2 Pearson相关系数法"></a>1.2 Pearson相关系数法</h5><p>计算各个特征对于标签的Pearson相关系数和p值，选择前k名的特征。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"><span class="comment">#参数k为选择的特征个数</span></span><br><span class="line">SelectKBest(<span class="keyword">lambda</span> X, Y: array(<span class="built_in">map</span>(<span class="keyword">lambda</span> x:pearsonr(x, Y), X.T)).T, k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Pearson法的缺陷在于只对线性相关敏感，对非线性关系不敏感。</p>
</blockquote>
<h5 id="1-3-卡方检验-互信息法等方法"><a href="#1-3-卡方检验-互信息法等方法" class="headerlink" title="1.3 卡方检验\互信息法等方法"></a>1.3 卡方检验\互信息法等方法</h5><p>也是用来评价X与y的相关性，先构建评价函数，再选择前K名的特征。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#chi2</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line">SelectKBest(chi2, k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br><span class="line"></span><br><span class="line"><span class="comment">#MIC</span></span><br><span class="line"><span class="keyword">from</span> minepy <span class="keyword">import</span> MINE</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mic</span>(<span class="params">x, y</span>):</span></span><br><span class="line">     m = MINE()</span><br><span class="line">     m.compute_score(x, y)</span><br><span class="line">     <span class="keyword">return</span> (m.mic(), <span class="number">0.5</span>)</span><br><span class="line">SelectKBest(<span class="keyword">lambda</span> X, Y: array(<span class="built_in">map</span>(<span class="keyword">lambda</span> x:mic(x, Y), X.T)).T, k=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>

<h4 id="2-Wrapper-包装法"><a href="#2-Wrapper-包装法" class="headerlink" title="2. Wrapper 包装法:"></a>2. Wrapper 包装法:</h4><blockquote>
<p>Wrapper的优点在于能够识别模型最适宜的特征子集，缺点在于训练多次模型，算法复杂性高，且特征子集不一定是<u>大多数解释变量</u>。</p>
</blockquote>
<p>Wrapper最具代表性的方法就是RFE递归消除特征法，即使用一个基模型来进行多轮训练，每轮训练都遍历所有特征，之后消除重要性(feature_importances_)低的特征，再基于新的特征集进行下一轮训练。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#RFE</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFE</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">rfe = RFE(estimator=LogisticRegression(), n_features_to_select=<span class="number">2</span>).fit_transform(iris.data, iris.target)</span><br><span class="line"></span><br><span class="line"><span class="comment">#RFECV</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> RFECV</span><br><span class="line">rfecv = RFECV(estimator=svc, step=<span class="number">1</span>, cv=StratifiedKFold(<span class="number">2</span>), scoring=<span class="string">&#x27;roc_auc&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="3-Embedded-嵌入法"><a href="#3-Embedded-嵌入法" class="headerlink" title="3. Embedded 嵌入法"></a>3. Embedded 嵌入法</h4><h5 id="3-1-基于惩罚项的特征选择法"><a href="#3-1-基于惩罚项的特征选择法" class="headerlink" title="3.1 基于惩罚项的特征选择法"></a>3.1 基于惩罚项的特征选择法</h5><p>使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。由于L1正则化会产生稀疏权值矩阵，所以其自带特征选择的特性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#L1正则</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">SelectFromModel(LogisticRegression(penalty=<span class="string">&quot;l1&quot;</span>, C=<span class="number">0.1</span>)).fit_transform(iris.data, iris.target)</span><br><span class="line"></span><br><span class="line"><span class="comment">#L2正则</span></span><br><span class="line">SelectFromModel(LogisticRegression(penalty=<span class="string">&quot;l2&quot;</span>,threshold=<span class="number">0.5</span>, C=<span class="number">0.1</span>)).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>

<h5 id="3-2-基于树模型的特征选择法"><a href="#3-2-基于树模型的特征选择法" class="headerlink" title="3.2 基于树模型的特征选择法"></a>3.2 基于树模型的特征选择法</h5><p>树模型中GBDT也可用来作为基模型进行特征选择。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectFromModel</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line">SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target)</span><br></pre></td></tr></table></figure>

<h4 id="4-基于SHAP值的特征筛选"><a href="#4-基于SHAP值的特征筛选" class="headerlink" title="4. 基于SHAP值的特征筛选"></a>4. 基于SHAP值的特征筛选</h4><p>SHAP是由Shapley value启发的可加性解释模型。对于每条样本，每个特征都会对应一个SHAP value值体现其对结果的贡献。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> shap</span><br><span class="line"><span class="comment"># 创建模型解释器</span></span><br><span class="line">explainer_xgb = shap.TreeExplainer(model1)</span><br><span class="line">explainer_lgb = shap.TreeExplainer(model2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取训练集每个样本每个特征特征的SHAP值，并对特征进行整体的可视化</span></span><br><span class="line">shape_values = explainer_lgb.shap_values(data[cols])</span><br><span class="line">shap.summary_plot(shape_values, data[cols], plot_type=<span class="string">&#x27;bar&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="5-基于对抗验证-Adversarial-Validation-的特征筛选"><a href="#5-基于对抗验证-Adversarial-Validation-的特征筛选" class="headerlink" title="5. 基于对抗验证(Adversarial Validation)的特征筛选"></a>5. 基于对抗验证(Adversarial Validation)的特征筛选</h4><p>常用于训练集与测试集相差非常大的情况。实现步骤：<br>1.将训练集和测试集合并，分别打上0和1的标签。<br>2.构建模型进行训练，逐个将特征输入模型，记录AUC。<br>3.最后将AUC高的特征删除(将测试和训练样本差别很大的特征删除)，通过删掉这些特征实现模型效果提升。</p>
]]></content>
      <categories>
        <category>Feature Engineering</category>
      </categories>
      <tags>
        <tag>Embedding</tag>
        <tag>Wapper</tag>
        <tag>Filter</tag>
      </tags>
  </entry>
</search>
